[0m15:13:36.863102 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107664220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1243cee20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1243ceeb0>]}


============================== 15:13:36.883771 | 5467e132-3943-432a-8501-456f4d10b784 ==============================
[0m15:13:36.883771 [info ] [MainThread]: Running with dbt=1.5.0
[0m15:13:36.884090 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:13:36.898644 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5467e132-3943-432a-8501-456f4d10b784', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105274c10>]}
[0m15:13:36.900146 [debug] [MainThread]: Set downloads directory='/var/folders/94/_5czc1mn6cz2scfnd_7y6_6w0000gp/T/dbt-downloads-rqet79iv'
[0m15:13:36.900547 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m15:13:37.095285 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m15:13:37.097340 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m15:13:37.236485 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m15:13:37.247008 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/spark_utils.json
[0m15:13:37.386031 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/spark_utils.json 200
[0m15:13:37.390821 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_external_tables.json
[0m15:13:37.549634 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_external_tables.json 200
[0m15:13:37.561532 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m15:13:38.047166 [info ] [MainThread]: Installed from version 0.8.0
[0m15:13:38.047646 [info ] [MainThread]: Updated version available: 1.1.1
[0m15:13:38.047964 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '5467e132-3943-432a-8501-456f4d10b784', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124454640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1244549d0>]}
[0m15:13:38.048265 [info ] [MainThread]: Installing dbt-labs/spark_utils
[0m15:13:38.326933 [info ] [MainThread]: Installed from version 0.3.0
[0m15:13:38.327381 [info ] [MainThread]: Up to date!
[0m15:13:38.327738 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '5467e132-3943-432a-8501-456f4d10b784', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124454b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124454280>]}
[0m15:13:38.328045 [info ] [MainThread]: Installing dbt-labs/dbt_external_tables
[0m15:13:38.757271 [info ] [MainThread]: Installed from version 0.8.2
[0m15:13:38.757679 [info ] [MainThread]: Updated version available: 0.8.5
[0m15:13:38.757991 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '5467e132-3943-432a-8501-456f4d10b784', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124454a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1244884c0>]}
[0m15:13:38.758258 [info ] [MainThread]: 
[0m15:13:38.758482 [info ] [MainThread]: Updates available for packages: ['dbt-labs/dbt_utils', 'dbt-labs/dbt_external_tables']                 
Update your versions in packages.yml, then run dbt deps
[0m15:13:38.760028 [debug] [MainThread]: Command `dbt deps` succeeded at 15:13:38.759919 after 1.91 seconds
[0m15:13:38.760300 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107664220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051277c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105274c10>]}
[0m15:13:38.760533 [debug] [MainThread]: Flushing usage events
[0m15:14:06.325895 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104748730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1060cedc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1060ced60>]}


============================== 15:14:06.347326 | 4685bf26-5b53-4939-b600-62840b00e86c ==============================
[0m15:14:06.347326 [info ] [MainThread]: Running with dbt=1.5.0
[0m15:14:06.347670 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi/logs', 'profiles_dir': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:14:07.171315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4685bf26-5b53-4939-b600-62840b00e86c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10090b7c0>]}
[0m15:14:07.179396 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4685bf26-5b53-4939-b600-62840b00e86c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13f1c5370>]}
[0m15:14:07.207529 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m15:14:07.218122 [debug] [MainThread]: Failed to load parsed file from disk at /Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi/target/partial_parse.msgpack: Field "nodes" of type MutableMapping[str, Union[AnalysisNode, SingularTestNode, HookNode, ModelNode, RPCNode, SqlNode, GenericTestNode, SnapshotNode, SeedNode]] in Manifest has invalid value {'model.dbsql_dbt_tpch.DimTrade': {'database': None, 'schema': 'dbt_shabbirkdb', 'name': 'DimTrade', 'resource_type': 'model', 'package_name': 'dbsql_dbt_tpch', 'path': 'incremental/DimTrade.sql', 'original_file_path': 'models/incremental/DimTrade.sql', 'unique_id': 'model.dbsql_dbt_tpch.DimTrade', 'fqn': ['dbsql_dbt_tpch', 'incremental', 'DimTrade'], 'alias': 'DimTrade', 'checksum': {'name': 'sha256', 'checksum': '14356f09b5be97f3a95128fd69cd244293e90261cc8f41e6ab797734e542b42a'}, 'config': {'enabled': True, 'alias': None, 'schema': None, 'database': None, 'tags': [], 'meta': {}, 'materialized': 'table', 'incremental_strategy': None, 'persist_docs': {}, 'quoting': {}, 'column_types': {}, 'full_refresh': None, 'unique_key': None, 'on_schema_change': 'ignore', 'grants': {}, 'packages': [], 'docs': {'show': True, 'node_color': None}, 'post-hook': [], 'pre-hook': []}, 'tags': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {'materialized': 'table'}, 'created_at': 1681993370.3724065, 'config_call_dict': {'materialized': 'table'}, 'relation_name': '`dbt_shabbirkdb`.`DimTrade`', 'raw_code': '{{\n    config(\n        materialized = \'table\'\n    )\n}}\nSELECT\n  trade.tradeid,\n  sk_brokerid,\n  trade.sk_createdateid,\n  trade.sk_createtimeid,\n  trade.sk_closedateid,\n  trade.sk_closetimeid,\n  st_name status,\n  tt_name type,\n  trade.cashflag,\n  sk_securityid,\n  sk_companyid,\n  trade.quantity,\n  trade.bidprice,\n  sk_customerid,\n  sk_accountid,\n  trade.executedby,\n  trade.tradeprice,\n  trade.fee,\n  trade.commission,\n  trade.tax,\n  trade.batchid\nFROM (\n  SELECT * EXCEPT(t_dts)\n  FROM (\n    SELECT\n      tradeid,\n      min(date(t_dts)) OVER (PARTITION BY tradeid) createdate,\n      t_dts,\n      coalesce(sk_createdateid, last_value(sk_createdateid) IGNORE NULLS OVER (\n        PARTITION BY tradeid ORDER BY t_dts)) sk_createdateid,\n      coalesce(sk_createtimeid, last_value(sk_createtimeid) IGNORE NULLS OVER (\n        PARTITION BY tradeid ORDER BY t_dts)) sk_createtimeid,\n      coalesce(sk_closedateid, last_value(sk_closedateid) IGNORE NULLS OVER (\n        PARTITION BY tradeid ORDER BY t_dts)) sk_closedateid,\n      coalesce(sk_closetimeid, last_value(sk_closetimeid) IGNORE NULLS OVER (\n        PARTITION BY tradeid ORDER BY t_dts)) sk_closetimeid,\n      cashflag,\n      t_st_id,\n      t_tt_id,\n      t_s_symb,\n      quantity,\n      bidprice,\n      t_ca_id,\n      executedby,\n      tradeprice,\n      fee,\n      commission,\n      tax,\n      batchid\n    FROM (\n      SELECT\n        tradeid,\n        t_dts,\n        if(create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_createdateid,\n        if(create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_createtimeid,\n        if(!create_flg, sk_dateid, cast(NULL AS BIGINT)) sk_closedateid,\n        if(!create_flg, sk_timeid, cast(NULL AS BIGINT)) sk_closetimeid,\n        CASE \n          WHEN t_is_cash = 1 then TRUE\n          WHEN t_is_cash = 0 then FALSE\n          ELSE cast(null as BOOLEAN) END AS cashflag,\n        t_st_id,\n        t_tt_id,\n        t_s_symb,\n        quantity,\n        bidprice,\n        t_ca_id,\n        executedby,\n        tradeprice,\n        fee,\n        commission,\n        tax,\n        t.batchid\n      FROM (\n        SELECT\n          t_id tradeid,\n          th_dts t_dts,\n          t_st_id,\n          t_tt_id,\n          t_is_cash,\n          t_s_symb,\n          t_qty AS quantity,\n          t_bid_price AS bidprice,\n          t_ca_id,\n          t_exec_name AS executedby,\n          t_trade_price AS tradeprice,\n          t_chrg AS fee,\n          t_comm AS commission,\n          t_tax AS tax,\n          1 batchid,\n          CASE \n            WHEN (th_st_id == "SBMT" AND t_tt_id IN ("TMB", "TMS")) OR th_st_id = "PNDG" THEN TRUE \n            WHEN th_st_id IN ("CMPT", "CNCL") THEN FALSE \n            ELSE cast(null as boolean) END AS create_flg\n        FROM {{ source(\'tpcdi\', \'TradeHistory\') }} t\n        JOIN {{ source(\'tpcdi\', \'TradeHistoryRaw\') }} th\n          ON th_t_id = t_id\n        UNION ALL\n        SELECT\n          t_id tradeid,\n          t_dts,\n          t_st_id,\n          t_tt_id,\n          t_is_cash,\n          t_s_symb,\n          t_qty AS quantity,\n          t_bid_price AS bidprice,\n          t_ca_id,\n          t_exec_name AS executedby,\n          t_trade_price AS tradeprice,\n          t_chrg AS fee,\n          t_comm AS commission,\n          t_tax AS tax,\n          t.batchid,\n          CASE \n            WHEN cdc_flag = \'I\' THEN TRUE \n            WHEN t_st_id IN ("CMPT", "CNCL") THEN FALSE \n            ELSE cast(null as boolean) END AS create_flg\n        FROM {{ source(\'tpcdi\', \'TradeIncremental\') }} t\n      ) t\n      JOIN {{ source(\'tpcdi\', \'DimDate\') }} dd\n        ON date(t.t_dts) = dd.datevalue\n      JOIN {{ source(\'tpcdi\', \'DimTime\') }} dt\n        ON date_format(t.t_dts, \'HH:mm:ss\') = dt.timevalue\n    )\n  )\n  QUALIFY ROW_NUMBER() OVER (PARTITION BY tradeid ORDER BY t_dts desc) = 1\n) trade\nJOIN {{ source(\'tpcdi\', \'StatusType\') }} status\n  ON status.st_id = trade.t_st_id\nJOIN {{ source(\'tpcdi\', \'TradeType\') }} tt\n  ON tt.tt_id == trade.t_tt_id\n-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Account IDs are missing from DimSecurity/DimAccount, causing audit check failures. \n${dq_left_flg} JOIN {{ ref(\'DimSecurity\') }} ds\n  ON \n    ds.symbol = trade.t_s_symb\n    AND createdate >= ds.effectivedate \n    AND createdate < ds.enddate\n${dq_left_flg} JOIN {{ ref(\'DimAccount\') }} da\n  ON \n    trade.t_ca_id = da.accountid \n    AND createdate >= da.effectivedate \n    AND createdate < da.enddate', 'language': 'sql', 'refs': [['DimSecurity'], ['DimAccount']], 'sources': [['tpcdi', 'TradeHistory'], ['tpcdi', 'StatusType'], ['tpcdi', 'DimDate'], ['tpcdi', 'TradeType'], ['tpcdi', 'TradeIncremental'], ['tpcdi', 'DimTime'], ['tpcdi', 'TradeHistoryRaw']], 'metrics': [], 'depends_on': {'macros': [], 'nodes': ['source.dbsql_dbt_tpch.tpcdi.TradeHistory', 'source.dbsql_dbt_tpch.tpcdi.StatusType', 'source.dbsql_dbt_tpch.tpcdi.DimDate', 'source.dbsql_dbt_tpch.tpcdi.TradeType', 'source.dbsql_dbt_tpch.tpcdi.TradeIncremental', 'source.dbsql_dbt_tpch.tpcdi.DimTime', 'source.dbsql_dbt_tpch.tpcdi.TradeHistoryRaw', 'model.dbsql_dbt_tpch.DimSecurity', 'model.dbsql_dbt_tpch.DimAccount']}, 'compiled_path': None}, 'model.dbsql_dbt_tpch.DimCustomerStg': {'database': None, 'schema': 'dbt_shabbirkdb', 'name': 'DimCustomerStg', 'resource_type': 'model', 'package_name': 'dbsql_dbt_tpch', 'path': 'incremental/DimCustomerStg.sql', 'original_file_path': 'models/incremental/DimCustomerStg.sql', 'unique_id': 'model.dbsql_dbt_tpch.DimCustomerStg', 'fqn': ['dbsql_dbt_tpch', 'incremental', 'DimCustomerStg'], 'alias': 'DimCustomerStg', 'checksum': {'name': 'sha256', 'checksum': 'f9d10507a9510887b4aaf3e0b168f5d40b033b483d5e3d501f39b03817a84ee9'}, 'config': {'enabled': True, 'alias': None, 'schema': None, 'database': None, 'tags': [], 'meta': {}, 'materialized': 'table', 'incremental_strategy': None, 'persist_docs': {}, 'quoting': {}, 'column_types': {}, 'full_refresh': None, 'unique_key': None, 'on_schema_change': 'ignore', 'grants': {}, 'packages': [], 'docs': {'show': True, 'node_color': None}, 'post-hook': [], 'pre-hook': []}, 'tags': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {'materialized': 'table'}, 'created_at': 1681993370.3868117, 'config_call_dict': {'materialized': 'table'}, 'relation_name': '`dbt_shabbirkdb`.`DimCustomerStg`', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT * FROM (\n  SELECT\n    sk_customerid,\n    customerid,\n    coalesce(taxid, last_value(taxid) IGNORE NULLS OVER (\n        PARTITION BY customerid\n        ORDER BY update_ts)) taxid,\n    status,\n    coalesce(lastname, last_value(lastname) IGNORE NULLS OVER (\n        PARTITION BY customerid\n        ORDER BY update_ts)) lastname,\n    coalesce(firstname, last_value(firstname) IGNORE NULLS OVER (\n        PARTITION BY customerid\n        ORDER BY update_ts)) firstname,\n    coalesce(middleinitial, last_value(middleinitial) IGNORE NULLS OVER (\n        PARTITION BY customerid\n        ORDER BY update_ts)) middleinitial,\n    coalesce(gender, last_value(gender) IGNORE NULLS OVER (\n        PARTITION BY customerid\n        ORDER BY update_ts)) gender,\n    coalesce(tier, last_value(tier) IGNORE NULLS OVER (\n        PARTITION BY customerid\n        ORDER BY update_ts)) tier,\n    coalesce(dob, last_value(dob) IGNORE NULLS OVER (\n        PARTITION BY customerid\n        ORDER BY update_ts)) dob,\n    coalesce(addressline1, last_value(addressline1) IGNORE NULLS OVER (\n        PARTITION BY customerid\n        ORDER BY update_ts)) addressline1,\n    coalesce(addressline2, last_value(addressline2) IGNORE NULLS OVER (\n        PARTITION BY customerid\n        ORDER BY update_ts)) addressline2,\n    coalesce(postalcode, last_value(postalcode) IGNORE NULLS OVER (\n        PARTITION BY customerid\n        ORDER BY update_ts)) postalcode,\n    coalesce(CITY, last_value(CITY) IGNORE NULLS OVER (\n        PARTITION BY customerid\n        ORDER BY update_ts)) CITY,\n    coalesce(stateprov, last_value(stateprov) IGNORE NULLS OVER (\n        PARTITION BY customerid\n        ORDER BY update_ts)) stateprov,\n    coalesce(country, last_value(country) IGNORE NULLS OVER (\n        PARTITION BY customerid\n        ORDER BY update_ts)) country,\n    coalesce(phone1, last_value(phone1) IGNORE NULLS OVER (\n        PARTITION BY customerid\n        ORDER BY update_ts)) phone1,\n    coalesce(phone2, last_value(phone2) IGNORE NULLS OVER (\n        PARTITION BY customerid\n        ORDER BY update_ts)) phone2,\n    coalesce(phone3, last_value(phone3) IGNORE NULLS OVER (\n        PARTITION BY customerid\n        ORDER BY update_ts)) phone3,\n    coalesce(email1, last_value(email1) IGNORE NULLS OVER (\n        PARTITION BY customerid\n        ORDER BY update_ts)) email1,\n    coalesce(email2, last_value(email2) IGNORE NULLS OVER (\n        PARTITION BY customerid\n        ORDER BY update_ts)) email2,\n    coalesce(LCL_TX_ID, last_value(LCL_TX_ID) IGNORE NULLS OVER (\n        PARTITION BY customerid\n        ORDER BY update_ts)) LCL_TX_ID,\n    coalesce(NAT_TX_ID, last_value(NAT_TX_ID) IGNORE NULLS OVER (\n        PARTITION BY customerid\n        ORDER BY update_ts)) NAT_TX_ID,\n    batchid,\n    nvl2(lead(update_ts) OVER (PARTITION BY customerid ORDER BY update_ts), false, true) iscurrent,\n    date(update_ts) effectivedate,\n    coalesce(lead(date(update_ts)) OVER (PARTITION BY customerid ORDER BY update_ts), date('9999-12-31')) enddate\n  FROM (\n    SELECT\n      md5(customerid::string) as sk_customerid,\n      customerid,\n      taxid,\n      status,\n      lastname,\n      firstname,\n      middleinitial,\n      gender,\n      tier,\n      dob,\n      addressline1,\n      addressline2,\n      postalcode,\n      city,\n      stateprov,\n      country,\n      phone1,\n      phone2,\n      phone3,\n      email1,\n      email2,\n      lcl_tx_id,\n      nat_tx_id,\n      1 batchid,\n      update_ts\n    FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n    WHERE ActionType in ('NEW', 'INACT', 'UPDCUST')\n    UNION ALL\n    SELECT\n      md5(c.customerid::string) as sk_customerid,\n      c.customerid,\n      nullif(c.taxid, '') taxid,\n      nullif(s.st_name, '') as status,\n      nullif(c.lastname, '') lastname,\n      nullif(c.firstname, '') firstname,\n      nullif(c.middleinitial, '') middleinitial,\n      gender,\n      c.tier,\n      c.dob,\n      nullif(c.addressline1, '') addressline1,\n      nullif(c.addressline2, '') addressline2,\n      nullif(c.postalcode, '') postalcode,\n      nullif(c.city, '') city,\n      nullif(c.stateprov, '') stateprov,\n      nullif(c.country, '') country,\n      CASE\n        WHEN isnull(c_local_1) then c_local_1\n        ELSE concat(\n          nvl2(c_ctry_1, '+' || c_ctry_1 || ' ', ''),\n          nvl2(c_area_1, '(' || c_area_1 || ') ', ''),\n          c_local_1,\n          nvl(c_ext_1, '')) END as phone1,\n      CASE\n        WHEN isnull(c_local_2) then c_local_2\n        ELSE concat(\n          nvl2(c_ctry_2, '+' || c_ctry_2 || ' ', ''),\n          nvl2(c_area_2, '(' || c_area_2 || ') ', ''),\n          c_local_2,\n          nvl(c_ext_2, '')) END as phone2,\n      CASE\n        WHEN isnull(c_local_3) then c_local_3\n        ELSE concat(\n          nvl2(c_ctry_3, '+' || c_ctry_3 || ' ', ''),\n          nvl2(c_area_3, '(' || c_area_3 || ') ', ''),\n          c_local_3,\n          nvl(c_ext_3, '')) END as phone3,\n      nullif(c.email1, '') email1,\n      nullif(c.email2, '') email2,\n      c.LCL_TX_ID, \n      c.NAT_TX_ID,\n      c.batchid,\n      timestamp(bd.batchdate) update_ts\n    FROM {{ source('tpcdi', 'CustomerIncremental') }} c\n    JOIN {{ source('tpcdi', 'BatchDate') }} bd\n      ON c.batchid = bd.batchid\n    JOIN {{ source('tpcdi', 'StatusType') }} s \n      ON c.status = s.st_id\n  ) c\n  )", 'language': 'sql', 'refs': [], 'sources': [['tpcdi', 'StatusType'], ['tpcdi', 'BatchDate'], ['tpcdi', 'CustomerIncremental']], 'metrics': [], 'depends_on': {'macros': [], 'nodes': ['source.dbsql_dbt_tpch.tpcdi.StatusType', 'source.dbsql_dbt_tpch.tpcdi.BatchDate', 'source.dbsql_dbt_tpch.tpcdi.CustomerIncremental']}, 'compiled_path': None}, 'model.dbsql_dbt_tpch.FactCashBalances': {'database': None, 'schema': 'dbt_shabbirkdb', 'name': 'FactCashBalances', 'resource_type': 'model', 'package_name': 'dbsql_dbt_tpch', 'path': 'incremental/FactCashBalances.sql', 'original_file_path': 'models/incremental/FactCashBalances.sql', 'unique_id': 'model.dbsql_dbt_tpch.FactCashBalances', 'fqn': ['dbsql_dbt_tpch', 'incremental', 'FactCashBalances'], 'alias': 'FactCashBalances', 'checksum': {'name': 'sha256', 'checksum': 'fbe50e9ccf6dd41416bcdf491d6da6c10020807dda669c4cf2882a32da6080be'}, 'config': {'enabled': True, 'alias': None, 'schema': None, 'database': None, 'tags': [], 'meta': {}, 'materialized': 'table', 'incremental_strategy': None, 'persist_docs': {}, 'quoting': {}, 'column_types': {}, 'full_refresh': None, 'unique_key': None, 'on_schema_change': 'ignore', 'grants': {}, 'packages': [], 'docs': {'show': True, 'node_color': None}, 'post-hook': [], 'pre-hook': []}, 'tags': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {'materialized': 'table'}, 'created_at': 1681993370.3897843, 'config_call_dict': {'materialized': 'table'}, 'relation_name': '`dbt_shabbirkdb`.`FactCashBalances`', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\n\nSELECT\n  a.sk_customerid, \n  a.sk_accountid, \n  d.sk_dateid, \n  sum(account_daily_total) OVER (partition by c.accountid order by c.datevalue) cash,\n  c.batchid\nFROM (\n  SELECT \n    ct_ca_id accountid,\n    to_date(ct_dts) datevalue,\n    sum(ct_amt) account_daily_total,\n    batchid\n  FROM (\n    SELECT * , 1 batchid\n    FROM {{ source('tpcdi', 'CashTransactionHistory') }}\n    UNION ALL\n    SELECT * except(cdc_flag, cdc_dsn)\n    FROM {{ source('tpcdi', 'CashTransactionIncremental') }}\n  )\n  GROUP BY\n    accountid,\n    datevalue,\n    batchid) c \nJOIN {{ source('tpcdi', 'DimDate') }} d \n  ON c.datevalue = d.datevalue\n-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Account IDs are missing from DimAccount, causing audit check failures. \n JOIN {{ ref( 'DimAccount') }} a \n  ON \n    c.accountid = a.accountid\n    AND c.datevalue >= a.effectivedate \n    AND c.datevalue < a.enddate", 'language': 'sql', 'refs': [['DimAccount']], 'sources': [['tpcdi', 'CashTransactionIncremental'], ['tpcdi', 'DimDate'], ['tpcdi', 'CashTransactionHistory']], 'metrics': [], 'depends_on': {'macros': [], 'nodes': ['source.dbsql_dbt_tpch.tpcdi.CashTransactionIncremental', 'source.dbsql_dbt_tpch.tpcdi.DimDate', 'source.dbsql_dbt_tpch.tpcdi.CashTransactionHistory', 'model.dbsql_dbt_tpch.DimAccount']}, 'compiled_path': None}, 'model.dbsql_dbt_tpch.FactHoldings': {'database': None, 'schema': 'dbt_shabbirkdb', 'name': 'FactHoldings', 'resource_type': 'model', 'package_name': 'dbsql_dbt_tpch', 'path': 'incremental/FactHoldings.sql', 'original_file_path': 'models/incremental/FactHoldings.sql', 'unique_id': 'model.dbsql_dbt_tpch.FactHoldings', 'fqn': ['dbsql_dbt_tpch', 'incremental', 'FactHoldings'], 'alias': 'FactHoldings', 'checksum': {'name': 'sha256', 'checksum': '1c26983fdb85969c895b9b2b6a6819ea9217578697b4d9276766b76c1508e6d6'}, 'config': {'enabled': True, 'alias': None, 'schema': None, 'database': None, 'tags': [], 'meta': {}, 'materialized': 'table', 'incremental_strategy': None, 'persist_docs': {}, 'quoting': {}, 'column_types': {}, 'full_refresh': None, 'unique_key': None, 'on_schema_change': 'ignore', 'grants': {}, 'packages': [], 'docs': {'show': True, 'node_color': None}, 'post-hook': [], 'pre-hook': []}, 'tags': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {'materialized': 'table'}, 'created_at': 1681993370.392956, 'config_call_dict': {'materialized': 'table'}, 'relation_name': '`dbt_shabbirkdb`.`FactHoldings`', 'raw_code': '{{\n    config(\n        materialized = \'table\'\n    )\n}}\n\nSELECT \n  hh_h_t_id tradeid,\n  hh_t_id currenttradeid,\n  sk_customerid,\n  sk_accountid,\n  sk_securityid,\n  sk_companyid,\n  sk_closedateid sk_dateid,\n  sk_closetimeid sk_timeid,\n  tradeprice currentprice,\n  hh_after_qty currentholding,\n  hh.batchid\nFROM (\n  SELECT \n    * ,\n    1 batchid\n  FROM {{ source(\'tpcdi\', \'HoldingHistory\') }}\n  UNION ALL\n  SELECT * except(cdc_flag, cdc_dsn)\n  FROM {{ source(\'tpcdi\', \'HoldingIncremental\') }}) hh\n-- Converts to LEFT JOIN if this is run as DQ EDITION. It is possible, because of the issues upstream with DimSecurity/DimAccount on "some" scale factors, that DimTrade may be missing some rows.\n${dq_left_flg} JOIN {{ ref(\'DimTrade\') }} dt\n  ON tradeid = hh_t_id', 'language': 'sql', 'refs': [['DimTrade']], 'sources': [['tpcdi', 'HoldingIncremental'], ['tpcdi', 'HoldingHistory']], 'metrics': [], 'depends_on': {'macros': [], 'nodes': ['source.dbsql_dbt_tpch.tpcdi.HoldingIncremental', 'source.dbsql_dbt_tpch.tpcdi.HoldingHistory', 'model.dbsql_dbt_tpch.DimTrade']}, 'compiled_path': None}, 'model.dbsql_dbt_tpch.FactMarketHistory': {'database': None, 'schema': 'dbt_shabbirkdb', 'name': 'FactMarketHistory', 'resource_type': 'model', 'package_name': 'dbsql_dbt_tpch', 'path': 'incremental/FactMarketHistory.sql', 'original_file_path': 'models/incremental/FactMarketHistory.sql', 'unique_id': 'model.dbsql_dbt_tpch.FactMarketHistory', 'fqn': ['dbsql_dbt_tpch', 'incremental', 'FactMarketHistory'], 'alias': 'FactMarketHistory', 'checksum': {'name': 'sha256', 'checksum': '48e1e8a6761a3778e606b0423889ffa4bc58594fa8f5e8578c366996fa2f7bec'}, 'config': {'enabled': True, 'alias': None, 'schema': None, 'database': None, 'tags': [], 'meta': {}, 'materialized': 'table', 'incremental_strategy': None, 'persist_docs': {}, 'quoting': {}, 'column_types': {}, 'full_refresh': None, 'unique_key': None, 'on_schema_change': 'ignore', 'grants': {}, 'packages': [], 'docs': {'show': True, 'node_color': None}, 'post-hook': [], 'pre-hook': []}, 'tags': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {'materialized': 'table'}, 'created_at': 1681993370.3957996, 'config_call_dict': {'materialized': 'table'}, 'relation_name': '`dbt_shabbirkdb`.`FactMarketHistory`', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  s.sk_securityid,\n  s.sk_companyid,\n  sk_dateid,\n  fmh.dm_close / sum_fi_basic_eps AS peratio,\n  (s.dividend / fmh.dm_close) / 100 yield,\n  fiftytwoweekhigh,\n  sk_fiftytwoweekhighdate,\n  fiftytwoweeklow,\n  sk_fiftytwoweeklowdate,\n  dm_close closeprice,\n  dm_high dayhigh,\n  dm_low daylow,\n  dm_vol volume,\n  fmh.batchid\nFROM (\n  SELECT * FROM (\n    SELECT \n      a.*,\n      b.sk_dateid AS sk_fiftytwoweeklowdate,\n      c.sk_dateid AS sk_fiftytwoweekhighdate\n    FROM\n      {{ ref('tempDailyMarketHistorical') }}a\n    JOIN  {{ ref('tempDailyMarketHistorical') }} b \n      ON\n        a.dm_s_symb = b.dm_s_symb\n        AND a.fiftytwoweeklow = b.dm_low\n        AND b.dm_date between add_months(a.dm_date, -12) AND a.dm_date\n    JOIN  {{ ref('tempDailyMarketHistorical') }} c \n      ON \n        a.dm_s_symb = c.dm_s_symb\n        AND a.fiftytwoweekhigh = c.dm_high\n        AND c.dm_date between add_months(a.dm_date, -12) AND a.dm_date) dmh\n  QUALIFY ROW_NUMBER() OVER (\n    PARTITION BY dm_s_symb, dm_date \n    ORDER BY sk_fiftytwoweeklowdate, sk_fiftytwoweekhighdate) = 1) fmh\n-- Converts to LEFT JOIN if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security Security symbols are missing from DimSecurity, causing audit check failures. \n${dq_left_flg} JOIN {{ ref('DimSecurity') }} s \n  ON \n    s.symbol = fmh.dm_s_symb\n    AND fmh.dm_date >= s.effectivedate \n    AND fmh.dm_date < s.enddate\nLEFT JOIN  {{ ref('tempSumpFiBasicEps') }} f \n  ON \n    f.sk_companyid = s.sk_companyid\n    AND quarter(fmh.dm_date) = quarter(fi_qtr_start_date)\n    AND year(fmh.dm_date) = year(fi_qtr_start_date);", 'language': 'sql', 'refs': [['tempDailyMarketHistorical'], ['tempDailyMarketHistorical'], ['tempDailyMarketHistorical'], ['DimSecurity'], ['tempSumpFiBasicEps']], 'sources': [], 'metrics': [], 'depends_on': {'macros': [], 'nodes': ['model.dbsql_dbt_tpch.tempDailyMarketHistorical', 'model.dbsql_dbt_tpch.tempDailyMarketHistorical', 'model.dbsql_dbt_tpch.tempDailyMarketHistorical', 'model.dbsql_dbt_tpch.DimSecurity', 'model.dbsql_dbt_tpch.tempSumpFiBasicEps']}, 'compiled_path': None}, 'model.dbsql_dbt_tpch.tempSumpFiBasicEps': {'database': None, 'schema': 'dbt_shabbirkdb', 'name': 'tempSumpFiBasicEps', 'resource_type': 'model', 'package_name': 'dbsql_dbt_tpch', 'path': 'incremental/tempSumpFiBasicEps.sql', 'original_file_path': 'models/incremental/tempSumpFiBasicEps.sql', 'unique_id': 'model.dbsql_dbt_tpch.tempSumpFiBasicEps', 'fqn': ['dbsql_dbt_tpch', 'incremental', 'tempSumpFiBasicEps'], 'alias': 'tempSumpFiBasicEps', 'checksum': {'name': 'sha256', 'checksum': 'a5ed264d4f99ccb5b921bf16ce6510b5b84d213be200f508f815d74f791d6d8b'}, 'config': {'enabled': True, 'alias': None, 'schema': None, 'database': None, 'tags': [], 'meta': {}, 'materialized': 'table', 'incremental_strategy': None, 'persist_docs': {}, 'quoting': {}, 'column_types': {}, 'full_refresh': None, 'unique_key': None, 'on_schema_change': 'ignore', 'grants': {}, 'packages': [], 'docs': {'show': True, 'node_color': None}, 'post-hook': [], 'pre-hook': []}, 'tags': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {'materialized': 'table'}, 'created_at': 1681993370.3988762, 'config_call_dict': {'materialized': 'table'}, 'relation_name': '`dbt_shabbirkdb`.`tempSumpFiBasicEps`', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT\n  sk_companyid,\n  fi_qtr_start_date,\n  sum(fi_basic_eps) OVER (PARTITION BY companyid ORDER BY fi_qtr_start_date ROWS BETWEEN 4 PRECEDING AND CURRENT ROW) - fi_basic_eps sum_fi_basic_eps\nFROM {{ ref('Financial') }}\nJOIN {{ ref('DimCompany') }}\n  USING (sk_companyid);", 'language': 'sql', 'refs': [['Financial'], ['DimCompany']], 'sources': [], 'metrics': [], 'depends_on': {'macros': [], 'nodes': ['model.dbsql_dbt_tpch.Financial', 'model.dbsql_dbt_tpch.DimCompany']}, 'compiled_path': None}, 'model.dbsql_dbt_tpch.DimCustomer': {'database': None, 'schema': 'dbt_shabbirkdb', 'name': 'DimCustomer', 'resource_type': 'model', 'package_name': 'dbsql_dbt_tpch', 'path': 'incremental/DimCustomer.sql', 'original_file_path': 'models/incremental/DimCustomer.sql', 'unique_id': 'model.dbsql_dbt_tpch.DimCustomer', 'fqn': ['dbsql_dbt_tpch', 'incremental', 'DimCustomer'], 'alias': 'DimCustomer', 'checksum': {'name': 'sha256', 'checksum': '1fc2ecd7d106272b597b2ef8bf0f516a5e9e1296294d49352319c5a99720cea6'}, 'config': {'enabled': True, 'alias': None, 'schema': None, 'database': None, 'tags': [], 'meta': {}, 'materialized': 'table', 'incremental_strategy': None, 'persist_docs': {}, 'quoting': {}, 'column_types': {}, 'full_refresh': None, 'unique_key': None, 'on_schema_change': 'ignore', 'grants': {}, 'packages': [], 'docs': {'show': True, 'node_color': None}, 'post-hook': [], 'pre-hook': []}, 'tags': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {'materialized': 'table'}, 'created_at': 1681993370.4024148, 'config_call_dict': {'materialized': 'table'}, 'relation_name': '`dbt_shabbirkdb`.`DimCustomer`', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\n\nSELECT \n  c.sk_customerid,\n  c.customerid,\n  c.taxid,\n  c.status,\n  c.lastname,\n  c.firstname,\n  c.middleinitial,\n  if(c.gender IN ('M', 'F'), c.gender, 'U') gender,\n  c.tier,\n  c.dob,\n  c.addressline1,\n  c.addressline2,\n  c.postalcode,\n  c.city,\n  c.stateprov,\n  c.country,\n  c.phone1,\n  c.phone2,\n  c.phone3,\n  c.email1,\n  c.email2,\n  r_nat.TX_NAME as nationaltaxratedesc,\n  r_nat.TX_RATE as nationaltaxrate,\n  r_lcl.TX_NAME as localtaxratedesc,\n  r_lcl.TX_RATE as localtaxrate,\n  p.agencyid,\n  p.creditrating,\n  p.networth,\n  p.marketingnameplate,\n  c.iscurrent,\n  c.batchid,\n  c.effectivedate,\n  c.enddate\nFROM {{ ref('DimCustomerStg') }} c\nLEFT JOIN {{ source('tpcdi', 'TaxRate') }} r_lcl \n  ON c.LCL_TX_ID = r_lcl.TX_ID\nLEFT JOIN {{ source('tpcdi', 'TaxRate') }} r_nat \n  ON c.NAT_TX_ID = r_nat.TX_ID\nLEFT JOIN {{ ref('Prospect') }} p \n  on upper(p.lastname) = upper(c.lastname)\n  and upper(p.firstname) = upper(c.firstname)\n  and upper(p.addressline1) = upper(c.addressline1)\n  and upper(nvl(p.addressline2, '')) = upper(nvl(c.addressline2, ''))\n  and upper(p.postalcode) = upper(c.postalcode);", 'language': 'sql', 'refs': [['DimCustomerStg'], ['Prospect']], 'sources': [['tpcdi', 'TaxRate']], 'metrics': [], 'depends_on': {'macros': [], 'nodes': ['source.dbsql_dbt_tpch.tpcdi.TaxRate', 'model.dbsql_dbt_tpch.DimCustomerStg', 'model.dbsql_dbt_tpch.Prospect']}, 'compiled_path': None}, 'model.dbsql_dbt_tpch.Prospect': {'database': None, 'schema': 'dbt_shabbirkdb', 'name': 'Prospect', 'resource_type': 'model', 'package_name': 'dbsql_dbt_tpch', 'path': 'incremental/Prospect.sql', 'original_file_path': 'models/incremental/Prospect.sql', 'unique_id': 'model.dbsql_dbt_tpch.Prospect', 'fqn': ['dbsql_dbt_tpch', 'incremental', 'Prospect'], 'alias': 'Prospect', 'checksum': {'name': 'sha256', 'checksum': 'daf239644cc462448c238a5b332d2ab5bb8528ff948cb7f1d0193ebc39e86849'}, 'config': {'enabled': True, 'alias': None, 'schema': None, 'database': None, 'tags': [], 'meta': {}, 'materialized': 'table', 'incremental_strategy': None, 'persist_docs': {}, 'quoting': {}, 'column_types': {}, 'full_refresh': None, 'unique_key': None, 'on_schema_change': 'ignore', 'grants': {}, 'packages': [], 'docs': {'show': True, 'node_color': None}, 'post-hook': [], 'pre-hook': []}, 'tags': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {'materialized': 'table'}, 'created_at': 1681993370.4054286, 'config_call_dict': {'materialized': 'table'}, 'relation_name': '`dbt_shabbirkdb`.`Prospect`', 'raw_code': '{{\n    config(\n        materialized = \'table\'\n    )\n}}\nSELECT \n  agencyid,\n  recdate.sk_dateid sk_recorddateid,\n  origdate.sk_dateid sk_updatedateid,\n  p.batchid,\n  nvl2(c.customerid, True, False) iscustomer, \n  p.lastname,\n  p.firstname,\n  p.middleinitial,\n  p.gender,\n  p.addressline1,\n  p.addressline2,\n  p.postalcode,\n  city,\n  state,\n  country,\n  phone,\n  income,\n  numbercars,\n  numberchildren,\n  maritalstatus,\n  age,\n  creditrating,\n  ownorrentflag,\n  employer,\n  numbercreditcards,\n  networth,\n  if(\n    isnotnull(\n      if(networth > 1000000 or income > 200000,"HighValue+","") || \n      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||\n      if(age > 45, "Boomer+", "") ||\n      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||\n      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||\n      if(age < 25 and networth > 1000000, "Inherited+","")),\n    left(\n      if(networth > 1000000 or income > 200000,"HighValue+","") || \n      if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||\n      if(age > 45, "Boomer+", "") ||\n      if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||\n      if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||\n      if(age < 25 and networth > 1000000, "Inherited+",""),\n      length(\n        if(networth > 1000000 or income > 200000,"HighValue+","") || \n        if(numberchildren > 3 or numbercreditcards > 5,"Expenses+","") ||\n        if(age > 45, "Boomer+", "") ||\n        if(income < 50000 or creditrating < 600 or networth < 100000, "MoneyAlert+","") ||\n        if(numbercars > 3 or numbercreditcards > 7, "Spender+","") ||\n        if(age < 25 and networth > 1000000, "Inherited+",""))\n      -1),\n    NULL) marketingnameplate\nFROM (\n  SELECT \n    * FROM (\n    SELECT\n      agencyid,\n      max(batchid) recordbatchid,\n      lastname,\n      firstname,\n      middleinitial,\n      gender,\n      addressline1,\n      addressline2,\n      postalcode,\n      city,\n      state,\n      country,\n      phone,\n      income,\n      numbercars,\n      numberchildren,\n      maritalstatus,\n      age,\n      creditrating,\n      ownorrentflag,\n      employer,\n      numbercreditcards,\n      networth,\n      min(batchid) batchid\n    FROM {{ source(\'tpcdi\', \'ProspectRaw\') }} p\n    GROUP BY\n      agencyid,\n      lastname,\n      firstname,\n      middleinitial,\n      gender,\n      addressline1,\n      addressline2,\n      postalcode,\n      city,\n      state,\n      country,\n      phone,\n      income,\n      numbercars,\n      numberchildren,\n      maritalstatus,\n      age,\n      creditrating,\n      ownorrentflag,\n      employer,\n      numbercreditcards,\n      networth)\n  QUALIFY ROW_NUMBER() OVER (PARTITION BY agencyid ORDER BY batchid DESC) = 1) p\nJOIN (\n  SELECT \n    sk_dateid,\n    batchid\n  FROM {{ source(\'tpcdi\', \'BatchDate\') }} b \n  JOIN {{ source(\'tpcdi\', \'DimDate\') }} d \n    ON b.batchdate = d.datevalue) recdate\n  ON p.recordbatchid = recdate.batchid\nJOIN (\n  SELECT \n    sk_dateid,\n    batchid\n  FROM {{ source(\'tpcdi\', \'BatchDate\') }} b \n  JOIN {{ source(\'tpcdi\', \'DimDate\') }} d \n    ON b.batchdate = d.datevalue) origdate\n  ON p.batchid = origdate.batchid\nLEFT JOIN (\n  SELECT \n    customerid,\n    lastname,\n    firstname,\n    addressline1,\n    addressline2,\n    postalcode\n  FROM {{ ref(\'DimCustomerStg\') }}\n  WHERE iscurrent) c\n  ON \n    upper(p.LastName) = upper(c.lastname)\n    and upper(p.FirstName) = upper(c.firstname)\n    and upper(p.AddressLine1) = upper(c.addressline1)\n    and upper(nvl(p.addressline2, \'\')) = upper(nvl(c.addressline2, \'\'))\n    and upper(p.PostalCode) = upper(c.postalcode)', 'language': 'sql', 'refs': [['DimCustomerStg']], 'sources': [['tpcdi', 'ProspectRaw'], ['tpcdi', 'DimDate'], ['tpcdi', 'BatchDate']], 'metrics': [], 'depends_on': {'macros': [], 'nodes': ['source.dbsql_dbt_tpch.tpcdi.ProspectRaw', 'source.dbsql_dbt_tpch.tpcdi.DimDate', 'source.dbsql_dbt_tpch.tpcdi.BatchDate', 'model.dbsql_dbt_tpch.DimCustomerStg']}, 'compiled_path': None}, 'model.dbsql_dbt_tpch.DimAccount': {'database': None, 'schema': 'dbt_shabbirkdb', 'name': 'DimAccount', 'resource_type': 'model', 'package_name': 'dbsql_dbt_tpch', 'path': 'incremental/DimAccount.sql', 'original_file_path': 'models/incremental/DimAccount.sql', 'unique_id': 'model.dbsql_dbt_tpch.DimAccount', 'fqn': ['dbsql_dbt_tpch', 'incremental', 'DimAccount'], 'alias': 'DimAccount', 'checksum': {'name': 'sha256', 'checksum': '59d2a929be76ecca03362a6085dd912e75c1f55308eb112fdb27cdc6bdb5d368'}, 'config': {'enabled': True, 'alias': None, 'schema': None, 'database': None, 'tags': [], 'meta': {}, 'materialized': 'table', 'incremental_strategy': None, 'persist_docs': {}, 'quoting': {}, 'column_types': {}, 'full_refresh': None, 'unique_key': None, 'on_schema_change': 'ignore', 'grants': {}, 'packages': [], 'docs': {'show': True, 'node_color': None}, 'post-hook': [], 'pre-hook': []}, 'tags': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {'materialized': 'table'}, 'created_at': 1681993370.4091167, 'config_call_dict': {'materialized': 'table'}, 'relation_name': '`dbt_shabbirkdb`.`DimAccount`', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT\n  md5(a.accountid::string) as sk_accountid,\n  a.accountid,\n  b.sk_brokerid,\n  a.sk_customerid,\n  a.accountdesc,\n  a.TaxStatus,\n  a.status,\n  a.batchid,\n  a.effectivedate,\n  a.enddate\nFROM (\n  SELECT\n    a.* except(effectivedate, enddate, customerid),\n    c.sk_customerid,\n    if(a.effectivedate < c.effectivedate, c.effectivedate, a.effectivedate) effectivedate,\n    if(a.enddate > c.enddate, c.enddate, a.enddate) enddate\n  FROM (\n    SELECT *\n    FROM (\n      SELECT\n        accountid,\n        customerid,\n        coalesce(accountdesc, last_value(accountdesc) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) accountdesc,\n        coalesce(taxstatus, last_value(taxstatus) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) taxstatus,\n        coalesce(brokerid, last_value(brokerid) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) brokerid,\n        coalesce(status, last_value(status) IGNORE NULLS OVER (\n          PARTITION BY accountid ORDER BY update_ts)) status,\n        date(update_ts) effectivedate,\n        nvl(lead(date(update_ts)) OVER (PARTITION BY accountid ORDER BY update_ts), date('9999-12-31')) enddate,\n        batchid\n      FROM (\n        SELECT\n          accountid,\n          customerid,\n          accountdesc,\n          taxstatus,\n          brokerid,\n          status,\n          update_ts,\n          1 batchid\n        FROM roberto_salcido_tpcdi_dlt_10_stage.CustomerMgmt c\n        WHERE ActionType NOT IN ('UPDCUST', 'INACT')\n        UNION ALL\n        SELECT\n          accountid,\n          a.ca_c_id customerid,\n          accountDesc,\n          TaxStatus,\n          a.ca_b_id brokerid,\n          st_name as status,\n          TIMESTAMP(bd.batchdate) update_ts,\n          a.batchid\n        FROM {{ source('tpcdi', 'AccountIncremental') }} a\n        JOIN {{ source('tpcdi', 'BatchDate') }} bd\n          ON a.batchid = bd.batchid\n        JOIN {{ source('tpcdi', 'StatusType') }} st \n          ON a.CA_ST_ID = st.st_id\n      ) a\n    ) a\n    WHERE a.effectivedate < a.enddate\n  ) a\n  FULL OUTER JOIN {{ ref('DimCustomerStg') }} c \n    ON \n      a.customerid = c.customerid\n      AND c.enddate > a.effectivedate\n      AND c.effectivedate < a.enddate\n) a\nLEFT JOIN {{ ref('DimBroker') }} b \n  ON a.brokerid = b.brokerid;", 'language': 'sql', 'refs': [['DimCustomerStg'], ['DimBroker']], 'sources': [['tpcdi', 'AccountIncremental'], ['tpcdi', 'StatusType'], ['tpcdi', 'BatchDate']], 'metrics': [], 'depends_on': {'macros': [], 'nodes': ['source.dbsql_dbt_tpch.tpcdi.AccountIncremental', 'source.dbsql_dbt_tpch.tpcdi.StatusType', 'source.dbsql_dbt_tpch.tpcdi.BatchDate', 'model.dbsql_dbt_tpch.DimCustomerStg', 'model.dbsql_dbt_tpch.DimBroker']}, 'compiled_path': None}, 'model.dbsql_dbt_tpch.tempDailyMarketHistorical': {'database': None, 'schema': 'dbt_shabbirkdb', 'name': 'tempDailyMarketHistorical', 'resource_type': 'model', 'package_name': 'dbsql_dbt_tpch', 'path': 'incremental/tempDailyMarketHistorical.sql', 'original_file_path': 'models/incremental/tempDailyMarketHistorical.sql', 'unique_id': 'model.dbsql_dbt_tpch.tempDailyMarketHistorical', 'fqn': ['dbsql_dbt_tpch', 'incremental', 'tempDailyMarketHistorical'], 'alias': 'tempDailyMarketHistorical', 'checksum': {'name': 'sha256', 'checksum': '6550ea02ae94f85b8362f2a2686f4df6aa96bc27e4e02cb360878c2706a3e1a4'}, 'config': {'enabled': True, 'alias': None, 'schema': None, 'database': None, 'tags': [], 'meta': {}, 'materialized': 'table', 'incremental_strategy': None, 'persist_docs': {}, 'quoting': {}, 'column_types': {}, 'full_refresh': None, 'unique_key': None, 'on_schema_change': 'ignore', 'grants': {}, 'packages': [], 'docs': {'show': True, 'node_color': None}, 'post-hook': [], 'pre-hook': []}, 'tags': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {'materialized': 'table'}, 'created_at': 1681993370.412445, 'config_call_dict': {'materialized': 'table'}, 'relation_name': '`dbt_shabbirkdb`.`tempDailyMarketHistorical`', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT\n  dmh.*,\n  sk_dateid,\n  min(dm_low) OVER (\n    PARTITION BY dm_s_symb\n    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW\n  ) fiftytwoweeklow,\n  max(dm_high) OVER (\n    PARTITION by dm_s_symb\n    ORDER BY dm_date ASC ROWS BETWEEN 364 PRECEDING AND CURRENT ROW\n  ) fiftytwoweekhigh\nFROM (\n  SELECT * FROM {{ source('tpcdi', 'DailyMarketHistorical') }}\n  UNION ALL\n  SELECT * except(cdc_flag, cdc_dsn) FROM {{ source('tpcdi', 'DailyMarketIncremental') }}) dmh\nJOIN {{ source('tpcdi', 'DimDate') }} d \n  ON d.datevalue = dm_date;", 'language': 'sql', 'refs': [], 'sources': [['tpcdi', 'DailyMarketHistorical'], ['tpcdi', 'DailyMarketIncremental'], ['tpcdi', 'DimDate']], 'metrics': [], 'depends_on': {'macros': [], 'nodes': ['source.dbsql_dbt_tpch.tpcdi.DailyMarketHistorical', 'source.dbsql_dbt_tpch.tpcdi.DailyMarketIncremental', 'source.dbsql_dbt_tpch.tpcdi.DimDate']}, 'compiled_path': None}, 'model.dbsql_dbt_tpch.FactWatches': {'database': None, 'schema': 'dbt_shabbirkdb', 'name': 'FactWatches', 'resource_type': 'model', 'package_name': 'dbsql_dbt_tpch', 'path': 'incremental/FactWatches.sql', 'original_file_path': 'models/incremental/FactWatches.sql', 'unique_id': 'model.dbsql_dbt_tpch.FactWatches', 'fqn': ['dbsql_dbt_tpch', 'incremental', 'FactWatches'], 'alias': 'FactWatches', 'checksum': {'name': 'sha256', 'checksum': 'a84d035bc4ff3750521f3d73d3296336304ac491dcef871b42531fe33c83803d'}, 'config': {'enabled': True, 'alias': None, 'schema': None, 'database': None, 'tags': [], 'meta': {}, 'materialized': 'table', 'incremental_strategy': None, 'persist_docs': {}, 'quoting': {}, 'column_types': {}, 'full_refresh': None, 'unique_key': None, 'on_schema_change': 'ignore', 'grants': {}, 'packages': [], 'docs': {'show': True, 'node_color': None}, 'post-hook': [], 'pre-hook': []}, 'tags': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {'materialized': 'table'}, 'created_at': 1681993370.4157348, 'config_call_dict': {'materialized': 'table'}, 'relation_name': '`dbt_shabbirkdb`.`FactWatches`', 'raw_code': '{{\n    config(\n        materialized = \'table\'\n    )\n}}\nSELECT\n  c.sk_customerid sk_customerid,\n  s.sk_securityid sk_securityid,\n  sk_dateid_dateplaced,\n  sk_dateid_dateremoved,\n  wh.batchid\nFROM (\n  SELECT * EXCEPT(w_dts)\n  FROM (\n    SELECT\n      customerid,\n      symbol,\n      coalesce(sk_dateid_dateplaced, last_value(sk_dateid_dateplaced) IGNORE NULLS OVER (\n        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateplaced,\n      coalesce(sk_dateid_dateremoved, last_value(sk_dateid_dateremoved) IGNORE NULLS OVER (\n        PARTITION BY customerid, symbol ORDER BY w_dts)) sk_dateid_dateremoved,\n      coalesce(dateplaced, last_value(dateplaced) IGNORE NULLS OVER (\n        PARTITION BY customerid, symbol ORDER BY w_dts)) dateplaced,\n      w_dts,\n      coalesce(batchid, last_value(batchid) IGNORE NULLS OVER (\n        PARTITION BY customerid, symbol ORDER BY w_dts)) batchid\n    FROM ( \n      SELECT \n        wh.w_c_id customerid,\n        wh.w_s_symb symbol,\n        if(w_action = \'ACTV\', d.sk_dateid, null) sk_dateid_dateplaced,\n        if(w_action = \'CNCL\', d.sk_dateid, null) sk_dateid_dateremoved,\n        if(w_action = \'ACTV\', d.datevalue, null) dateplaced,\n        wh.w_dts,\n        batchid \n      FROM (\n        SELECT *, 1 batchid FROM {{ source(\'tpcdi\', \'WatchHistory\') }}\n        UNION ALL\n        SELECT * except(cdc_flag, cdc_dsn) FROM {{ source(\'tpcdi\', \'WatchIncremental\') }}) wh\n      JOIN {{ source(\'tpcdi\', \'DimDate\') }} d\n        ON d.datevalue = date(wh.w_dts)))\n  QUALIFY ROW_NUMBER() OVER (PARTITION BY customerid, symbol ORDER BY w_dts desc) = 1) wh\n-- Converts to LEFT JOINs if this is run as DQ EDITION. On some higher Scale Factors, a small number of Security symbols or Customer IDs "may" be missing from DimSecurity/DimCustomer, causing audit check failures. \n${dq_left_flg} JOIN {{ ref(\'DimSecurity\') }} s \n  ON \n    s.symbol = wh.symbol\n    AND wh.dateplaced >= s.effectivedate \n    AND wh.dateplaced < s.enddate\n${dq_left_flg} JOIN {{ ref(\'DimCustomer\') }} c \n  ON\n    wh.customerid = c.customerid\n    AND wh.dateplaced >= c.effectivedate \n    AND wh.dateplaced < c.enddate;', 'language': 'sql', 'refs': [['DimSecurity'], ['DimCustomer']], 'sources': [['tpcdi', 'WatchIncremental'], ['tpcdi', 'DimDate'], ['tpcdi', 'WatchHistory']], 'metrics': [], 'depends_on': {'macros': [], 'nodes': ['source.dbsql_dbt_tpch.tpcdi.WatchIncremental', 'source.dbsql_dbt_tpch.tpcdi.DimDate', 'source.dbsql_dbt_tpch.tpcdi.WatchHistory', 'model.dbsql_dbt_tpch.DimSecurity', 'model.dbsql_dbt_tpch.DimCustomer']}, 'compiled_path': None}, 'model.dbsql_dbt_tpch.DimSecurity': {'database': None, 'schema': 'dbt_shabbirkdb', 'name': 'DimSecurity', 'resource_type': 'model', 'package_name': 'dbsql_dbt_tpch', 'path': 'silver/DimSecurity.sql', 'original_file_path': 'models/silver/DimSecurity.sql', 'unique_id': 'model.dbsql_dbt_tpch.DimSecurity', 'fqn': ['dbsql_dbt_tpch', 'silver', 'DimSecurity'], 'alias': 'DimSecurity', 'checksum': {'name': 'sha256', 'checksum': '1f2649a4d4135dfe5001a46d7e07d6d0431300d639d90c46a0719267b924cec5'}, 'config': {'enabled': True, 'alias': None, 'schema': None, 'database': None, 'tags': [], 'meta': {}, 'materialized': 'table', 'incremental_strategy': None, 'persist_docs': {}, 'quoting': {}, 'column_types': {}, 'full_refresh': None, 'unique_key': None, 'on_schema_change': 'ignore', 'grants': {}, 'packages': [], 'docs': {'show': True, 'node_color': None}, 'post-hook': [], 'pre-hook': []}, 'tags': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {'materialized': 'table'}, 'created_at': 1681993370.41877, 'config_call_dict': {'materialized': 'table'}, 'relation_name': '`dbt_shabbirkdb`.`DimSecurity`', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  Symbol,\n  issue,\n  status,\n  Name,\n  exchangeid,\n  sk_companyid,\n  sharesoutstanding,\n  firsttrade,\n  firsttradeonexchange,\n  Dividend,\n  if(enddate = date('9999-12-31'), True, False) iscurrent,\n  1 batchid,\n  effectivedate,\n  enddate\nFROM (\n  SELECT \n    fws.Symbol,\n    fws.issue,\n    fws.status,\n    fws.Name,\n    fws.exchangeid,\n    dc.sk_companyid,\n    fws.sharesoutstanding,\n    fws.firsttrade,\n    fws.firsttradeonexchange,\n    fws.Dividend,\n    if(fws.effectivedate < dc.effectivedate, dc.effectivedate, fws.effectivedate) effectivedate,\n    if(fws.enddate > dc.enddate, dc.enddate, fws.enddate) enddate\n  FROM (\n    SELECT \n      fws.* except(Status, conameorcik),\n      nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik,\n      s.ST_NAME as status,\n      coalesce(\n        lead(effectivedate) OVER (\n          PARTITION BY symbol\n          ORDER BY effectivedate),\n        date('9999-12-31')\n      ) enddate\n    FROM (\n      SELECT\n        date(to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss')) AS effectivedate,\n        trim(substring(value, 19, 15)) AS Symbol,\n        trim(substring(value, 34, 6)) AS issue,\n        trim(substring(value, 40, 4)) AS Status,\n        trim(substring(value, 44, 70)) AS Name,\n        trim(substring(value, 114, 6)) AS exchangeid,\n        cast(substring(value, 120, 13) as BIGINT) AS sharesoutstanding,\n        to_date(substring(value, 133, 8), 'yyyyMMdd') AS firsttrade,\n        to_date(substring(value, 141, 8), 'yyyyMMdd') AS firsttradeonexchange,\n        cast(substring(value, 149, 12) AS DOUBLE) AS Dividend,\n        trim(substring(value, 161, 60)) AS conameorcik\n      FROM {{ source('tpcdi', 'FinWire') }}\n      WHERE rectype = 'SEC'\n      ) fws\n    JOIN {{ source('tpcdi', 'StatusType') }}s \n      ON s.ST_ID = fws.status\n    ) fws\n  JOIN (\n    SELECT \n      sk_companyid,\n      name conameorcik,\n      EffectiveDate,\n      EndDate\n    FROM {{ ref('DimCompany') }}\n    UNION ALL\n    SELECT \n      sk_companyid,\n      cast(companyid as string) conameorcik,\n      EffectiveDate,\n      EndDate\n    FROM {{ ref('DimCompany') }}\n  ) dc \n  ON\n    fws.conameorcik = dc.conameorcik \n    AND fws.EffectiveDate < dc.EndDate\n    AND fws.EndDate > dc.EffectiveDate\n) fws\nWHERE effectivedate != enddate", 'language': 'sql', 'refs': [['DimCompany'], ['DimCompany']], 'sources': [['tpcdi', 'StatusType'], ['tpcdi', 'FinWire']], 'metrics': [], 'depends_on': {'macros': [], 'nodes': ['source.dbsql_dbt_tpch.tpcdi.StatusType', 'source.dbsql_dbt_tpch.tpcdi.FinWire', 'model.dbsql_dbt_tpch.DimCompany', 'model.dbsql_dbt_tpch.DimCompany']}, 'compiled_path': None}, 'model.dbsql_dbt_tpch.DimBroker': {'database': None, 'schema': 'dbt_shabbirkdb', 'name': 'DimBroker', 'resource_type': 'model', 'package_name': 'dbsql_dbt_tpch', 'path': 'silver/DimBroker.sql', 'original_file_path': 'models/silver/DimBroker.sql', 'unique_id': 'model.dbsql_dbt_tpch.DimBroker', 'fqn': ['dbsql_dbt_tpch', 'silver', 'DimBroker'], 'alias': 'DimBroker', 'checksum': {'name': 'sha256', 'checksum': '3f0d3d4a5db506281e71f50b479b9a216fb955fbb90555a7de3568fdc3bb52e0'}, 'config': {'enabled': True, 'alias': None, 'schema': None, 'database': None, 'tags': [], 'meta': {}, 'materialized': 'table', 'incremental_strategy': None, 'persist_docs': {}, 'quoting': {}, 'column_types': {}, 'full_refresh': None, 'unique_key': None, 'on_schema_change': 'ignore', 'grants': {}, 'packages': [], 'docs': {'show': True, 'node_color': None}, 'post-hook': [], 'pre-hook': []}, 'tags': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {'materialized': 'table'}, 'created_at': 1681993370.4219124, 'config_call_dict': {'materialized': 'table'}, 'relation_name': '`dbt_shabbirkdb`.`DimBroker`', 'raw_code': "{{ config(\n  materialized='table'\n) }}\n\nSELECT\n  md5(employeeid) as sk_brokerid,\n  cast(employeeid as BIGINT) brokerid,\n  cast(managerid as BIGINT) managerid,\n  employeefirstname firstname,\n  employeelastname lastname,\n  employeemi middleinitial,\n  employeebranch branch,\n  employeeoffice office,\n  employeephone phone,\n  true iscurrent,\n  1 batchid,\n  (SELECT min(to_date(datevalue)) as effectivedate FROM {{ source('tpcdi', 'DimDate') }}) effectivedate,\n  date('9999-12-31') enddate\nFROM  {{ source('tpcdi', 'HR') }}\nWHERE employeejobcode = 314", 'language': 'sql', 'refs': [], 'sources': [['tpcdi', 'HR'], ['tpcdi', 'DimDate']], 'metrics': [], 'depends_on': {'macros': [], 'nodes': ['source.dbsql_dbt_tpch.tpcdi.HR', 'source.dbsql_dbt_tpch.tpcdi.DimDate']}, 'compiled_path': None}, 'model.dbsql_dbt_tpch.Financial': {'database': None, 'schema': 'dbt_shabbirkdb', 'name': 'Financial', 'resource_type': 'model', 'package_name': 'dbsql_dbt_tpch', 'path': 'silver/Financial.sql', 'original_file_path': 'models/silver/Financial.sql', 'unique_id': 'model.dbsql_dbt_tpch.Financial', 'fqn': ['dbsql_dbt_tpch', 'silver', 'Financial'], 'alias': 'Financial', 'checksum': {'name': 'sha256', 'checksum': 'd34bc97371dd769d1f56f539d21c63f4cdcad512b557924be6d2b8abb59b415d'}, 'config': {'enabled': True, 'alias': None, 'schema': None, 'database': None, 'tags': [], 'meta': {}, 'materialized': 'table', 'incremental_strategy': None, 'persist_docs': {}, 'quoting': {}, 'column_types': {}, 'full_refresh': None, 'unique_key': None, 'on_schema_change': 'ignore', 'grants': {}, 'packages': [], 'docs': {'show': True, 'node_color': None}, 'post-hook': [], 'pre-hook': []}, 'tags': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {'materialized': 'table'}, 'created_at': 1681993370.4246593, 'config_call_dict': {'materialized': 'table'}, 'relation_name': '`dbt_shabbirkdb`.`Financial`', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\n\nSELECT \n  sk_companyid,\n  fi_year,\n  fi_qtr,\n  fi_qtr_start_date,\n  fi_revenue,\n  fi_net_earn,\n  fi_basic_eps,\n  fi_dilut_eps,\n  fi_margin,\n  fi_inventory,\n  fi_assets,\n  fi_liability,\n  fi_out_basic,\n  fi_out_dilut\nFROM (\n  SELECT \n    * except(conameorcik),\n    nvl(string(try_cast(conameorcik as bigint)), conameorcik) conameorcik\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      cast(substring(value, 19, 4) AS INT) AS fi_year,\n      cast(substring(value, 23, 1) AS INT) AS fi_qtr,\n      to_date(substring(value, 24, 8), 'yyyyMMdd') AS fi_qtr_start_date,\n      --to_date(substring(value, 32, 8), 'yyyyMMdd') AS PostingDate,\n      cast(substring(value, 40, 17) AS DOUBLE) AS fi_revenue,\n      cast(substring(value, 57, 17) AS DOUBLE) AS fi_net_earn,\n      cast(substring(value, 74, 12) AS DOUBLE) AS fi_basic_eps,\n      cast(substring(value, 86, 12) AS DOUBLE) AS fi_dilut_eps,\n      cast(substring(value, 98, 12) AS DOUBLE) AS fi_margin,\n      cast(substring(value, 110, 17) AS DOUBLE) AS fi_inventory,\n      cast(substring(value, 127, 17) AS DOUBLE) AS fi_assets,\n      cast(substring(value, 144, 17) AS DOUBLE) AS fi_liability,\n      cast(substring(value, 161, 13) AS BIGINT) AS fi_out_basic,\n      cast(substring(value, 174, 13) AS BIGINT) AS fi_out_dilut,\n      trim(substring(value, 187, 60)) AS conameorcik\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'FIN'\n  ) f \n) f\nJOIN (\n  SELECT \n    sk_companyid,\n    name conameorcik,\n    EffectiveDate,\n    EndDate\n  FROM {{ ref('DimCompany') }}\n  UNION ALL\n  SELECT \n    sk_companyid,\n    cast(companyid as string) conameorcik,\n    EffectiveDate,\n    EndDate\n  FROM {{ ref('DimCompany') }}\n) dc \nON\n  f.conameorcik = dc.conameorcik \n  AND date(PTS) >= dc.effectivedate \n  AND date(PTS) < dc.enddate", 'language': 'sql', 'refs': [['DimCompany'], ['DimCompany']], 'sources': [['tpcdi', 'FinWire']], 'metrics': [], 'depends_on': {'macros': [], 'nodes': ['source.dbsql_dbt_tpch.tpcdi.FinWire', 'model.dbsql_dbt_tpch.DimCompany', 'model.dbsql_dbt_tpch.DimCompany']}, 'compiled_path': None}, 'model.dbsql_dbt_tpch.DimCompany': {'database': None, 'schema': 'dbt_shabbirkdb', 'name': 'DimCompany', 'resource_type': 'model', 'package_name': 'dbsql_dbt_tpch', 'path': 'silver/DimCompany.sql', 'original_file_path': 'models/silver/DimCompany.sql', 'unique_id': 'model.dbsql_dbt_tpch.DimCompany', 'fqn': ['dbsql_dbt_tpch', 'silver', 'DimCompany'], 'alias': 'DimCompany', 'checksum': {'name': 'sha256', 'checksum': 'df98871ae1749967e0c1f8dbe6926183fc091762fbaeb581e363bf117207cf99'}, 'config': {'enabled': True, 'alias': None, 'schema': None, 'database': None, 'tags': [], 'meta': {}, 'materialized': 'table', 'incremental_strategy': None, 'persist_docs': {}, 'quoting': {}, 'column_types': {}, 'full_refresh': None, 'unique_key': None, 'on_schema_change': 'ignore', 'grants': {}, 'packages': [], 'docs': {'show': True, 'node_color': None}, 'post-hook': [], 'pre-hook': []}, 'tags': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {'materialized': 'table'}, 'created_at': 1681993370.4277713, 'config_call_dict': {'materialized': 'table'}, 'relation_name': '`dbt_shabbirkdb`.`DimCompany`', 'raw_code': "{{\n    config(\n        materialized = 'table'\n    )\n}}\nSELECT \n  * FROM (\n  SELECT\n    md5(cik) sk_companyid,\n    cast(cik as BIGINT) companyid,\n    st.st_name status,\n    companyname name,\n    ind.in_name industry,\n    if(\n      SPrating IN ('AAA','AA','AA+','AA-','A','A+','A-','BBB','BBB+','BBB-','BB','BB+','BB-','B','B+','B-','CCC','CCC+','CCC-','CC','C','D'), \n      SPrating, \n      cast(null as string)) sprating, \n    CASE\n      WHEN SPrating IN ('AAA','AA','A','AA+','A+','AA-','A-','BBB','BBB+','BBB-') THEN false\n      WHEN SPrating IN ('BB','B','CCC','CC','C','D','BB+','B+','CCC+','BB-','B-','CCC-') THEN true\n      ELSE cast(null as boolean)\n      END as islowgrade, \n    ceoname ceo,\n    addrline1 addressline1,\n    addrline2 addressline2,\n    postalcode,\n    city,\n    stateprovince stateprov,\n    country,\n    description,\n    foundingdate,\n    nvl2(lead(pts) OVER (PARTITION BY cik ORDER BY pts), true, false) iscurrent,\n    1 batchid,\n    date(pts) effectivedate,\n    coalesce(\n      lead(date(pts)) OVER (PARTITION BY cik ORDER BY pts),\n      cast('9999-12-31' as date)) enddate\n  FROM (\n    SELECT\n      to_timestamp(substring(value, 1, 15), 'yyyyMMdd-HHmmss') AS PTS,\n      trim(substring(value, 19, 60)) AS CompanyName,\n      trim(substring(value, 79, 10)) AS CIK,\n      trim(substring(value, 89, 4)) AS Status,\n      trim(substring(value, 93, 2)) AS IndustryID,\n      trim(substring(value, 95, 4)) AS SPrating,\n      to_date(iff(trim(substring(value, 99, 8))='',NULL,substring(value, 99, 8)), 'yyyyMMdd') AS FoundingDate,\n      trim(substring(value, 107, 80)) AS AddrLine1,\n      trim(substring(value, 187, 80)) AS AddrLine2,\n      trim(substring(value, 267, 12)) AS PostalCode,\n      trim(substring(value, 279, 25)) AS City,\n      trim(substring(value, 304, 20)) AS StateProvince,\n      trim(substring(value, 324, 24)) AS Country,\n      trim(substring(value, 348, 46)) AS CEOname,\n      trim(substring(value, 394, 150)) AS Description\n    FROM {{ source('tpcdi', 'FinWire') }}\n    WHERE rectype = 'CMP'\n       ) cmp\n  JOIN {{ source('tpcdi', 'StatusType') }} st ON cmp.status = st.st_id\n  JOIN {{ source('tpcdi', 'Industry') }} ind ON cmp.industryid = ind.in_id\n)", 'language': 'sql', 'refs': [], 'sources': [['tpcdi', 'Industry'], ['tpcdi', 'StatusType'], ['tpcdi', 'FinWire']], 'metrics': [], 'depends_on': {'macros': [], 'nodes': ['source.dbsql_dbt_tpch.tpcdi.Industry', 'source.dbsql_dbt_tpch.tpcdi.StatusType', 'source.dbsql_dbt_tpch.tpcdi.FinWire']}, 'compiled_path': None}, 'test.dbsql_dbt_tpch.test': {'database': None, 'schema': 'dbt_shabbirkdb_dbt_test__audit', 'name': 'test', 'resource_type': 'test', 'package_name': 'dbsql_dbt_tpch', 'path': 'test.sql', 'original_file_path': 'tests/test.sql', 'unique_id': 'test.dbsql_dbt_tpch.test', 'fqn': ['dbsql_dbt_tpch', 'test'], 'alias': 'test', 'checksum': {'name': 'sha256', 'checksum': '01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b'}, 'config': {'enabled': True, 'alias': None, 'schema': 'dbt_test__audit', 'database': None, 'tags': [], 'meta': {}, 'materialized': 'test', 'severity': 'ERROR', 'store_failures': None, 'where': None, 'limit': None, 'fail_calc': 'count(*)', 'warn_if': '!= 0', 'error_if': '!= 0'}, 'tags': [], 'description': '', 'columns': {}, 'meta': {}, 'docs': {'show': True, 'node_color': None}, 'patch_path': None, 'build_path': None, 'deferred': False, 'unrendered_config': {}, 'created_at': 1681993370.4429424, 'config_call_dict': {}, 'relation_name': None, 'raw_code': '', 'language': 'sql', 'refs': [], 'sources': [], 'metrics': [], 'depends_on': {'macros': [], 'nodes': []}, 'compiled_path': None}}
[0m15:14:07.220706 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '4685bf26-5b53-4939-b600-62840b00e86c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13f1dbcd0>]}
[0m15:14:08.048930 [debug] [MainThread]: 1699: static parser successfully parsed incremental/FactWatches.sql
[0m15:14:08.056841 [debug] [MainThread]: 1699: static parser successfully parsed incremental/Prospect.sql
[0m15:14:08.058943 [debug] [MainThread]: 1699: static parser successfully parsed incremental/FactCashBalances.sql
[0m15:14:08.061225 [debug] [MainThread]: 1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
[0m15:14:08.063416 [debug] [MainThread]: 1699: static parser successfully parsed incremental/FactHoldings.sql
[0m15:14:08.065420 [debug] [MainThread]: 1699: static parser successfully parsed incremental/FactMarketHistory.sql
[0m15:14:08.067811 [debug] [MainThread]: 1699: static parser successfully parsed incremental/DimCustomer.sql
[0m15:14:08.069895 [debug] [MainThread]: 1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
[0m15:14:08.072228 [debug] [MainThread]: 1699: static parser successfully parsed incremental/DimTrade.sql
[0m15:14:08.074270 [debug] [MainThread]: 1699: static parser successfully parsed incremental/DimAccount.sql
[0m15:14:08.076415 [debug] [MainThread]: 1699: static parser successfully parsed incremental/DimCustomerStg.sql
[0m15:14:08.078520 [debug] [MainThread]: 1699: static parser successfully parsed silver/DimCompany.sql
[0m15:14:08.080663 [debug] [MainThread]: 1699: static parser successfully parsed silver/DimBroker.sql
[0m15:14:08.082808 [debug] [MainThread]: 1699: static parser successfully parsed silver/Financial.sql
[0m15:14:08.085254 [debug] [MainThread]: 1699: static parser successfully parsed silver/DimSecurity.sql
[0m15:14:08.087310 [debug] [MainThread]: 1699: static parser successfully parsed base/CustomerIncremental.sql
[0m15:14:08.089180 [debug] [MainThread]: 1699: static parser successfully parsed base/CashTransactionIncremental.sql
[0m15:14:08.091138 [debug] [MainThread]: 1603: static parser failed on base/CustomerMgmtView.sql
[0m15:14:08.094145 [debug] [MainThread]: 1602: parser fallback to jinja rendering on base/CustomerMgmtView.sql
[0m15:14:08.095313 [debug] [MainThread]: 1699: static parser successfully parsed base/TradeIncremental.sql
[0m15:14:08.097263 [debug] [MainThread]: 1699: static parser successfully parsed base/HoldingIncremental.sql
[0m15:14:08.099206 [debug] [MainThread]: 1699: static parser successfully parsed base/DailyMarketIncremental.sql
[0m15:14:08.101029 [debug] [MainThread]: 1699: static parser successfully parsed base/BatchDate.sql
[0m15:14:08.103042 [debug] [MainThread]: 1699: static parser successfully parsed base/AccountIncremental.sql
[0m15:14:08.105207 [debug] [MainThread]: 1603: static parser failed on base/FinWire.sql
[0m15:14:08.108702 [debug] [MainThread]: 1602: parser fallback to jinja rendering on base/FinWire.sql
[0m15:14:08.109803 [debug] [MainThread]: 1699: static parser successfully parsed base/WatchIncremental.sql
[0m15:14:08.112024 [debug] [MainThread]: 1699: static parser successfully parsed base/ProspectRaw.sql
[0m15:14:08.114036 [debug] [MainThread]: 1699: static parser successfully parsed base/DailyMarketHistorical.sql
[0m15:14:08.353683 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4685bf26-5b53-4939-b600-62840b00e86c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13fa69d00>]}
[0m15:14:08.399963 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4685bf26-5b53-4939-b600-62840b00e86c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13f52e100>]}
[0m15:14:08.400293 [info ] [MainThread]: Found 27 models, 18 tests, 0 snapshots, 0 analyses, 681 macros, 0 operations, 0 seed files, 33 sources, 0 exposures, 0 metrics, 0 groups
[0m15:14:08.400513 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4685bf26-5b53-4939-b600-62840b00e86c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13f52e4f0>]}
[0m15:14:08.400950 [debug] [MainThread]: Acquiring new databricks connection 'macro_stage_external_sources'
[0m15:14:08.401128 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:14:08.401263 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:14:08.411649 [info ] [MainThread]: 1 of 33 START external source rsstgtest10.StatusType
[0m15:14:08.415155 [debug] [MainThread]: On "macro_stage_external_sources": cache miss for schema ".rsstgtest10", this is inefficient
[0m15:14:08.422691 [debug] [MainThread]: Using databricks connection "macro_stage_external_sources"
[0m15:14:08.422888 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */
show tables in `rsstgtest10`
  
[0m15:14:08.423040 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:14:09.067433 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.RequestError'>: Error during request to server: : Invalid access token.
[0m15:14:09.067759 [debug] [MainThread]: Databricks adapter: attempt: 1/30
[0m15:14:09.067910 [debug] [MainThread]: Databricks adapter: bounded-retry-delay: None
[0m15:14:09.068045 [debug] [MainThread]: Databricks adapter: elapsed-seconds: 0.6307697296142578/900.0
[0m15:14:09.068169 [debug] [MainThread]: Databricks adapter: error-message: : Invalid access token.
[0m15:14:09.068289 [debug] [MainThread]: Databricks adapter: http-code: 403
[0m15:14:09.068407 [debug] [MainThread]: Databricks adapter: method: OpenSession
[0m15:14:09.068531 [debug] [MainThread]: Databricks adapter: no-retry-reason: non-retryable error
[0m15:14:09.068650 [debug] [MainThread]: Databricks adapter: original-exception: 
[0m15:14:09.068768 [debug] [MainThread]: Databricks adapter: query-id: None
[0m15:14:09.068886 [debug] [MainThread]: Databricks adapter: session-id: None
[0m15:14:09.069057 [debug] [MainThread]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */
show tables in `rsstgtest10`
  
[0m15:14:09.069200 [debug] [MainThread]: Databricks adapter: Database Error
  Error during request to server: : Invalid access token.
[0m15:14:09.069387 [debug] [MainThread]: Databricks adapter: Error while running:
macro show_tables
[0m15:14:09.069517 [debug] [MainThread]: Databricks adapter: Runtime Error
  Database Error
    Error during request to server: : Invalid access token.
[0m15:14:09.069700 [debug] [MainThread]: Databricks adapter: Error while running:
macro list_relations_without_caching
[0m15:14:09.069829 [debug] [MainThread]: Databricks adapter: Runtime Error
  Runtime Error
    Database Error
      Error during request to server: : Invalid access token.
[0m15:14:09.070105 [debug] [MainThread]: Databricks adapter: Error while retrieving information about `rsstgtest10`: Runtime Error
  Runtime Error
    Database Error
      Error during request to server: : Invalid access token.
[0m15:14:09.070285 [debug] [MainThread]: Databricks adapter: Error while running:
macro stage_external_sources
[0m15:14:09.070415 [debug] [MainThread]: Databricks adapter: Runtime Error
  Runtime Error
    Runtime Error
      Database Error
        Error during request to server: : Invalid access token.
[0m15:14:09.070596 [debug] [MainThread]: On macro_stage_external_sources: No close available on handle
[0m15:14:09.070943 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  Runtime Error
    Runtime Error
      Runtime Error
        Database Error
          Error during request to server: : Invalid access token.
[0m15:14:09.079638 [debug] [MainThread]: Traceback (most recent call last):
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/connections.py", line 241, in retry_connection
    connection.handle = connect()
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 809, in connect
    conn: DatabricksSQLConnection = dbsql.connect(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/__init__.py", line 50, in connect
    return Connection(server_hostname, http_path, access_token, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/client.py", line 189, in __init__
    self._session_handle = self.thrift_backend.open_session(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 483, in open_session
    response = self.make_request(self._client.OpenSession, open_session_req)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 412, in make_request
    self._handle_request_error(error_info, attempt, elapsed)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 264, in _handle_request_error
    raise network_request_error
databricks.sql.exc.RequestError: Error during request to server: : Invalid access token.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 660, in exception_handler
    yield
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 704, in add_query
    cursor = cast(DatabricksSQLConnectionWrapper, connection.handle).cursor()
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/contracts/connection.py", line 94, in handle
    self._handle.resolve(self)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/contracts/connection.py", line 118, in resolve
    return self.opener(connection)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 838, in open
    return cls.retry_connection(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/connections.py", line 271, in retry_connection
    raise dbt.exceptions.FailedToConnectError(str(e))
dbt.exceptions.FailedToConnectError: Database Error
  Error during request to server: : Invalid access token.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 660, in exception_handler
    yield
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 1043, in execute_macro
    result = macro_function(**kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 21, in macro
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 298, in call
    return __obj(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 33, in macro
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 298, in call
    return __obj(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 52, in macro
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 298, in call
    return __obj(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/impl.py", line 131, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 289, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 728, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 722, in add_query
    cursor.close()
  File "/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 678, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  Database Error
    Error during request to server: : Invalid access token.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 660, in exception_handler
    yield
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 1043, in execute_macro
    result = macro_function(**kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 21, in macro
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 298, in call
    return __obj(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 21, in macro
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 298, in call
    return __obj(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/impl.py", line 222, in get_relations_without_caching
    tables = self.execute_macro(SHOW_TABLES_MACRO_NAME, kwargs=kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 1043, in execute_macro
    result = macro_function(**kwargs)
  File "/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 678, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  Runtime Error
    Database Error
      Error during request to server: : Invalid access token.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 660, in exception_handler
    yield
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 1043, in execute_macro
    result = macro_function(**kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 165, in macro
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 298, in call
    return __obj(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 21, in macro
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 298, in call
    return __obj(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 35, in macro
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 298, in call
    return __obj(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/impl.py", line 286, in get_relation
    cached: Optional[DatabricksRelation] = super(SparkAdapter, self).get_relation(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 800, in get_relation
    relations_list = self.list_relations(database, schema)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 738, in list_relations
    relations = self.list_relations_without_caching(schema_relation)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/impl.py", line 149, in list_relations_without_caching
    raise e
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/impl.py", line 141, in list_relations_without_caching
    results = self.execute_macro(LIST_RELATIONS_MACRO_NAME, kwargs=kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 1043, in execute_macro
    result = macro_function(**kwargs)
  File "/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 678, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  Runtime Error
    Runtime Error
      Database Error
        Error during request to server: : Invalid access token.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/task/run_operation.py", line 47, in run
    self._run_unsafe()
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/task/run_operation.py", line 37, in _run_unsafe
    res = adapter.execute_macro(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 1043, in execute_macro
    result = macro_function(**kwargs)
  File "/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 678, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  Runtime Error
    Runtime Error
      Runtime Error
        Database Error
          Error during request to server: : Invalid access token.

[0m15:14:09.081032 [debug] [MainThread]: Command `dbt run-operation` failed at 15:14:09.080957 after 2.77 seconds
[0m15:14:09.081223 [debug] [MainThread]: Connection 'macro_stage_external_sources' was properly closed.
[0m15:14:09.081390 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104748730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13f4f1ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13f4f1a90>]}
[0m15:14:09.081587 [debug] [MainThread]: Flushing usage events
[0m15:14:40.496591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106524220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10858fdf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10858fe80>]}


============================== 15:14:40.517079 | 3617d52c-f77e-4e9c-a5b9-38540ce49153 ==============================
[0m15:14:40.517079 [info ] [MainThread]: Running with dbt=1.5.0
[0m15:14:40.517403 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:14:41.152554 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3617d52c-f77e-4e9c-a5b9-38540ce49153', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10444a550>]}
[0m15:14:41.160562 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3617d52c-f77e-4e9c-a5b9-38540ce49153', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f546400>]}
[0m15:14:41.184176 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m15:14:41.198530 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m15:14:41.198863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '3617d52c-f77e-4e9c-a5b9-38540ce49153', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f5638b0>]}
[0m15:14:42.040310 [debug] [MainThread]: 1699: static parser successfully parsed incremental/FactWatches.sql
[0m15:14:42.047172 [debug] [MainThread]: 1699: static parser successfully parsed incremental/Prospect.sql
[0m15:14:42.049103 [debug] [MainThread]: 1699: static parser successfully parsed incremental/FactCashBalances.sql
[0m15:14:42.050918 [debug] [MainThread]: 1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
[0m15:14:42.052676 [debug] [MainThread]: 1699: static parser successfully parsed incremental/FactHoldings.sql
[0m15:14:42.054564 [debug] [MainThread]: 1699: static parser successfully parsed incremental/FactMarketHistory.sql
[0m15:14:42.056442 [debug] [MainThread]: 1699: static parser successfully parsed incremental/DimCustomer.sql
[0m15:14:42.058161 [debug] [MainThread]: 1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
[0m15:14:42.060066 [debug] [MainThread]: 1699: static parser successfully parsed incremental/DimTrade.sql
[0m15:14:42.062205 [debug] [MainThread]: 1699: static parser successfully parsed incremental/DimAccount.sql
[0m15:14:42.064164 [debug] [MainThread]: 1699: static parser successfully parsed incremental/DimCustomerStg.sql
[0m15:14:42.066412 [debug] [MainThread]: 1699: static parser successfully parsed silver/DimCompany.sql
[0m15:14:42.068463 [debug] [MainThread]: 1699: static parser successfully parsed silver/DimBroker.sql
[0m15:14:42.070272 [debug] [MainThread]: 1699: static parser successfully parsed silver/Financial.sql
[0m15:14:42.072366 [debug] [MainThread]: 1699: static parser successfully parsed silver/DimSecurity.sql
[0m15:14:42.074416 [debug] [MainThread]: 1699: static parser successfully parsed base/CustomerIncremental.sql
[0m15:14:42.076088 [debug] [MainThread]: 1699: static parser successfully parsed base/CashTransactionIncremental.sql
[0m15:14:42.077704 [debug] [MainThread]: 1603: static parser failed on base/CustomerMgmtView.sql
[0m15:14:42.080639 [debug] [MainThread]: 1602: parser fallback to jinja rendering on base/CustomerMgmtView.sql
[0m15:14:42.081629 [debug] [MainThread]: 1699: static parser successfully parsed base/TradeIncremental.sql
[0m15:14:42.083290 [debug] [MainThread]: 1699: static parser successfully parsed base/HoldingIncremental.sql
[0m15:14:42.085441 [debug] [MainThread]: 1699: static parser successfully parsed base/DailyMarketIncremental.sql
[0m15:14:42.087122 [debug] [MainThread]: 1699: static parser successfully parsed base/BatchDate.sql
[0m15:14:42.088751 [debug] [MainThread]: 1699: static parser successfully parsed base/AccountIncremental.sql
[0m15:14:42.090494 [debug] [MainThread]: 1603: static parser failed on base/FinWire.sql
[0m15:14:42.093102 [debug] [MainThread]: 1602: parser fallback to jinja rendering on base/FinWire.sql
[0m15:14:42.094051 [debug] [MainThread]: 1699: static parser successfully parsed base/WatchIncremental.sql
[0m15:14:42.095758 [debug] [MainThread]: 1699: static parser successfully parsed base/ProspectRaw.sql
[0m15:14:42.097534 [debug] [MainThread]: 1699: static parser successfully parsed base/DailyMarketHistorical.sql
[0m15:14:42.363265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3617d52c-f77e-4e9c-a5b9-38540ce49153', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f9d1d00>]}
[0m15:14:42.374696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3617d52c-f77e-4e9c-a5b9-38540ce49153', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f5d87f0>]}
[0m15:14:42.374987 [info ] [MainThread]: Found 27 models, 18 tests, 0 snapshots, 0 analyses, 681 macros, 0 operations, 0 seed files, 33 sources, 0 exposures, 0 metrics, 0 groups
[0m15:14:42.375206 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3617d52c-f77e-4e9c-a5b9-38540ce49153', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f574f40>]}
[0m15:14:42.375622 [debug] [MainThread]: Acquiring new databricks connection 'macro_stage_external_sources'
[0m15:14:42.375796 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:14:42.375935 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:14:42.385561 [info ] [MainThread]: 1 of 33 START external source rsstgtest10.StatusType
[0m15:14:42.389297 [debug] [MainThread]: On "macro_stage_external_sources": cache miss for schema ".rsstgtest10", this is inefficient
[0m15:14:42.396982 [debug] [MainThread]: Using databricks connection "macro_stage_external_sources"
[0m15:14:42.397180 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */
show tables in `rsstgtest10`
  
[0m15:14:42.397329 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:14:43.097032 [debug] [MainThread]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */
show tables in `rsstgtest10`
  
[0m15:14:43.097939 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [SCHEMA_NOT_FOUND] The schema `rsstgtest10` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.
To tolerate the error on drop use DROP SCHEMA IF EXISTS.
[0m15:14:43.098886 [debug] [MainThread]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [SCHEMA_NOT_FOUND] org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException: [SCHEMA_NOT_FOUND] The schema `rsstgtest10` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.
To tolerate the error on drop use DROP SCHEMA IF EXISTS.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException: [SCHEMA_NOT_FOUND] The schema `rsstgtest10` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.
To tolerate the error on drop use DROP SCHEMA IF EXISTS.
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.requireScExists(ManagedCatalogSessionCatalog.scala:276)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.doListTables(ManagedCatalogSessionCatalog.scala:1491)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.listTables(ManagedCatalogSessionCatalog.scala:1558)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.$anonfun$listTables$1(UnityCatalogV2Proxy.scala:192)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.assertSingleNamespace(UnityCatalogV2Proxy.scala:107)
	at com.databricks.sql.managedcatalog.UnityCatalogV2Proxy.listTables(UnityCatalogV2Proxy.scala:191)
	at org.apache.spark.sql.execution.datasources.v2.ShowTablesExec.run(ShowTablesExec.scala:42)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:246)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:246)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:227)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:410)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:172)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1040)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:122)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:360)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:245)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:223)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:241)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:229)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:316)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:312)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:229)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:229)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:183)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:174)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:253)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:499)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:520)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:581)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:685)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency(QueryResultCache.scala:149)
	at org.apache.spark.sql.execution.qrc.CacheEventLogger.recordLatency$(QueryResultCache.scala:145)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.recordLatency(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:581)
	... 21 more

[0m15:14:43.099788 [debug] [MainThread]: Databricks adapter: operation-id: b'\x01\xee\x0e\xdd\xedr\x17\xdf\xb5\xdc\x18w8\xc4\xd9\xc1'
[0m15:14:43.100208 [debug] [MainThread]: Databricks adapter: Error while running:
macro show_tables
[0m15:14:43.100513 [debug] [MainThread]: Databricks adapter: Runtime Error
  [SCHEMA_NOT_FOUND] The schema `rsstgtest10` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.
  To tolerate the error on drop use DROP SCHEMA IF EXISTS.
[0m15:14:43.100904 [debug] [MainThread]: Databricks adapter: Error while running:
macro list_relations_without_caching
[0m15:14:43.101217 [debug] [MainThread]: Databricks adapter: Runtime Error
  Runtime Error
    [SCHEMA_NOT_FOUND] The schema `rsstgtest10` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.
    To tolerate the error on drop use DROP SCHEMA IF EXISTS.
[0m15:14:43.101863 [debug] [MainThread]: While listing relations in database=, schema=rsstgtest10, found: 
[0m15:14:43.116458 [info ] [MainThread]: 1 of 33 (1) drop table if exists `hive_metastore`.`rsstgtest10`.`StatusType`
[0m15:14:43.118600 [debug] [MainThread]: Using databricks connection "macro_stage_external_sources"
[0m15:14:43.118863 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
        drop table if exists `hive_metastore`.`rsstgtest10`.`StatusType`
    

            
[0m15:14:43.470775 [debug] [MainThread]: SQL status: OK in 0.3499999940395355 seconds
[0m15:14:43.480315 [info ] [MainThread]: 1 of 33 (1) OK
[0m15:14:43.480827 [info ] [MainThread]: 1 of 33 (2) create table `hive_metastore`.`rsstgtest10`.`StatusType` (                    st...  
[0m15:14:43.481711 [debug] [MainThread]: Using databricks connection "macro_stage_external_sources"
[0m15:14:43.482046 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table `hive_metastore`.`rsstgtest10`.`StatusType` (
        
            st_id string,
            st_name string
    )  using csv
    options ('sep' = '|', 
'header' = 'false')
    
    
    
    location '/FileStore/tables/Batch1/StatusType.txt'
    

            
[0m15:14:43.744788 [debug] [MainThread]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table `hive_metastore`.`rsstgtest10`.`StatusType` (
        
            st_id string,
            st_name string
    )  using csv
    options ('sep' = '|', 
'header' = 'false')
    
    
    
    location '/FileStore/tables/Batch1/StatusType.txt'
    

            
[0m15:14:43.745777 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [SCHEMA_NOT_FOUND] The schema `rsstgtest10` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.
To tolerate the error on drop use DROP SCHEMA IF EXISTS.
[0m15:14:43.746466 [debug] [MainThread]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [SCHEMA_NOT_FOUND] org.apache.spark.sql.AnalysisException: [SCHEMA_NOT_FOUND] The schema `rsstgtest10` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.
To tolerate the error on drop use DROP SCHEMA IF EXISTS.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [SCHEMA_NOT_FOUND] The schema `rsstgtest10` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.
To tolerate the error on drop use DROP SCHEMA IF EXISTS.
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:14:43.747115 [debug] [MainThread]: Databricks adapter: operation-id: b'\x01\xee\x0e\xdd\xed\xde\x17\x1e\xb9%\xa95jV\xb3\x99'
[0m15:14:43.747617 [debug] [MainThread]: Databricks adapter: Error while running:
macro stage_external_sources
[0m15:14:43.747999 [debug] [MainThread]: Databricks adapter: Runtime Error
  [SCHEMA_NOT_FOUND] The schema `rsstgtest10` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.
  To tolerate the error on drop use DROP SCHEMA IF EXISTS.
[0m15:14:43.748485 [debug] [MainThread]: On macro_stage_external_sources: ROLLBACK
[0m15:14:43.748804 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:14:43.749125 [debug] [MainThread]: On macro_stage_external_sources: Close
[0m15:14:43.815669 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  Runtime Error
    [SCHEMA_NOT_FOUND] The schema `rsstgtest10` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.
    To tolerate the error on drop use DROP SCHEMA IF EXISTS.
[0m15:14:43.821352 [debug] [MainThread]: Traceback (most recent call last):
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 660, in exception_handler
    yield
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 705, in add_query
    cursor.execute(sql, bindings)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 477, in execute
    self._cursor.execute(sql, bindings)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 852, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 944, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 786, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 503, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: [SCHEMA_NOT_FOUND] The schema `rsstgtest10` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.
To tolerate the error on drop use DROP SCHEMA IF EXISTS.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 660, in exception_handler
    yield
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 1043, in execute_macro
    result = macro_function(**kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 220, in macro
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 298, in call
    return __obj(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 52, in macro
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 298, in call
    return __obj(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/impl.py", line 131, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 289, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 728, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 722, in add_query
    cursor.close()
  File "/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 665, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  [SCHEMA_NOT_FOUND] The schema `rsstgtest10` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.
  To tolerate the error on drop use DROP SCHEMA IF EXISTS.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/task/run_operation.py", line 47, in run
    self._run_unsafe()
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/task/run_operation.py", line 37, in _run_unsafe
    res = adapter.execute_macro(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 1043, in execute_macro
    result = macro_function(**kwargs)
  File "/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 678, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  Runtime Error
    [SCHEMA_NOT_FOUND] The schema `rsstgtest10` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.
    To tolerate the error on drop use DROP SCHEMA IF EXISTS.

[0m15:14:43.823373 [debug] [MainThread]: Command `dbt run-operation` failed at 15:14:43.823223 after 3.34 seconds
[0m15:14:43.823821 [debug] [MainThread]: Connection 'macro_stage_external_sources' was properly closed.
[0m15:14:43.824222 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106524220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f8149d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f66de50>]}
[0m15:14:43.824700 [debug] [MainThread]: Flushing usage events
[0m15:15:51.832430 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102823820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103f83d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103f83d00>]}


============================== 15:15:51.853436 | 9ab29b51-f2e1-4bee-90f4-e5e22897cea3 ==============================
[0m15:15:51.853436 [info ] [MainThread]: Running with dbt=1.5.0
[0m15:15:51.853747 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:15:52.510264 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9ab29b51-f2e1-4bee-90f4-e5e22897cea3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1002f77c0>]}
[0m15:15:52.518281 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9ab29b51-f2e1-4bee-90f4-e5e22897cea3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16bf05310>]}
[0m15:15:52.542439 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m15:15:52.651639 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:15:52.654077 [debug] [MainThread]: Partial parsing: updated file: dbsql_dbt_tpch://models/sources.yml
[0m15:15:52.667822 [debug] [MainThread]: 1699: static parser successfully parsed incremental/FactHoldings.sql
[0m15:15:52.676261 [debug] [MainThread]: 1699: static parser successfully parsed incremental/DimTrade.sql
[0m15:15:52.679316 [debug] [MainThread]: 1699: static parser successfully parsed incremental/FactCashBalances.sql
[0m15:15:52.681491 [debug] [MainThread]: 1699: static parser successfully parsed incremental/DimAccount.sql
[0m15:15:52.683573 [debug] [MainThread]: 1699: static parser successfully parsed incremental/FactMarketHistory.sql
[0m15:15:52.685670 [debug] [MainThread]: 1699: static parser successfully parsed incremental/FactWatches.sql
[0m15:15:52.687662 [debug] [MainThread]: 1699: static parser successfully parsed silver/DimSecurity.sql
[0m15:15:52.689538 [debug] [MainThread]: 1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
[0m15:15:52.691876 [debug] [MainThread]: 1699: static parser successfully parsed silver/Financial.sql
[0m15:15:52.694104 [debug] [MainThread]: 1699: static parser successfully parsed silver/DimCompany.sql
[0m15:15:52.696170 [debug] [MainThread]: 1699: static parser successfully parsed incremental/DimCustomer.sql
[0m15:15:52.698392 [debug] [MainThread]: 1699: static parser successfully parsed incremental/Prospect.sql
[0m15:15:52.700502 [debug] [MainThread]: 1699: static parser successfully parsed incremental/DimCustomerStg.sql
[0m15:15:52.702464 [debug] [MainThread]: 1699: static parser successfully parsed base/AccountIncremental.sql
[0m15:15:52.704518 [debug] [MainThread]: 1699: static parser successfully parsed base/CashTransactionIncremental.sql
[0m15:15:52.706431 [debug] [MainThread]: 1699: static parser successfully parsed base/CustomerIncremental.sql
[0m15:15:52.708458 [debug] [MainThread]: 1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
[0m15:15:52.710791 [debug] [MainThread]: 1699: static parser successfully parsed base/DailyMarketHistorical.sql
[0m15:15:52.713003 [debug] [MainThread]: 1699: static parser successfully parsed base/DailyMarketIncremental.sql
[0m15:15:52.715044 [debug] [MainThread]: 1699: static parser successfully parsed silver/DimBroker.sql
[0m15:15:52.717044 [debug] [MainThread]: 1699: static parser successfully parsed base/HoldingIncremental.sql
[0m15:15:52.718962 [debug] [MainThread]: 1699: static parser successfully parsed base/BatchDate.sql
[0m15:15:52.720768 [debug] [MainThread]: 1699: static parser successfully parsed base/TradeIncremental.sql
[0m15:15:52.722872 [debug] [MainThread]: 1699: static parser successfully parsed base/WatchIncremental.sql
[0m15:15:52.724889 [debug] [MainThread]: 1699: static parser successfully parsed base/ProspectRaw.sql
[0m15:15:52.924980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9ab29b51-f2e1-4bee-90f4-e5e22897cea3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16ce1d0d0>]}
[0m15:15:52.936505 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9ab29b51-f2e1-4bee-90f4-e5e22897cea3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16bd736a0>]}
[0m15:15:52.936790 [info ] [MainThread]: Found 27 models, 18 tests, 0 snapshots, 0 analyses, 681 macros, 0 operations, 0 seed files, 33 sources, 0 exposures, 0 metrics, 0 groups
[0m15:15:52.937007 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9ab29b51-f2e1-4bee-90f4-e5e22897cea3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102b3a1c0>]}
[0m15:15:52.937424 [debug] [MainThread]: Acquiring new databricks connection 'macro_stage_external_sources'
[0m15:15:52.937606 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:15:52.937743 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:15:52.947823 [info ] [MainThread]: 1 of 33 START external source tpcdi.StatusType
[0m15:15:52.951375 [debug] [MainThread]: On "macro_stage_external_sources": cache miss for schema ".tpcdi", this is inefficient
[0m15:15:52.959128 [debug] [MainThread]: Using databricks connection "macro_stage_external_sources"
[0m15:15:52.959383 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */
show tables in `tpcdi`
  
[0m15:15:52.959548 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:15:53.471043 [debug] [MainThread]: SQL status: OK in 0.5099999904632568 seconds
[0m15:15:53.484029 [debug] [MainThread]: While listing relations in database=, schema=tpcdi, found: 
[0m15:15:53.498293 [info ] [MainThread]: 1 of 33 (1) drop table if exists `main`.`tpcdi`.`StatusType`
[0m15:15:53.500264 [debug] [MainThread]: Using databricks connection "macro_stage_external_sources"
[0m15:15:53.500530 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
        drop table if exists `main`.`tpcdi`.`StatusType`
    

            
[0m15:15:53.620047 [debug] [MainThread]: SQL status: OK in 0.11999999731779099 seconds
[0m15:15:53.621845 [info ] [MainThread]: 1 of 33 (1) OK
[0m15:15:53.622339 [info ] [MainThread]: 1 of 33 (2) create table `main`.`tpcdi`.`StatusType` (                    st_id string,     ...  
[0m15:15:53.623252 [debug] [MainThread]: Using databricks connection "macro_stage_external_sources"
[0m15:15:53.623622 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table `main`.`tpcdi`.`StatusType` (
        
            st_id string,
            st_name string
    )  using csv
    options ('sep' = '|', 
'header' = 'false')
    
    
    
    location '/FileStore/tables/Batch1/StatusType.txt'
    

            
[0m15:15:53.793741 [debug] [MainThread]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table `main`.`tpcdi`.`StatusType` (
        
            st_id string,
            st_name string
    )  using csv
    options ('sep' = '|', 
'header' = 'false')
    
    
    
    location '/FileStore/tables/Batch1/StatusType.txt'
    

            
[0m15:15:53.794810 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [RequestId=3e6822d3-8e0b-4aa0-9df1-d3791658b4f7 ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme
[0m15:15:53.796420 [debug] [MainThread]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: com.databricks.sql.managedcatalog.UnityCatalogServiceException: [RequestId=3e6822d3-8e0b-4aa0-9df1-d3791658b4f7 ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.databricks.sql.managedcatalog.UnityCatalogServiceException: [RequestId=3e6822d3-8e0b-4aa0-9df1-d3791658b4f7 ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:33)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:23)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:105)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:3282)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.generateTemporaryPathCredentials(ManagedCatalogClientImpl.scala:3000)
	at com.databricks.sql.managedcatalog.ManagedCatalogCommon.generateTemporaryPathCredentials(ManagedCatalogCommon.scala:938)
	at com.databricks.unity.CredentialScopeSQLHelper$.checkPathOperations(CredentialScopeSQLHelper.scala:67)
	at com.databricks.unity.CredentialScopeSQLHelper$.register(CredentialScopeSQLHelper.scala:107)
	at com.databricks.unity.CredentialScopeSQLHelper$.registerCreateTableAccess(CredentialScopeSQLHelper.scala:277)
	at com.databricks.sql.managedcatalog.CredentialScopeTableCredentialHandler.injectCredential(ResolveWithCredential.scala:466)
	at com.databricks.sql.managedcatalog.ResolveWithCredential.com$databricks$sql$managedcatalog$ResolveWithCredential$$maybeDecorateCatalogTable(ResolveWithCredential.scala:75)
	at com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:148)
	at com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:146)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:209)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:208)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:32)
	at com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:146)
	at com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:61)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:229)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:229)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:226)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:218)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:296)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:296)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:296)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:383)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:376)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:283)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:376)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:304)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:379)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:355)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:156)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:348)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:380)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:817)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:380)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1040)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:377)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:150)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:140)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getRuntimeCategory$1(QueryRuntimePrediction.scala:300)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1040)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:299)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:349)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:345)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more

[0m15:15:53.797979 [debug] [MainThread]: Databricks adapter: operation-id: b'\x01\xee\x0e\xde\x17\xad\x1e\x00\xac6\x0b\x81V\xa8`\xd4'
[0m15:15:53.798529 [debug] [MainThread]: Databricks adapter: Error while running:
macro stage_external_sources
[0m15:15:53.798868 [debug] [MainThread]: Databricks adapter: Runtime Error
  [RequestId=3e6822d3-8e0b-4aa0-9df1-d3791658b4f7 ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme
[0m15:15:53.799333 [debug] [MainThread]: On macro_stage_external_sources: ROLLBACK
[0m15:15:53.799673 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:15:53.799999 [debug] [MainThread]: On macro_stage_external_sources: Close
[0m15:15:53.872666 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  Runtime Error
    [RequestId=3e6822d3-8e0b-4aa0-9df1-d3791658b4f7 ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme
[0m15:15:53.879852 [debug] [MainThread]: Traceback (most recent call last):
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 660, in exception_handler
    yield
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 705, in add_query
    cursor.execute(sql, bindings)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 477, in execute
    self._cursor.execute(sql, bindings)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 852, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 944, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 786, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 503, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: [RequestId=3e6822d3-8e0b-4aa0-9df1-d3791658b4f7 ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 660, in exception_handler
    yield
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 1043, in execute_macro
    result = macro_function(**kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 220, in macro
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 298, in call
    return __obj(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 52, in macro
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 298, in call
    return __obj(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/impl.py", line 131, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 289, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 728, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 722, in add_query
    cursor.close()
  File "/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 665, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  [RequestId=3e6822d3-8e0b-4aa0-9df1-d3791658b4f7 ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/task/run_operation.py", line 47, in run
    self._run_unsafe()
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/task/run_operation.py", line 37, in _run_unsafe
    res = adapter.execute_macro(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 1043, in execute_macro
    result = macro_function(**kwargs)
  File "/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 678, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  Runtime Error
    [RequestId=3e6822d3-8e0b-4aa0-9df1-d3791658b4f7 ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme

[0m15:15:53.881787 [debug] [MainThread]: Command `dbt run-operation` failed at 15:15:53.881636 after 2.07 seconds
[0m15:15:53.882221 [debug] [MainThread]: Connection 'macro_stage_external_sources' was properly closed.
[0m15:15:53.882635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102823820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16bd73940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16bd736a0>]}
[0m15:15:53.883115 [debug] [MainThread]: Flushing usage events
[0m15:16:25.579077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1029fb1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10424fd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10424fe20>]}


============================== 15:16:25.598830 | 5abb6081-023f-42f6-b30f-3231e52c8501 ==============================
[0m15:16:25.598830 [info ] [MainThread]: Running with dbt=1.5.0
[0m15:16:25.599144 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:16:26.256504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5abb6081-023f-42f6-b30f-3231e52c8501', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100922550>]}
[0m15:16:26.265426 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5abb6081-023f-42f6-b30f-3231e52c8501', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fd853a0>]}
[0m15:16:26.293939 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m15:16:26.413779 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:16:26.415380 [debug] [MainThread]: Partial parsing: updated file: dbsql_dbt_tpch://models/sources.yml
[0m15:16:26.432288 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5abb6081-023f-42f6-b30f-3231e52c8501', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1281b00d0>]}
[0m15:16:26.445395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5abb6081-023f-42f6-b30f-3231e52c8501', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fd3bcd0>]}
[0m15:16:26.445742 [info ] [MainThread]: Found 27 models, 18 tests, 0 snapshots, 0 analyses, 681 macros, 0 operations, 0 seed files, 33 sources, 0 exposures, 0 metrics, 0 groups
[0m15:16:26.445985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5abb6081-023f-42f6-b30f-3231e52c8501', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fd3bd00>]}
[0m15:16:26.446422 [debug] [MainThread]: Acquiring new databricks connection 'macro_stage_external_sources'
[0m15:16:26.446599 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:16:26.446740 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:16:26.457226 [info ] [MainThread]: 1 of 33 START external source tpcdi.StatusType
[0m15:16:26.460951 [debug] [MainThread]: On "macro_stage_external_sources": cache miss for schema ".tpcdi", this is inefficient
[0m15:16:26.469451 [debug] [MainThread]: Using databricks connection "macro_stage_external_sources"
[0m15:16:26.469751 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */
show tables in `tpcdi`
  
[0m15:16:26.469910 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:16:27.250817 [debug] [MainThread]: SQL status: OK in 0.7799999713897705 seconds
[0m15:16:27.258735 [debug] [MainThread]: While listing relations in database=, schema=tpcdi, found: 
[0m15:16:27.272856 [info ] [MainThread]: 1 of 33 (1) drop table if exists `main`.`tpcdi`.`StatusType`
[0m15:16:27.275279 [debug] [MainThread]: Using databricks connection "macro_stage_external_sources"
[0m15:16:27.275596 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
        drop table if exists `main`.`tpcdi`.`StatusType`
    

            
[0m15:16:27.389342 [debug] [MainThread]: SQL status: OK in 0.10999999940395355 seconds
[0m15:16:27.391405 [info ] [MainThread]: 1 of 33 (1) OK
[0m15:16:27.391991 [info ] [MainThread]: 1 of 33 (2) create table `main`.`tpcdi`.`StatusType` (                    st_id string,     ...  
[0m15:16:27.393017 [debug] [MainThread]: Using databricks connection "macro_stage_external_sources"
[0m15:16:27.393378 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table `main`.`tpcdi`.`StatusType` (
        
            st_id string,
            st_name string
    )  using csv
    options ('sep' = '|', 
'header' = 'false')
    
    
    
    location '/FileStore/tables/Batch1/StatusType.txt'
    

            
[0m15:16:27.573652 [debug] [MainThread]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table `main`.`tpcdi`.`StatusType` (
        
            st_id string,
            st_name string
    )  using csv
    options ('sep' = '|', 
'header' = 'false')
    
    
    
    location '/FileStore/tables/Batch1/StatusType.txt'
    

            
[0m15:16:27.574362 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [RequestId=03a37abd-fb57-4c33-8972-47b60d5490bd ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme
[0m15:16:27.575217 [debug] [MainThread]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: com.databricks.sql.managedcatalog.UnityCatalogServiceException: [RequestId=03a37abd-fb57-4c33-8972-47b60d5490bd ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.databricks.sql.managedcatalog.UnityCatalogServiceException: [RequestId=03a37abd-fb57-4c33-8972-47b60d5490bd ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:33)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:23)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:105)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:3282)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.generateTemporaryPathCredentials(ManagedCatalogClientImpl.scala:3000)
	at com.databricks.sql.managedcatalog.ManagedCatalogCommon.generateTemporaryPathCredentials(ManagedCatalogCommon.scala:938)
	at com.databricks.unity.CredentialScopeSQLHelper$.checkPathOperations(CredentialScopeSQLHelper.scala:67)
	at com.databricks.unity.CredentialScopeSQLHelper$.register(CredentialScopeSQLHelper.scala:107)
	at com.databricks.unity.CredentialScopeSQLHelper$.registerCreateTableAccess(CredentialScopeSQLHelper.scala:277)
	at com.databricks.sql.managedcatalog.CredentialScopeTableCredentialHandler.injectCredential(ResolveWithCredential.scala:466)
	at com.databricks.sql.managedcatalog.ResolveWithCredential.com$databricks$sql$managedcatalog$ResolveWithCredential$$maybeDecorateCatalogTable(ResolveWithCredential.scala:75)
	at com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:148)
	at com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:146)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:209)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:208)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:32)
	at com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:146)
	at com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:61)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:229)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:229)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:226)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:218)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:296)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:296)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:296)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:383)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:376)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:283)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:376)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:304)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:379)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:355)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:156)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:348)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:380)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:817)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:380)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1040)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:377)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:150)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:140)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getRuntimeCategory$1(QueryRuntimePrediction.scala:300)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1040)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:299)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:349)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:345)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more

[0m15:16:27.576074 [debug] [MainThread]: Databricks adapter: operation-id: b"\x01\xee\x0e\xde+\xd1\x19\xc7\xb8\n\\\xd3\x93\x06'\xc6"
[0m15:16:27.576406 [debug] [MainThread]: Databricks adapter: Error while running:
macro stage_external_sources
[0m15:16:27.576648 [debug] [MainThread]: Databricks adapter: Runtime Error
  [RequestId=03a37abd-fb57-4c33-8972-47b60d5490bd ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme
[0m15:16:27.576968 [debug] [MainThread]: On macro_stage_external_sources: ROLLBACK
[0m15:16:27.577192 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:16:27.577414 [debug] [MainThread]: On macro_stage_external_sources: Close
[0m15:16:27.642629 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  Runtime Error
    [RequestId=03a37abd-fb57-4c33-8972-47b60d5490bd ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme
[0m15:16:27.646978 [debug] [MainThread]: Traceback (most recent call last):
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 660, in exception_handler
    yield
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 705, in add_query
    cursor.execute(sql, bindings)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 477, in execute
    self._cursor.execute(sql, bindings)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 852, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 944, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 786, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 503, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: [RequestId=03a37abd-fb57-4c33-8972-47b60d5490bd ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 660, in exception_handler
    yield
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 1043, in execute_macro
    result = macro_function(**kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 220, in macro
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 298, in call
    return __obj(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 52, in macro
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 298, in call
    return __obj(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/impl.py", line 131, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 289, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 728, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 722, in add_query
    cursor.close()
  File "/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 665, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  [RequestId=03a37abd-fb57-4c33-8972-47b60d5490bd ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/task/run_operation.py", line 47, in run
    self._run_unsafe()
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/task/run_operation.py", line 37, in _run_unsafe
    res = adapter.execute_macro(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 1043, in execute_macro
    result = macro_function(**kwargs)
  File "/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 678, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  Runtime Error
    [RequestId=03a37abd-fb57-4c33-8972-47b60d5490bd ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme

[0m15:16:27.647893 [debug] [MainThread]: Command `dbt run-operation` failed at 15:16:27.647805 after 2.09 seconds
[0m15:16:27.648127 [debug] [MainThread]: Connection 'macro_stage_external_sources' was properly closed.
[0m15:16:27.648317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1029fb1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100871c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x128004f40>]}
[0m15:16:27.648538 [debug] [MainThread]: Flushing usage events
[0m15:17:51.295511 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10672b0a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107e8feb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107e8fe50>]}


============================== 15:17:51.316153 | babcf366-7f29-4a29-b528-588c96f397c8 ==============================
[0m15:17:51.316153 [info ] [MainThread]: Running with dbt=1.5.0
[0m15:17:51.316477 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi', 'log_path': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:17:51.331314 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'babcf366-7f29-4a29-b528-588c96f397c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104668c10>]}
[0m15:17:51.332879 [debug] [MainThread]: Set downloads directory='/var/folders/94/_5czc1mn6cz2scfnd_7y6_6w0000gp/T/dbt-downloads-48i45g06'
[0m15:17:51.333136 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m15:17:51.524684 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m15:17:51.527079 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m15:17:51.702211 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m15:17:51.709000 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/spark_utils.json
[0m15:17:51.874734 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/spark_utils.json 200
[0m15:17:51.878287 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_external_tables.json
[0m15:17:52.031707 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_external_tables.json 200
[0m15:17:52.046783 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m15:17:52.487409 [info ] [MainThread]: Installed from version 0.8.0
[0m15:17:52.487765 [info ] [MainThread]: Updated version available: 1.1.1
[0m15:17:52.488033 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'babcf366-7f29-4a29-b528-588c96f397c8', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107e8f490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107e8ffd0>]}
[0m15:17:52.488265 [info ] [MainThread]: Installing dbt-labs/spark_utils
[0m15:17:52.787616 [info ] [MainThread]: Installed from version 0.3.0
[0m15:17:52.788098 [info ] [MainThread]: Up to date!
[0m15:17:52.788459 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'babcf366-7f29-4a29-b528-588c96f397c8', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f15df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f15640>]}
[0m15:17:52.788788 [info ] [MainThread]: Installing dbt-labs/dbt_external_tables
[0m15:17:53.263167 [info ] [MainThread]: Installed from version 0.8.2
[0m15:17:53.263521 [info ] [MainThread]: Updated version available: 0.8.5
[0m15:17:53.263780 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'babcf366-7f29-4a29-b528-588c96f397c8', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f150d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f47880>]}
[0m15:17:53.263996 [info ] [MainThread]: 
[0m15:17:53.264196 [info ] [MainThread]: Updates available for packages: ['dbt-labs/dbt_utils', 'dbt-labs/dbt_external_tables']                 
Update your versions in packages.yml, then run dbt deps
[0m15:17:53.265395 [debug] [MainThread]: Command `dbt deps` succeeded at 15:17:53.265326 after 1.99 seconds
[0m15:17:53.265642 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10672b0a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10451b7c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104668c10>]}
[0m15:17:53.265845 [debug] [MainThread]: Flushing usage events
[0m15:17:57.695443 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10708f220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120a0ff70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120a5e040>]}


============================== 15:17:57.715564 | 06c16f55-c18f-4f75-8098-5f4f4f702661 ==============================
[0m15:17:57.715564 [info ] [MainThread]: Running with dbt=1.5.0
[0m15:17:57.715897 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:17:58.400477 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '06c16f55-c18f-4f75-8098-5f4f4f702661', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104fccdc0>]}
[0m15:17:58.408514 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '06c16f55-c18f-4f75-8098-5f4f4f702661', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1371c35e0>]}
[0m15:17:58.434635 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m15:17:58.539858 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:17:58.540142 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:17:58.545779 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '06c16f55-c18f-4f75-8098-5f4f4f702661', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x141502d60>]}
[0m15:17:58.558471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '06c16f55-c18f-4f75-8098-5f4f4f702661', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13714ba30>]}
[0m15:17:58.558738 [info ] [MainThread]: Found 27 models, 18 tests, 0 snapshots, 0 analyses, 681 macros, 0 operations, 0 seed files, 33 sources, 0 exposures, 0 metrics, 0 groups
[0m15:17:58.560759 [info ] [MainThread]: 
[0m15:17:58.561324 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m15:17:58.563045 [debug] [ThreadPool]: Acquiring new databricks connection 'list_main'
[0m15:17:58.563521 [debug] [ThreadPool]: Acquiring new databricks connection 'list_main'
[0m15:17:58.563718 [debug] [ThreadPool]: Using databricks connection "list_main"
[0m15:17:58.563908 [debug] [ThreadPool]: Using databricks connection "list_main"
[0m15:17:58.564082 [debug] [ThreadPool]: On list_main: GetSchemas(database=`main`, schema=None)
[0m15:17:58.564227 [debug] [ThreadPool]: On list_main: GetSchemas(database=`main`, schema=None)
[0m15:17:58.564415 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:17:58.564591 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:17:59.123465 [debug] [ThreadPool]: SQL status: OK in 0.5600000023841858 seconds
[0m15:17:59.127811 [debug] [ThreadPool]: On list_main: Close
[0m15:17:59.130451 [debug] [ThreadPool]: SQL status: OK in 0.5699999928474426 seconds
[0m15:17:59.132283 [debug] [ThreadPool]: On list_main: Close
[0m15:17:59.196478 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_main, now create_main_tpcdi)
[0m15:17:59.197260 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_main, now create_main_tpcdi_dbt_test__audit)
[0m15:17:59.199390 [debug] [ThreadPool]: Creating schema "database: "main"
schema: "tpcdi"
"
[0m15:17:59.200267 [debug] [ThreadPool]: Creating schema "database: "main"
schema: "tpcdi_dbt_test__audit"
"
[0m15:17:59.211472 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:17:59.214119 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:17:59.214408 [debug] [ThreadPool]: Using databricks connection "create_main_tpcdi"
[0m15:17:59.214638 [debug] [ThreadPool]: Using databricks connection "create_main_tpcdi_dbt_test__audit"
[0m15:17:59.214870 [debug] [ThreadPool]: On create_main_tpcdi: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "create_main_tpcdi"} */
create schema if not exists `main`.`tpcdi`
  
[0m15:17:59.215120 [debug] [ThreadPool]: On create_main_tpcdi_dbt_test__audit: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "create_main_tpcdi_dbt_test__audit"} */
create schema if not exists `main`.`tpcdi_dbt_test__audit`
  
[0m15:17:59.215337 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:17:59.215530 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:17:59.614542 [debug] [ThreadPool]: SQL status: OK in 0.4000000059604645 seconds
[0m15:17:59.615307 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m15:17:59.615505 [debug] [ThreadPool]: On create_main_tpcdi: ROLLBACK
[0m15:17:59.615664 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m15:17:59.615803 [debug] [ThreadPool]: On create_main_tpcdi: Close
[0m15:17:59.655868 [debug] [ThreadPool]: SQL status: OK in 0.4399999976158142 seconds
[0m15:17:59.657508 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m15:17:59.658002 [debug] [ThreadPool]: On create_main_tpcdi_dbt_test__audit: ROLLBACK
[0m15:17:59.658436 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m15:17:59.658822 [debug] [ThreadPool]: On create_main_tpcdi_dbt_test__audit: Close
[0m15:17:59.726022 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_main_tpcdi_dbt_test__audit, now list_main_tpcdi)
[0m15:17:59.726962 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_main_tpcdi, now list_main_tpcdi_dbt_test__audit)
[0m15:17:59.733949 [debug] [ThreadPool]: Using databricks connection "list_main_tpcdi_dbt_test__audit"
[0m15:17:59.742400 [debug] [ThreadPool]: On list_main_tpcdi_dbt_test__audit: GetTables(database=main, schema=tpcdi_dbt_test__audit, identifier=None)
[0m15:17:59.749855 [debug] [ThreadPool]: Using databricks connection "list_main_tpcdi"
[0m15:17:59.750128 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:17:59.750398 [debug] [ThreadPool]: On list_main_tpcdi: GetTables(database=main, schema=tpcdi, identifier=None)
[0m15:17:59.750753 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:18:00.197722 [debug] [ThreadPool]: SQL status: OK in 0.44999998807907104 seconds
[0m15:18:00.198038 [debug] [ThreadPool]: SQL status: OK in 0.44999998807907104 seconds
[0m15:18:00.200828 [debug] [ThreadPool]: On list_main_tpcdi_dbt_test__audit: Close
[0m15:18:00.202098 [debug] [ThreadPool]: On list_main_tpcdi: Close
[0m15:18:00.303111 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '06c16f55-c18f-4f75-8098-5f4f4f702661', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120a0f340>]}
[0m15:18:00.304189 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:18:00.304678 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:18:00.305743 [info ] [MainThread]: Concurrency: 25 threads (target='dev')
[0m15:18:00.306337 [info ] [MainThread]: 
[0m15:18:00.326327 [debug] [Thread-1  ]: Began running node model.dbsql_dbt_tpch.AccountIncremental
[0m15:18:00.326735 [debug] [Thread-2  ]: Began running node model.dbsql_dbt_tpch.BatchDate
[0m15:18:00.327048 [debug] [Thread-3  ]: Began running node model.dbsql_dbt_tpch.CashTransactionIncremental
[0m15:18:00.327318 [debug] [Thread-4  ]: Began running node model.dbsql_dbt_tpch.CustomerIncremental
[0m15:18:00.327573 [debug] [Thread-5  ]: Began running node model.dbsql_dbt_tpch.CustomerMgmtView
[0m15:18:00.327833 [debug] [Thread-6  ]: Began running node model.dbsql_dbt_tpch.DailyMarketHistorical
[0m15:18:00.328213 [info ] [Thread-1  ]: 1 of 45 START sql view model tpcdi.AccountIncremental .......................... [RUN]
[0m15:18:00.328567 [debug] [Thread-7  ]: Began running node model.dbsql_dbt_tpch.DailyMarketIncremental
[0m15:18:00.328919 [debug] [Thread-8  ]: Began running node model.dbsql_dbt_tpch.DimBroker
[0m15:18:00.329260 [debug] [Thread-9  ]: Began running node model.dbsql_dbt_tpch.FinWire
[0m15:18:00.329522 [debug] [Thread-10 ]: Began running node model.dbsql_dbt_tpch.HoldingIncremental
[0m15:18:00.330080 [debug] [Thread-11 ]: Began running node model.dbsql_dbt_tpch.ProspectRaw
[0m15:18:00.330371 [debug] [Thread-12 ]: Began running node model.dbsql_dbt_tpch.TradeIncremental
[0m15:18:00.329830 [info ] [Thread-2  ]: 2 of 45 START sql view model tpcdi.BatchDate ................................... [RUN]
[0m15:18:00.330753 [debug] [Thread-13 ]: Began running node model.dbsql_dbt_tpch.WatchIncremental
[0m15:18:00.331091 [info ] [Thread-3  ]: 3 of 45 START sql view model tpcdi.CashTransactionIncremental .................. [RUN]
[0m15:18:00.331571 [info ] [Thread-4  ]: 4 of 45 START sql view model tpcdi.CustomerIncremental ......................... [RUN]
[0m15:18:00.332086 [info ] [Thread-5  ]: 5 of 45 START sql view model tpcdi.CustomerMgmtView ............................ [RUN]
[0m15:18:00.332577 [info ] [Thread-6  ]: 6 of 45 START sql view model tpcdi.DailyMarketHistorical ....................... [RUN]
[0m15:18:00.333436 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_main_tpcdi, now model.dbsql_dbt_tpch.AccountIncremental)
[0m15:18:00.333890 [info ] [Thread-7  ]: 7 of 45 START sql view model tpcdi.DailyMarketIncremental ...................... [RUN]
[0m15:18:00.334471 [info ] [Thread-8  ]: 8 of 45 START sql table model tpcdi.DimBroker .................................. [RUN]
[0m15:18:00.335084 [info ] [Thread-9  ]: 9 of 45 START sql view model tpcdi.FinWire ..................................... [RUN]
[0m15:18:00.335725 [info ] [Thread-10 ]: 10 of 45 START sql view model tpcdi.HoldingIncremental ......................... [RUN]
[0m15:18:00.336247 [info ] [Thread-11 ]: 11 of 45 START sql view model tpcdi.ProspectRaw ................................ [RUN]
[0m15:18:00.336683 [info ] [Thread-12 ]: 12 of 45 START sql view model tpcdi.TradeIncremental ........................... [RUN]
[0m15:18:00.337327 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_main_tpcdi_dbt_test__audit, now model.dbsql_dbt_tpch.BatchDate)
[0m15:18:00.337704 [info ] [Thread-13 ]: 13 of 45 START sql view model tpcdi.WatchIncremental ........................... [RUN]
[0m15:18:00.338367 [debug] [Thread-3  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.CashTransactionIncremental'
[0m15:18:00.338892 [debug] [Thread-4  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.CustomerIncremental'
[0m15:18:00.339376 [debug] [Thread-5  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.CustomerMgmtView'
[0m15:18:00.339855 [debug] [Thread-6  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.DailyMarketHistorical'
[0m15:18:00.340112 [debug] [Thread-1  ]: Began compiling node model.dbsql_dbt_tpch.AccountIncremental
[0m15:18:00.340591 [debug] [Thread-7  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.DailyMarketIncremental'
[0m15:18:00.341230 [debug] [Thread-8  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimBroker'
[0m15:18:00.341689 [debug] [Thread-9  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.FinWire'
[0m15:18:00.342156 [debug] [Thread-10 ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.HoldingIncremental'
[0m15:18:00.342607 [debug] [Thread-11 ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.ProspectRaw'
[0m15:18:00.343063 [debug] [Thread-12 ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.TradeIncremental'
[0m15:18:00.343289 [debug] [Thread-2  ]: Began compiling node model.dbsql_dbt_tpch.BatchDate
[0m15:18:00.343753 [debug] [Thread-13 ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.WatchIncremental'
[0m15:18:00.343979 [debug] [Thread-3  ]: Began compiling node model.dbsql_dbt_tpch.CashTransactionIncremental
[0m15:18:00.344194 [debug] [Thread-4  ]: Began compiling node model.dbsql_dbt_tpch.CustomerIncremental
[0m15:18:00.344407 [debug] [Thread-5  ]: Began compiling node model.dbsql_dbt_tpch.CustomerMgmtView
[0m15:18:00.344603 [debug] [Thread-6  ]: Began compiling node model.dbsql_dbt_tpch.DailyMarketHistorical
[0m15:18:00.347682 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.AccountIncremental"
[0m15:18:00.347979 [debug] [Thread-7  ]: Began compiling node model.dbsql_dbt_tpch.DailyMarketIncremental
[0m15:18:00.348190 [debug] [Thread-8  ]: Began compiling node model.dbsql_dbt_tpch.DimBroker
[0m15:18:00.348403 [debug] [Thread-9  ]: Began compiling node model.dbsql_dbt_tpch.FinWire
[0m15:18:00.348644 [debug] [Thread-10 ]: Began compiling node model.dbsql_dbt_tpch.HoldingIncremental
[0m15:18:00.348853 [debug] [Thread-11 ]: Began compiling node model.dbsql_dbt_tpch.ProspectRaw
[0m15:18:00.349080 [debug] [Thread-12 ]: Began compiling node model.dbsql_dbt_tpch.TradeIncremental
[0m15:18:00.352236 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.BatchDate"
[0m15:18:00.352607 [debug] [Thread-13 ]: Began compiling node model.dbsql_dbt_tpch.WatchIncremental
[0m15:18:00.358966 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.CustomerIncremental"
[0m15:18:00.363584 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.CashTransactionIncremental"
[0m15:18:00.366300 [debug] [Thread-5  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.CustomerMgmtView"
[0m15:18:00.368775 [debug] [Thread-6  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.DailyMarketHistorical"
[0m15:18:00.371706 [debug] [Thread-7  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.DailyMarketIncremental"
[0m15:18:00.374063 [debug] [Thread-8  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.DimBroker"
[0m15:18:00.376167 [debug] [Thread-9  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.FinWire"
[0m15:18:00.379058 [debug] [Thread-10 ]: Writing injected SQL for node "model.dbsql_dbt_tpch.HoldingIncremental"
[0m15:18:00.379331 [debug] [Thread-1  ]: Timing info for model.dbsql_dbt_tpch.AccountIncremental (compile): 15:18:00.344742 => 15:18:00.379206
[0m15:18:00.381587 [debug] [Thread-11 ]: Writing injected SQL for node "model.dbsql_dbt_tpch.ProspectRaw"
[0m15:18:00.383978 [debug] [Thread-12 ]: Writing injected SQL for node "model.dbsql_dbt_tpch.TradeIncremental"
[0m15:18:00.386687 [debug] [Thread-13 ]: Writing injected SQL for node "model.dbsql_dbt_tpch.WatchIncremental"
[0m15:18:00.387269 [debug] [Thread-2  ]: Timing info for model.dbsql_dbt_tpch.BatchDate (compile): 15:18:00.349225 => 15:18:00.387087
[0m15:18:00.387629 [debug] [Thread-4  ]: Timing info for model.dbsql_dbt_tpch.CustomerIncremental (compile): 15:18:00.356308 => 15:18:00.387496
[0m15:18:00.388013 [debug] [Thread-3  ]: Timing info for model.dbsql_dbt_tpch.CashTransactionIncremental (compile): 15:18:00.352780 => 15:18:00.387902
[0m15:18:00.388406 [debug] [Thread-5  ]: Timing info for model.dbsql_dbt_tpch.CustomerMgmtView (compile): 15:18:00.363820 => 15:18:00.388287
[0m15:18:00.388768 [debug] [Thread-6  ]: Timing info for model.dbsql_dbt_tpch.DailyMarketHistorical (compile): 15:18:00.366532 => 15:18:00.388669
[0m15:18:00.389004 [debug] [Thread-7  ]: Timing info for model.dbsql_dbt_tpch.DailyMarketIncremental (compile): 15:18:00.369191 => 15:18:00.388908
[0m15:18:00.389347 [debug] [Thread-8  ]: Timing info for model.dbsql_dbt_tpch.DimBroker (compile): 15:18:00.371940 => 15:18:00.389243
[0m15:18:00.389542 [debug] [Thread-9  ]: Timing info for model.dbsql_dbt_tpch.FinWire (compile): 15:18:00.374253 => 15:18:00.389455
[0m15:18:00.389733 [debug] [Thread-1  ]: Began executing node model.dbsql_dbt_tpch.AccountIncremental
[0m15:18:00.389999 [debug] [Thread-10 ]: Timing info for model.dbsql_dbt_tpch.HoldingIncremental (compile): 15:18:00.376342 => 15:18:00.389905
[0m15:18:00.390264 [debug] [Thread-2  ]: Began executing node model.dbsql_dbt_tpch.BatchDate
[0m15:18:00.390477 [debug] [Thread-4  ]: Began executing node model.dbsql_dbt_tpch.CustomerIncremental
[0m15:18:00.390717 [debug] [Thread-11 ]: Timing info for model.dbsql_dbt_tpch.ProspectRaw (compile): 15:18:00.379479 => 15:18:00.390624
[0m15:18:00.390913 [debug] [Thread-3  ]: Began executing node model.dbsql_dbt_tpch.CashTransactionIncremental
[0m15:18:00.391138 [debug] [Thread-12 ]: Timing info for model.dbsql_dbt_tpch.TradeIncremental (compile): 15:18:00.381751 => 15:18:00.391042
[0m15:18:00.391321 [debug] [Thread-13 ]: Timing info for model.dbsql_dbt_tpch.WatchIncremental (compile): 15:18:00.384331 => 15:18:00.391239
[0m15:18:00.391487 [debug] [Thread-5  ]: Began executing node model.dbsql_dbt_tpch.CustomerMgmtView
[0m15:18:00.391656 [debug] [Thread-6  ]: Began executing node model.dbsql_dbt_tpch.DailyMarketHistorical
[0m15:18:00.391822 [debug] [Thread-7  ]: Began executing node model.dbsql_dbt_tpch.DailyMarketIncremental
[0m15:18:00.391983 [debug] [Thread-8  ]: Began executing node model.dbsql_dbt_tpch.DimBroker
[0m15:18:00.392143 [debug] [Thread-9  ]: Began executing node model.dbsql_dbt_tpch.FinWire
[0m15:18:00.406350 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.AccountIncremental"
[0m15:18:00.406672 [debug] [Thread-10 ]: Began executing node model.dbsql_dbt_tpch.HoldingIncremental
[0m15:18:00.409543 [debug] [Thread-2  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.BatchDate"
[0m15:18:00.411573 [debug] [Thread-4  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.CustomerIncremental"
[0m15:18:00.411785 [debug] [Thread-11 ]: Began executing node model.dbsql_dbt_tpch.ProspectRaw
[0m15:18:00.413735 [debug] [Thread-3  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.CashTransactionIncremental"
[0m15:18:00.413929 [debug] [Thread-12 ]: Began executing node model.dbsql_dbt_tpch.TradeIncremental
[0m15:18:00.414101 [debug] [Thread-13 ]: Began executing node model.dbsql_dbt_tpch.WatchIncremental
[0m15:18:00.416243 [debug] [Thread-5  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.CustomerMgmtView"
[0m15:18:00.418303 [debug] [Thread-6  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.DailyMarketHistorical"
[0m15:18:00.420674 [debug] [Thread-7  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.DailyMarketIncremental"
[0m15:18:00.428971 [debug] [Thread-9  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.FinWire"
[0m15:18:00.443503 [debug] [Thread-8  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.DimBroker"
[0m15:18:00.446038 [debug] [Thread-10 ]: Writing runtime sql for node "model.dbsql_dbt_tpch.HoldingIncremental"
[0m15:18:00.448195 [debug] [Thread-11 ]: Writing runtime sql for node "model.dbsql_dbt_tpch.ProspectRaw"
[0m15:18:00.450431 [debug] [Thread-12 ]: Writing runtime sql for node "model.dbsql_dbt_tpch.TradeIncremental"
[0m15:18:00.452568 [debug] [Thread-13 ]: Writing runtime sql for node "model.dbsql_dbt_tpch.WatchIncremental"
[0m15:18:00.452871 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:18:00.453054 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:18:00.453218 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:18:00.453394 [debug] [Thread-3  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:18:00.453926 [debug] [Thread-5  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:18:00.454144 [debug] [Thread-7  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:18:00.454351 [debug] [Thread-6  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:18:00.454627 [debug] [Thread-9  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:18:00.454807 [debug] [Thread-10 ]: Spark adapter: NotImplemented: add_begin_query
[0m15:18:00.454984 [debug] [Thread-8  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:18:00.455293 [debug] [Thread-11 ]: Spark adapter: NotImplemented: add_begin_query
[0m15:18:00.455496 [debug] [Thread-1  ]: Using databricks connection "model.dbsql_dbt_tpch.AccountIncremental"
[0m15:18:00.455676 [debug] [Thread-12 ]: Spark adapter: NotImplemented: add_begin_query
[0m15:18:00.455834 [debug] [Thread-2  ]: Using databricks connection "model.dbsql_dbt_tpch.BatchDate"
[0m15:18:00.455970 [debug] [Thread-4  ]: Using databricks connection "model.dbsql_dbt_tpch.CustomerIncremental"
[0m15:18:00.456134 [debug] [Thread-13 ]: Spark adapter: NotImplemented: add_begin_query
[0m15:18:00.456262 [debug] [Thread-3  ]: Using databricks connection "model.dbsql_dbt_tpch.CashTransactionIncremental"
[0m15:18:00.456391 [debug] [Thread-5  ]: Using databricks connection "model.dbsql_dbt_tpch.CustomerMgmtView"
[0m15:18:00.456520 [debug] [Thread-7  ]: Using databricks connection "model.dbsql_dbt_tpch.DailyMarketIncremental"
[0m15:18:00.456645 [debug] [Thread-6  ]: Using databricks connection "model.dbsql_dbt_tpch.DailyMarketHistorical"
[0m15:18:00.456770 [debug] [Thread-9  ]: Using databricks connection "model.dbsql_dbt_tpch.FinWire"
[0m15:18:00.456894 [debug] [Thread-10 ]: Using databricks connection "model.dbsql_dbt_tpch.HoldingIncremental"
[0m15:18:00.457020 [debug] [Thread-8  ]: Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
[0m15:18:00.457146 [debug] [Thread-11 ]: Using databricks connection "model.dbsql_dbt_tpch.ProspectRaw"
[0m15:18:00.457306 [debug] [Thread-1  ]: On model.dbsql_dbt_tpch.AccountIncremental: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.AccountIncremental"} */
create or replace view `main`.`tpcdi`.`AccountIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`AccountIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`AccountIncrementaltres`

[0m15:18:00.457448 [debug] [Thread-12 ]: Using databricks connection "model.dbsql_dbt_tpch.TradeIncremental"
[0m15:18:00.457600 [debug] [Thread-2  ]: On model.dbsql_dbt_tpch.BatchDate: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.BatchDate"} */
create or replace view `main`.`tpcdi`.`BatchDate`
  
  
  as
    

select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`BatchDateuno`

 UNION ALL

select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`BatchDatedos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`BatchDatetres`

[0m15:18:00.457772 [debug] [Thread-4  ]: On model.dbsql_dbt_tpch.CustomerIncremental: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CustomerIncremental"} */
create or replace view `main`.`tpcdi`.`CustomerIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`customerincrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`customerincrementaltres`

[0m15:18:00.457913 [debug] [Thread-13 ]: Using databricks connection "model.dbsql_dbt_tpch.WatchIncremental"
[0m15:18:00.458065 [debug] [Thread-3  ]: On model.dbsql_dbt_tpch.CashTransactionIncremental: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CashTransactionIncremental"} */
create or replace view `main`.`tpcdi`.`CashTransactionIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`CashTransactionIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`CashTransactionIncrementaltres`

[0m15:18:00.458225 [debug] [Thread-5  ]: On model.dbsql_dbt_tpch.CustomerMgmtView: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CustomerMgmtView"} */
create or replace view `main`.`tpcdi`.`CustomerMgmtView`
  
  
  as
    
select
    *
from
    hive_metastore.roberto_salcido_tpcdi_stage.customermgmt10

[0m15:18:00.458390 [debug] [Thread-7  ]: On model.dbsql_dbt_tpch.DailyMarketIncremental: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DailyMarketIncremental"} */
create or replace view `main`.`tpcdi`.`DailyMarketIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`DailyMarketIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`DailyMarketIncrementaltres`

[0m15:18:00.458550 [debug] [Thread-6  ]: On model.dbsql_dbt_tpch.DailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DailyMarketHistorical"} */
create or replace view `main`.`tpcdi`.`DailyMarketHistorical`
  
  
  as
    
select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`DailyMarketHistorical`

[0m15:18:00.458708 [debug] [Thread-9  ]: On model.dbsql_dbt_tpch.FinWire: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.FinWire"} */
create or replace view `main`.`tpcdi`.`FinWire`
  
  
  as
    

select *, substring(value, 16, 3) rectype from 
text.`dbfs:/tmp/tpcdi/sf=10/Batch1/FINWIRE*`

[0m15:18:00.458873 [debug] [Thread-10 ]: On model.dbsql_dbt_tpch.HoldingIncremental: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.HoldingIncremental"} */
create or replace view `main`.`tpcdi`.`HoldingIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`HoldingIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`HoldingIncrementaltres`

[0m15:18:00.459066 [debug] [Thread-8  ]: On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

  
    
        create or replace table `main`.`tpcdi`.`DimBroker`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT

  cast(employeeid as BIGINT) brokerid,
  cast(managerid as BIGINT) managerid,
  employeefirstname firstname,
  employeelastname lastname,
  employeemi middleinitial,
  employeebranch branch,
  employeeoffice office,
  employeephone phone,
  true iscurrent,
  1 batchid,
  (SELECT min(to_date(datevalue)) as effectivedate FROM `main`.`tpcdi`.`DimDate`) effectivedate,
  date('9999-12-31') enddate,
  bigint(concat(date_format(enddate, 'yyyyMMdd'), cast(brokerid as string))) as sk_brokerid
FROM  `main`.`tpcdi`.`HR`
WHERE employeejobcode = 314
  
[0m15:18:00.459251 [debug] [Thread-11 ]: On model.dbsql_dbt_tpch.ProspectRaw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.ProspectRaw"} */
create or replace view `main`.`tpcdi`.`ProspectRaw`
  
  
  as
    

select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`ProspectRawuno`

 UNION ALL

select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`ProspectRawdos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`ProspectRawtres`

[0m15:18:00.459420 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:18:00.459575 [debug] [Thread-12 ]: On model.dbsql_dbt_tpch.TradeIncremental: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.TradeIncremental"} */
create or replace view `main`.`tpcdi`.`TradeIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`TradeIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`TradeIncrementaltres`

[0m15:18:00.459740 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m15:18:00.459895 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m15:18:00.460051 [debug] [Thread-13 ]: On model.dbsql_dbt_tpch.WatchIncremental: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.WatchIncremental"} */
create or replace view `main`.`tpcdi`.`WatchIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`WatchIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`WatchIncrementaltres`

[0m15:18:00.460231 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m15:18:00.460388 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m15:18:00.460544 [debug] [Thread-7  ]: Opening a new connection, currently in state init
[0m15:18:00.460696 [debug] [Thread-6  ]: Opening a new connection, currently in state init
[0m15:18:00.460847 [debug] [Thread-9  ]: Opening a new connection, currently in state init
[0m15:18:00.461012 [debug] [Thread-10 ]: Opening a new connection, currently in state init
[0m15:18:00.461161 [debug] [Thread-8  ]: Opening a new connection, currently in state init
[0m15:18:00.461308 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m15:18:00.461538 [debug] [Thread-12 ]: Opening a new connection, currently in state init
[0m15:18:00.473397 [debug] [Thread-13 ]: Opening a new connection, currently in state init
[0m15:18:00.939205 [debug] [Thread-6  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DailyMarketHistorical"} */
create or replace view `main`.`tpcdi`.`DailyMarketHistorical`
  
  
  as
    
select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`DailyMarketHistorical`

[0m15:18:00.942396 [debug] [Thread-4  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CustomerIncremental"} */
create or replace view `main`.`tpcdi`.`CustomerIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`customerincrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`customerincrementaltres`

[0m15:18:00.943245 [debug] [Thread-6  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketHistorical` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:18:00.945650 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.AccountIncremental"} */
create or replace view `main`.`tpcdi`.`AccountIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`AccountIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`AccountIncrementaltres`

[0m15:18:00.948155 [debug] [Thread-7  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DailyMarketIncremental"} */
create or replace view `main`.`tpcdi`.`DailyMarketIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`DailyMarketIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`DailyMarketIncrementaltres`

[0m15:18:00.948694 [debug] [Thread-4  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`customerincrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:18:00.949527 [debug] [Thread-6  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketHistorical` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketHistorical` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:18:00.950301 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`AccountIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:18:00.950781 [debug] [Thread-7  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:18:00.951505 [debug] [Thread-4  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`customerincrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`customerincrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:18:00.952194 [debug] [Thread-6  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xdecZ\x1b\xc1\xb1\xdc\xf5\xf0@j\x1d\xba'
[0m15:18:00.952863 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`AccountIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`AccountIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:18:00.953823 [debug] [Thread-7  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:18:00.954451 [debug] [Thread-4  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xdecZ\x12\xb1\x93B\x83;R`Q\xfe'
[0m15:18:00.954967 [debug] [Thread-6  ]: Timing info for model.dbsql_dbt_tpch.DailyMarketHistorical (execute): 15:18:00.416443 => 15:18:00.954773
[0m15:18:00.955305 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xdecZ\x10c\x8e\xb0\xe9\x0ec\xae\xafH'
[0m15:18:00.955642 [debug] [Thread-7  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xdec]\x16\x06\x91Jn:\x19R\x9d '
[0m15:18:00.956110 [debug] [Thread-4  ]: Timing info for model.dbsql_dbt_tpch.CustomerIncremental (execute): 15:18:00.409716 => 15:18:00.955943
[0m15:18:00.956460 [debug] [Thread-6  ]: On model.dbsql_dbt_tpch.DailyMarketHistorical: ROLLBACK
[0m15:18:00.956938 [debug] [Thread-1  ]: Timing info for model.dbsql_dbt_tpch.AccountIncremental (execute): 15:18:00.392257 => 15:18:00.956765
[0m15:18:00.957405 [debug] [Thread-7  ]: Timing info for model.dbsql_dbt_tpch.DailyMarketIncremental (execute): 15:18:00.418443 => 15:18:00.957244
[0m15:18:00.957733 [debug] [Thread-4  ]: On model.dbsql_dbt_tpch.CustomerIncremental: ROLLBACK
[0m15:18:00.958056 [debug] [Thread-6  ]: Databricks adapter: NotImplemented: rollback
[0m15:18:00.958374 [debug] [Thread-1  ]: On model.dbsql_dbt_tpch.AccountIncremental: ROLLBACK
[0m15:18:00.958695 [debug] [Thread-7  ]: On model.dbsql_dbt_tpch.DailyMarketIncremental: ROLLBACK
[0m15:18:00.959014 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m15:18:00.959327 [debug] [Thread-6  ]: On model.dbsql_dbt_tpch.DailyMarketHistorical: Close
[0m15:18:00.959640 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m15:18:00.959955 [debug] [Thread-7  ]: Databricks adapter: NotImplemented: rollback
[0m15:18:00.960468 [debug] [Thread-4  ]: On model.dbsql_dbt_tpch.CustomerIncremental: Close
[0m15:18:00.968932 [debug] [Thread-3  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CashTransactionIncremental"} */
create or replace view `main`.`tpcdi`.`CashTransactionIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`CashTransactionIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`CashTransactionIncrementaltres`

[0m15:18:00.969825 [debug] [Thread-1  ]: On model.dbsql_dbt_tpch.AccountIncremental: Close
[0m15:18:00.970461 [debug] [Thread-7  ]: On model.dbsql_dbt_tpch.DailyMarketIncremental: Close
[0m15:18:00.972604 [debug] [Thread-2  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.BatchDate"} */
create or replace view `main`.`tpcdi`.`BatchDate`
  
  
  as
    

select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`BatchDateuno`

 UNION ALL

select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`BatchDatedos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`BatchDatetres`

[0m15:18:00.973345 [debug] [Thread-3  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`CashTransactionIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:18:00.974725 [debug] [Thread-2  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`BatchDateuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:18:00.975404 [debug] [Thread-3  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`CashTransactionIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`CashTransactionIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:18:00.976115 [debug] [Thread-2  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`BatchDateuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`BatchDateuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:18:00.976598 [debug] [Thread-3  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xdec]\x15\x98\xa0\x81\x85Q\xe5\xdeV\x18'
[0m15:18:00.976866 [debug] [Thread-2  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xdecU\x10v\xbd\x97|\xfc\xa2\x10[\x95'
[0m15:18:00.977437 [debug] [Thread-3  ]: Timing info for model.dbsql_dbt_tpch.CashTransactionIncremental (execute): 15:18:00.411911 => 15:18:00.977228
[0m15:18:00.977912 [debug] [Thread-2  ]: Timing info for model.dbsql_dbt_tpch.BatchDate (execute): 15:18:00.406813 => 15:18:00.977747
[0m15:18:00.978229 [debug] [Thread-3  ]: On model.dbsql_dbt_tpch.CashTransactionIncremental: ROLLBACK
[0m15:18:00.978559 [debug] [Thread-2  ]: On model.dbsql_dbt_tpch.BatchDate: ROLLBACK
[0m15:18:00.979093 [debug] [Thread-3  ]: Databricks adapter: NotImplemented: rollback
[0m15:18:00.979537 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m15:18:00.979864 [debug] [Thread-3  ]: On model.dbsql_dbt_tpch.CashTransactionIncremental: Close
[0m15:18:00.980234 [debug] [Thread-2  ]: On model.dbsql_dbt_tpch.BatchDate: Close
[0m15:18:01.032175 [debug] [Thread-4  ]: Runtime Error in model CustomerIncremental (models/base/CustomerIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`customerincrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:18:01.032883 [debug] [Thread-6  ]: Runtime Error in model DailyMarketHistorical (models/base/DailyMarketHistorical.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketHistorical` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:18:01.033287 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '06c16f55-c18f-4f75-8098-5f4f4f702661', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14150a2b0>]}
[0m15:18:01.033627 [debug] [Thread-6  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '06c16f55-c18f-4f75-8098-5f4f4f702661', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1415bd190>]}
[0m15:18:01.034038 [error] [Thread-4  ]: 4 of 45 ERROR creating sql view model tpcdi.CustomerIncremental ................ [[31mERROR[0m in 0.69s]
[0m15:18:01.034443 [error] [Thread-6  ]: 6 of 45 ERROR creating sql view model tpcdi.DailyMarketHistorical .............. [[31mERROR[0m in 0.69s]
[0m15:18:01.034889 [debug] [Thread-4  ]: Finished running node model.dbsql_dbt_tpch.CustomerIncremental
[0m15:18:01.035165 [debug] [Thread-6  ]: Finished running node model.dbsql_dbt_tpch.DailyMarketHistorical
[0m15:18:01.040485 [debug] [Thread-3  ]: Runtime Error in model CashTransactionIncremental (models/base/CashTransactionIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`CashTransactionIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:18:01.040919 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '06c16f55-c18f-4f75-8098-5f4f4f702661', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14170e790>]}
[0m15:18:01.041289 [error] [Thread-3  ]: 3 of 45 ERROR creating sql view model tpcdi.CashTransactionIncremental ......... [[31mERROR[0m in 0.70s]
[0m15:18:01.041604 [debug] [Thread-3  ]: Finished running node model.dbsql_dbt_tpch.CashTransactionIncremental
[0m15:18:01.044951 [debug] [Thread-2  ]: Runtime Error in model BatchDate (models/base/BatchDate.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`BatchDateuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:18:01.045531 [debug] [Thread-7  ]: Runtime Error in model DailyMarketIncremental (models/base/DailyMarketIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:18:01.046155 [debug] [Thread-1  ]: Runtime Error in model AccountIncremental (models/base/AccountIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`AccountIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:18:01.046410 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '06c16f55-c18f-4f75-8098-5f4f4f702661', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14158f6a0>]}
[0m15:18:01.046665 [debug] [Thread-7  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '06c16f55-c18f-4f75-8098-5f4f4f702661', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14158f520>]}
[0m15:18:01.046897 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '06c16f55-c18f-4f75-8098-5f4f4f702661', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1417a4400>]}
[0m15:18:01.047242 [error] [Thread-2  ]: 2 of 45 ERROR creating sql view model tpcdi.BatchDate .......................... [[31mERROR[0m in 0.71s]
[0m15:18:01.047642 [error] [Thread-7  ]: 7 of 45 ERROR creating sql view model tpcdi.DailyMarketIncremental ............. [[31mERROR[0m in 0.71s]
[0m15:18:01.048023 [error] [Thread-1  ]: 1 of 45 ERROR creating sql view model tpcdi.AccountIncremental ................. [[31mERROR[0m in 0.71s]
[0m15:18:01.048425 [debug] [Thread-2  ]: Finished running node model.dbsql_dbt_tpch.BatchDate
[0m15:18:01.048724 [debug] [Thread-7  ]: Finished running node model.dbsql_dbt_tpch.DailyMarketIncremental
[0m15:18:01.048992 [debug] [Thread-1  ]: Finished running node model.dbsql_dbt_tpch.AccountIncremental
[0m15:18:01.049583 [debug] [Thread-15 ]: Began running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
[0m15:18:01.049918 [info ] [Thread-15 ]: 14 of 45 SKIP relation tpcdi.tempDailyMarketHistorical ......................... [[33mSKIP[0m]
[0m15:18:01.050194 [debug] [Thread-15 ]: Finished running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
[0m15:18:01.071503 [debug] [Thread-10 ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.HoldingIncremental"} */
create or replace view `main`.`tpcdi`.`HoldingIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`HoldingIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`HoldingIncrementaltres`

[0m15:18:01.071909 [debug] [Thread-10 ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HoldingIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:18:01.072250 [debug] [Thread-10 ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HoldingIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HoldingIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:18:01.073259 [debug] [Thread-13 ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.WatchIncremental"} */
create or replace view `main`.`tpcdi`.`WatchIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`WatchIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`WatchIncrementaltres`

[0m15:18:01.073489 [debug] [Thread-10 ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xdecv\x12\xbe\xad\x8c\x90\xe5\r \x08\x9d'
[0m15:18:01.073730 [debug] [Thread-13 ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`WatchIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:18:01.074048 [debug] [Thread-10 ]: Timing info for model.dbsql_dbt_tpch.HoldingIncremental (execute): 15:18:00.443721 => 15:18:01.073940
[0m15:18:01.074369 [debug] [Thread-13 ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`WatchIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`WatchIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:18:01.074704 [debug] [Thread-10 ]: On model.dbsql_dbt_tpch.HoldingIncremental: ROLLBACK
[0m15:18:01.074900 [debug] [Thread-13 ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xdecv\x12\x8f\x87\xac\xbbO\xd4\xc6x\xcf'
[0m15:18:01.075080 [debug] [Thread-10 ]: Databricks adapter: NotImplemented: rollback
[0m15:18:01.075338 [debug] [Thread-13 ]: Timing info for model.dbsql_dbt_tpch.WatchIncremental (execute): 15:18:00.450761 => 15:18:01.075245
[0m15:18:01.075513 [debug] [Thread-10 ]: On model.dbsql_dbt_tpch.HoldingIncremental: Close
[0m15:18:01.075690 [debug] [Thread-13 ]: On model.dbsql_dbt_tpch.WatchIncremental: ROLLBACK
[0m15:18:01.076097 [debug] [Thread-13 ]: Databricks adapter: NotImplemented: rollback
[0m15:18:01.076272 [debug] [Thread-13 ]: On model.dbsql_dbt_tpch.WatchIncremental: Close
[0m15:18:01.080412 [debug] [Thread-11 ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.ProspectRaw"} */
create or replace view `main`.`tpcdi`.`ProspectRaw`
  
  
  as
    

select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`ProspectRawuno`

 UNION ALL

select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`ProspectRawdos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`ProspectRawtres`

[0m15:18:01.080693 [debug] [Thread-11 ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`ProspectRawuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:18:01.081046 [debug] [Thread-11 ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`ProspectRawuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`ProspectRawuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:18:01.081373 [debug] [Thread-11 ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xdecv\x12\xec\xb5\x0c;\xc6\xe2\xd5\xfc\x1a'
[0m15:18:01.081667 [debug] [Thread-11 ]: Timing info for model.dbsql_dbt_tpch.ProspectRaw (execute): 15:18:00.446351 => 15:18:01.081558
[0m15:18:01.081875 [debug] [Thread-11 ]: On model.dbsql_dbt_tpch.ProspectRaw: ROLLBACK
[0m15:18:01.082060 [debug] [Thread-11 ]: Databricks adapter: NotImplemented: rollback
[0m15:18:01.082233 [debug] [Thread-11 ]: On model.dbsql_dbt_tpch.ProspectRaw: Close
[0m15:18:01.091052 [debug] [Thread-12 ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.TradeIncremental"} */
create or replace view `main`.`tpcdi`.`TradeIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`TradeIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`TradeIncrementaltres`

[0m15:18:01.091468 [debug] [Thread-12 ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`TradeIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:18:01.091871 [debug] [Thread-12 ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`TradeIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`TradeIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:18:01.092241 [debug] [Thread-12 ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xdecw\x17!\x85\x0cs\x18\xec\x11\xba_'
[0m15:18:01.092579 [debug] [Thread-12 ]: Timing info for model.dbsql_dbt_tpch.TradeIncremental (execute): 15:18:00.448467 => 15:18:01.092457
[0m15:18:01.092793 [debug] [Thread-12 ]: On model.dbsql_dbt_tpch.TradeIncremental: ROLLBACK
[0m15:18:01.092982 [debug] [Thread-12 ]: Databricks adapter: NotImplemented: rollback
[0m15:18:01.093162 [debug] [Thread-12 ]: On model.dbsql_dbt_tpch.TradeIncremental: Close
[0m15:18:01.140772 [debug] [Thread-10 ]: Runtime Error in model HoldingIncremental (models/base/HoldingIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HoldingIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:18:01.141206 [debug] [Thread-10 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '06c16f55-c18f-4f75-8098-5f4f4f702661', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1417c4f40>]}
[0m15:18:01.141635 [error] [Thread-10 ]: 10 of 45 ERROR creating sql view model tpcdi.HoldingIncremental ................ [[31mERROR[0m in 0.80s]
[0m15:18:01.142031 [debug] [Thread-10 ]: Finished running node model.dbsql_dbt_tpch.HoldingIncremental
[0m15:18:01.146851 [debug] [Thread-5  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CustomerMgmtView"} */
create or replace view `main`.`tpcdi`.`CustomerMgmtView`
  
  
  as
    
select
    *
from
    hive_metastore.roberto_salcido_tpcdi_stage.customermgmt10

[0m15:18:01.148015 [debug] [Thread-8  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

  
    
        create or replace table `main`.`tpcdi`.`DimBroker`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT

  cast(employeeid as BIGINT) brokerid,
  cast(managerid as BIGINT) managerid,
  employeefirstname firstname,
  employeelastname lastname,
  employeemi middleinitial,
  employeebranch branch,
  employeeoffice office,
  employeephone phone,
  true iscurrent,
  1 batchid,
  (SELECT min(to_date(datevalue)) as effectivedate FROM `main`.`tpcdi`.`DimDate`) effectivedate,
  date('9999-12-31') enddate,
  bigint(concat(date_format(enddate, 'yyyyMMdd'), cast(brokerid as string))) as sk_brokerid
FROM  `main`.`tpcdi`.`HR`
WHERE employeejobcode = 314
  
[0m15:18:01.148366 [debug] [Thread-5  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`roberto_salcido_tpcdi_stage`.`customermgmt10` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 10 pos 4
[0m15:18:01.149952 [debug] [Thread-13 ]: Runtime Error in model WatchIncremental (models/base/WatchIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`WatchIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:18:01.150353 [debug] [Thread-8  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HR` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 33 pos 6
[0m15:18:01.150960 [debug] [Thread-5  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`roberto_salcido_tpcdi_stage`.`customermgmt10` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 10 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`roberto_salcido_tpcdi_stage`.`customermgmt10` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 10 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:18:01.151546 [debug] [Thread-13 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '06c16f55-c18f-4f75-8098-5f4f4f702661', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107ea4a30>]}
[0m15:18:01.152277 [debug] [Thread-11 ]: Runtime Error in model ProspectRaw (models/base/ProspectRaw.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`ProspectRawuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:18:01.152788 [debug] [Thread-8  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HR` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 33 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HR` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 33 pos 6
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:18:01.153222 [debug] [Thread-5  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xdec]\x14\x93\xa3\x0f\x9a\xd3W\xfcU\xab'
[0m15:18:01.153689 [error] [Thread-13 ]: 13 of 45 ERROR creating sql view model tpcdi.WatchIncremental .................. [[31mERROR[0m in 0.81s]
[0m15:18:01.154097 [debug] [Thread-11 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '06c16f55-c18f-4f75-8098-5f4f4f702661', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x146d05910>]}
[0m15:18:01.154387 [debug] [Thread-8  ]: Databricks adapter: operation-id: b"\x01\xee\x0e\xdecv\x14#\xbb'\xcd\x9f\x16Q\xb6C"
[0m15:18:01.154783 [debug] [Thread-5  ]: Timing info for model.dbsql_dbt_tpch.CustomerMgmtView (execute): 15:18:00.414211 => 15:18:01.154643
[0m15:18:01.155089 [debug] [Thread-13 ]: Finished running node model.dbsql_dbt_tpch.WatchIncremental
[0m15:18:01.155523 [error] [Thread-11 ]: 11 of 45 ERROR creating sql view model tpcdi.ProspectRaw ....................... [[31mERROR[0m in 0.81s]
[0m15:18:01.155933 [debug] [Thread-8  ]: Timing info for model.dbsql_dbt_tpch.DimBroker (execute): 15:18:00.420816 => 15:18:01.155809
[0m15:18:01.156202 [debug] [Thread-5  ]: On model.dbsql_dbt_tpch.CustomerMgmtView: ROLLBACK
[0m15:18:01.157296 [debug] [Thread-11 ]: Finished running node model.dbsql_dbt_tpch.ProspectRaw
[0m15:18:01.157596 [debug] [Thread-8  ]: On model.dbsql_dbt_tpch.DimBroker: ROLLBACK
[0m15:18:01.157879 [debug] [Thread-5  ]: Databricks adapter: NotImplemented: rollback
[0m15:18:01.158511 [debug] [Thread-12 ]: Runtime Error in model TradeIncremental (models/base/TradeIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`TradeIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:18:01.158839 [debug] [Thread-8  ]: Databricks adapter: NotImplemented: rollback
[0m15:18:01.159047 [debug] [Thread-5  ]: On model.dbsql_dbt_tpch.CustomerMgmtView: Close
[0m15:18:01.159277 [debug] [Thread-12 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '06c16f55-c18f-4f75-8098-5f4f4f702661', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1371999d0>]}
[0m15:18:01.159505 [debug] [Thread-8  ]: On model.dbsql_dbt_tpch.DimBroker: Close
[0m15:18:01.160174 [error] [Thread-12 ]: 12 of 45 ERROR creating sql view model tpcdi.TradeIncremental .................. [[31mERROR[0m in 0.82s]
[0m15:18:01.160908 [debug] [Thread-12 ]: Finished running node model.dbsql_dbt_tpch.TradeIncremental
[0m15:18:01.229807 [debug] [Thread-5  ]: Runtime Error in model CustomerMgmtView (models/base/CustomerMgmtView.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`roberto_salcido_tpcdi_stage`.`customermgmt10` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 10 pos 4
[0m15:18:01.230316 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '06c16f55-c18f-4f75-8098-5f4f4f702661', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120a6bdc0>]}
[0m15:18:01.230783 [error] [Thread-5  ]: 5 of 45 ERROR creating sql view model tpcdi.CustomerMgmtView ................... [[31mERROR[0m in 0.89s]
[0m15:18:01.231188 [debug] [Thread-5  ]: Finished running node model.dbsql_dbt_tpch.CustomerMgmtView
[0m15:18:01.231683 [debug] [Thread-17 ]: Began running node model.dbsql_dbt_tpch.DimCustomerStg
[0m15:18:01.232013 [info ] [Thread-17 ]: 15 of 45 SKIP relation tpcdi.DimCustomerStg .................................... [[33mSKIP[0m]
[0m15:18:01.232369 [debug] [Thread-17 ]: Finished running node model.dbsql_dbt_tpch.DimCustomerStg
[0m15:18:01.232928 [debug] [Thread-19 ]: Began running node test.dbsql_dbt_tpch.dateval_DimCustomerStg_effectivedate.8a3a23ae4e
[0m15:18:01.233296 [info ] [Thread-19 ]: 16 of 45 SKIP test dateval_DimCustomerStg_effectivedate ........................ [[33mSKIP[0m]
[0m15:18:01.233692 [debug] [Thread-19 ]: Finished running node test.dbsql_dbt_tpch.dateval_DimCustomerStg_effectivedate.8a3a23ae4e
[0m15:18:01.234911 [debug] [Thread-21 ]: Began running node model.dbsql_dbt_tpch.Prospect
[0m15:18:01.235243 [info ] [Thread-21 ]: 17 of 45 SKIP relation tpcdi.Prospect .......................................... [[33mSKIP[0m]
[0m15:18:01.235630 [debug] [Thread-21 ]: Finished running node model.dbsql_dbt_tpch.Prospect
[0m15:18:01.236356 [debug] [Thread-8  ]: Runtime Error in model DimBroker (models/silver/DimBroker.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HR` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 33 pos 6
[0m15:18:01.236830 [debug] [Thread-8  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '06c16f55-c18f-4f75-8098-5f4f4f702661', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13714b730>]}
[0m15:18:01.237264 [error] [Thread-8  ]: 8 of 45 ERROR creating sql table model tpcdi.DimBroker ......................... [[31mERROR[0m in 0.90s]
[0m15:18:01.237606 [debug] [Thread-23 ]: Began running node model.dbsql_dbt_tpch.DimCustomer
[0m15:18:01.237887 [debug] [Thread-8  ]: Finished running node model.dbsql_dbt_tpch.DimBroker
[0m15:18:01.238163 [info ] [Thread-23 ]: 18 of 45 SKIP relation tpcdi.DimCustomer ....................................... [[33mSKIP[0m]
[0m15:18:01.238677 [debug] [Thread-23 ]: Finished running node model.dbsql_dbt_tpch.DimCustomer
[0m15:18:01.238899 [debug] [Thread-25 ]: Began running node model.dbsql_dbt_tpch.DimAccount
[0m15:18:01.239314 [info ] [Thread-25 ]: 19 of 45 SKIP relation tpcdi.DimAccount ........................................ [[33mSKIP[0m]
[0m15:18:01.239642 [debug] [Thread-4  ]: Began running node test.dbsql_dbt_tpch.accepted_values_DimCustomer_tier__1__2__3.7be85b3b7a
[0m15:18:01.239857 [debug] [Thread-25 ]: Finished running node model.dbsql_dbt_tpch.DimAccount
[0m15:18:01.240055 [debug] [Thread-6  ]: Began running node test.dbsql_dbt_tpch.not_null_DimCustomer_tier.e0964b5cac
[0m15:18:01.240300 [info ] [Thread-4  ]: 20 of 45 SKIP test accepted_values_DimCustomer_tier__1__2__3 ................... [[33mSKIP[0m]
[0m15:18:01.240814 [info ] [Thread-6  ]: 21 of 45 SKIP test not_null_DimCustomer_tier ................................... [[33mSKIP[0m]
[0m15:18:01.241314 [debug] [Thread-2  ]: Began running node test.dbsql_dbt_tpch.not_null_DimAccount_sk_brokerid.dd6e992726
[0m15:18:01.241622 [debug] [Thread-7  ]: Began running node test.dbsql_dbt_tpch.not_null_DimAccount_sk_customerid.b8baa6ce87
[0m15:18:01.241848 [debug] [Thread-4  ]: Finished running node test.dbsql_dbt_tpch.accepted_values_DimCustomer_tier__1__2__3.7be85b3b7a
[0m15:18:01.242112 [debug] [Thread-6  ]: Finished running node test.dbsql_dbt_tpch.not_null_DimCustomer_tier.e0964b5cac
[0m15:18:01.242369 [info ] [Thread-2  ]: 22 of 45 SKIP test not_null_DimAccount_sk_brokerid ............................. [[33mSKIP[0m]
[0m15:18:01.242669 [info ] [Thread-7  ]: 23 of 45 SKIP test not_null_DimAccount_sk_customerid ........................... [[33mSKIP[0m]
[0m15:18:01.243225 [debug] [Thread-2  ]: Finished running node test.dbsql_dbt_tpch.not_null_DimAccount_sk_brokerid.dd6e992726
[0m15:18:01.243561 [debug] [Thread-7  ]: Finished running node test.dbsql_dbt_tpch.not_null_DimAccount_sk_customerid.b8baa6ce87
[0m15:18:01.244015 [debug] [Thread-1  ]: Began running node model.dbsql_dbt_tpch.FactCashBalances
[0m15:18:01.244304 [info ] [Thread-1  ]: 24 of 45 SKIP relation tpcdi.FactCashBalances .................................. [[33mSKIP[0m]
[0m15:18:01.244647 [debug] [Thread-1  ]: Finished running node model.dbsql_dbt_tpch.FactCashBalances
[0m15:18:01.244968 [debug] [Thread-10 ]: Began running node test.dbsql_dbt_tpch.not_null_FactCashBalances_sk_accountid.878cf23033
[0m15:18:01.245217 [info ] [Thread-10 ]: 25 of 45 SKIP test not_null_FactCashBalances_sk_accountid ...................... [[33mSKIP[0m]
[0m15:18:01.245510 [debug] [Thread-10 ]: Finished running node test.dbsql_dbt_tpch.not_null_FactCashBalances_sk_accountid.878cf23033
[0m15:18:01.583981 [debug] [Thread-9  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.FinWire"} */
create or replace view `main`.`tpcdi`.`FinWire`
  
  
  as
    

select *, substring(value, 16, 3) rectype from 
text.`dbfs:/tmp/tpcdi/sf=10/Batch1/FINWIRE*`

[0m15:18:01.585593 [debug] [Thread-9  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: text; line 9 pos 0
[0m15:18:01.587311 [debug] [Thread-9  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: text; line 9 pos 0
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: text; line 9 pos 0
	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:76)
	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:171)
	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:93)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1306)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1305)
	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:81)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1335)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1332)
	at org.apache.spark.sql.catalyst.plans.logical.CreateView.mapChildren(v2Commands.scala:1350)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:79)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:78)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:93)
	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:51)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:229)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:229)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:226)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:218)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:296)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:296)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:296)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:383)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:376)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:283)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:376)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:304)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:379)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:355)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:156)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:348)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:380)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:817)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:380)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1040)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:377)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:150)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:140)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getRuntimeCategory$1(QueryRuntimePrediction.scala:300)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1040)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:299)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:349)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:345)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more
Caused by: org.apache.spark.sql.AnalysisException: [PATH_NOT_FOUND] Path does not exist: dbfs:/tmp/tpcdi/sf=10/Batch1/FINWIRE*.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:1592)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:844)
	at scala.collection.immutable.List.flatMap(List.scala:366)
	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:831)
	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:616)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:451)
	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:160)
	... 95 more

[0m15:18:01.588999 [debug] [Thread-9  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xdecp\x14\x03\xa4\x12\xf9\xe5U\xb6\x83\x9b'
[0m15:18:01.589824 [debug] [Thread-9  ]: Timing info for model.dbsql_dbt_tpch.FinWire (execute): 15:18:00.427157 => 15:18:01.589528
[0m15:18:01.590334 [debug] [Thread-9  ]: On model.dbsql_dbt_tpch.FinWire: ROLLBACK
[0m15:18:01.590734 [debug] [Thread-9  ]: Databricks adapter: NotImplemented: rollback
[0m15:18:01.591093 [debug] [Thread-9  ]: On model.dbsql_dbt_tpch.FinWire: Close
[0m15:18:01.655461 [debug] [Thread-9  ]: Runtime Error in model FinWire (models/base/FinWire.sql)
  [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: text; line 9 pos 0
[0m15:18:01.659779 [debug] [Thread-9  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '06c16f55-c18f-4f75-8098-5f4f4f702661', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1417a45b0>]}
[0m15:18:01.661666 [error] [Thread-9  ]: 9 of 45 ERROR creating sql view model tpcdi.FinWire ............................ [[31mERROR[0m in 1.32s]
[0m15:18:01.662867 [debug] [Thread-9  ]: Finished running node model.dbsql_dbt_tpch.FinWire
[0m15:18:01.664524 [debug] [Thread-11 ]: Began running node model.dbsql_dbt_tpch.DimCompany
[0m15:18:01.665131 [info ] [Thread-11 ]: 26 of 45 SKIP relation tpcdi.DimCompany ........................................ [[33mSKIP[0m]
[0m15:18:01.665750 [debug] [Thread-11 ]: Finished running node model.dbsql_dbt_tpch.DimCompany
[0m15:18:01.666569 [debug] [Thread-5  ]: Began running node test.dbsql_dbt_tpch.dateval_DimCompany_effectivedate.4df1621a42
[0m15:18:01.667197 [info ] [Thread-5  ]: 27 of 45 SKIP test dateval_DimCompany_effectivedate ............................ [[33mSKIP[0m]
[0m15:18:01.668163 [debug] [Thread-5  ]: Finished running node test.dbsql_dbt_tpch.dateval_DimCompany_effectivedate.4df1621a42
[0m15:18:01.669336 [debug] [Thread-17 ]: Began running node model.dbsql_dbt_tpch.DimSecurity
[0m15:18:01.669964 [debug] [Thread-20 ]: Began running node model.dbsql_dbt_tpch.Financial
[0m15:18:01.670689 [info ] [Thread-17 ]: 28 of 45 SKIP relation tpcdi.DimSecurity ....................................... [[33mSKIP[0m]
[0m15:18:01.671354 [info ] [Thread-20 ]: 29 of 45 SKIP relation tpcdi.Financial ......................................... [[33mSKIP[0m]
[0m15:18:01.671993 [debug] [Thread-17 ]: Finished running node model.dbsql_dbt_tpch.DimSecurity
[0m15:18:01.672458 [debug] [Thread-20 ]: Finished running node model.dbsql_dbt_tpch.Financial
[0m15:18:01.673354 [debug] [Thread-22 ]: Began running node test.dbsql_dbt_tpch.not_null_DimSecurity_sk_companyid.e728df82fe
[0m15:18:01.674035 [debug] [Thread-24 ]: Began running node test.dbsql_dbt_tpch.not_null_Financial_sk_companyid.529c7c2c12
[0m15:18:01.674467 [info ] [Thread-22 ]: 30 of 45 SKIP test not_null_DimSecurity_sk_companyid ........................... [[33mSKIP[0m]
[0m15:18:01.675011 [info ] [Thread-24 ]: 31 of 45 SKIP test not_null_Financial_sk_companyid ............................. [[33mSKIP[0m]
[0m15:18:01.675515 [debug] [Thread-22 ]: Finished running node test.dbsql_dbt_tpch.not_null_DimSecurity_sk_companyid.e728df82fe
[0m15:18:01.675891 [debug] [Thread-24 ]: Finished running node test.dbsql_dbt_tpch.not_null_Financial_sk_companyid.529c7c2c12
[0m15:18:01.676579 [debug] [Thread-14 ]: Began running node model.dbsql_dbt_tpch.DimTrade
[0m15:18:01.677067 [debug] [Thread-23 ]: Began running node model.dbsql_dbt_tpch.FactWatches
[0m15:18:01.677665 [debug] [Thread-3  ]: Began running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
[0m15:18:01.678215 [info ] [Thread-14 ]: 32 of 45 SKIP relation tpcdi.DimTrade .......................................... [[33mSKIP[0m]
[0m15:18:01.678852 [info ] [Thread-23 ]: 33 of 45 SKIP relation tpcdi.FactWatches ....................................... [[33mSKIP[0m]
[0m15:18:01.679376 [info ] [Thread-3  ]: 34 of 45 SKIP relation tpcdi.tempSumpFiBasicEps ................................ [[33mSKIP[0m]
[0m15:18:01.679936 [debug] [Thread-14 ]: Finished running node model.dbsql_dbt_tpch.DimTrade
[0m15:18:01.680341 [debug] [Thread-23 ]: Finished running node model.dbsql_dbt_tpch.FactWatches
[0m15:18:01.680664 [debug] [Thread-3  ]: Finished running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
[0m15:18:01.681592 [debug] [Thread-16 ]: Began running node test.dbsql_dbt_tpch.not_null_DimTrade_sk_accountid.4db26dca2c
[0m15:18:01.682001 [info ] [Thread-16 ]: 35 of 45 SKIP test not_null_DimTrade_sk_accountid .............................. [[33mSKIP[0m]
[0m15:18:01.682446 [debug] [Thread-4  ]: Began running node test.dbsql_dbt_tpch.not_null_DimTrade_sk_securityid.13bd8f9da6
[0m15:18:01.682895 [debug] [Thread-6  ]: Began running node test.dbsql_dbt_tpch.tradecom_DimTrade_commission.ad38ac2d42
[0m15:18:01.683287 [debug] [Thread-2  ]: Began running node test.dbsql_dbt_tpch.tradefee_DimTrade_fee.5ee05431ee
[0m15:18:01.683715 [debug] [Thread-16 ]: Finished running node test.dbsql_dbt_tpch.not_null_DimTrade_sk_accountid.4db26dca2c
[0m15:18:01.684087 [debug] [Thread-7  ]: Began running node test.dbsql_dbt_tpch.not_null_FactWatches_sk_customerid.1c7356b8c9
[0m15:18:01.684470 [debug] [Thread-15 ]: Began running node test.dbsql_dbt_tpch.not_null_FactWatches_sk_securityid.8a6b5e97b3
[0m15:18:01.684877 [debug] [Thread-1  ]: Began running node model.dbsql_dbt_tpch.FactMarketHistory
[0m15:18:01.685304 [info ] [Thread-4  ]: 36 of 45 SKIP test not_null_DimTrade_sk_securityid ............................. [[33mSKIP[0m]
[0m15:18:01.685821 [info ] [Thread-6  ]: 37 of 45 SKIP test tradecom_DimTrade_commission ................................ [[33mSKIP[0m]
[0m15:18:01.686280 [info ] [Thread-2  ]: 38 of 45 SKIP test tradefee_DimTrade_fee ....................................... [[33mSKIP[0m]
[0m15:18:01.686968 [info ] [Thread-7  ]: 39 of 45 SKIP test not_null_FactWatches_sk_customerid .......................... [[33mSKIP[0m]
[0m15:18:01.687548 [info ] [Thread-15 ]: 40 of 45 SKIP test not_null_FactWatches_sk_securityid .......................... [[33mSKIP[0m]
[0m15:18:01.688008 [info ] [Thread-1  ]: 41 of 45 SKIP relation tpcdi.FactMarketHistory ................................. [[33mSKIP[0m]
[0m15:18:01.688526 [debug] [Thread-4  ]: Finished running node test.dbsql_dbt_tpch.not_null_DimTrade_sk_securityid.13bd8f9da6
[0m15:18:01.688934 [debug] [Thread-6  ]: Finished running node test.dbsql_dbt_tpch.tradecom_DimTrade_commission.ad38ac2d42
[0m15:18:01.689237 [debug] [Thread-2  ]: Finished running node test.dbsql_dbt_tpch.tradefee_DimTrade_fee.5ee05431ee
[0m15:18:01.689551 [debug] [Thread-7  ]: Finished running node test.dbsql_dbt_tpch.not_null_FactWatches_sk_customerid.1c7356b8c9
[0m15:18:01.689840 [debug] [Thread-15 ]: Finished running node test.dbsql_dbt_tpch.not_null_FactWatches_sk_securityid.8a6b5e97b3
[0m15:18:01.690116 [debug] [Thread-1  ]: Finished running node model.dbsql_dbt_tpch.FactMarketHistory
[0m15:18:01.690735 [debug] [Thread-10 ]: Began running node model.dbsql_dbt_tpch.FactHoldings
[0m15:18:01.691372 [debug] [Thread-12 ]: Began running node test.dbsql_dbt_tpch.not_null_FactMarketHistory_PERatio.83761a27d0
[0m15:18:01.691678 [debug] [Thread-11 ]: Began running node test.dbsql_dbt_tpch.not_null_FactMarketHistory_sk_securityid.16a0e9b8b2
[0m15:18:01.691968 [info ] [Thread-10 ]: 42 of 45 SKIP relation tpcdi.FactHoldings ...................................... [[33mSKIP[0m]
[0m15:18:01.692350 [info ] [Thread-12 ]: 43 of 45 SKIP test not_null_FactMarketHistory_PERatio .......................... [[33mSKIP[0m]
[0m15:18:01.692703 [info ] [Thread-11 ]: 44 of 45 SKIP test not_null_FactMarketHistory_sk_securityid .................... [[33mSKIP[0m]
[0m15:18:01.693024 [debug] [Thread-10 ]: Finished running node model.dbsql_dbt_tpch.FactHoldings
[0m15:18:01.693270 [debug] [Thread-12 ]: Finished running node test.dbsql_dbt_tpch.not_null_FactMarketHistory_PERatio.83761a27d0
[0m15:18:01.693515 [debug] [Thread-11 ]: Finished running node test.dbsql_dbt_tpch.not_null_FactMarketHistory_sk_securityid.16a0e9b8b2
[0m15:18:01.693863 [debug] [Thread-5  ]: Began running node test.dbsql_dbt_tpch.not_null_FactHoldings_currentprice.98a44eac00
[0m15:18:01.694284 [info ] [Thread-5  ]: 45 of 45 SKIP test not_null_FactHoldings_currentprice .......................... [[33mSKIP[0m]
[0m15:18:01.694632 [debug] [Thread-5  ]: Finished running node test.dbsql_dbt_tpch.not_null_FactHoldings_currentprice.98a44eac00
[0m15:18:01.696337 [debug] [MainThread]: On master: ROLLBACK
[0m15:18:01.696590 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:18:01.880183 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:18:01.881101 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:18:01.881556 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:18:01.881978 [debug] [MainThread]: On master: ROLLBACK
[0m15:18:01.882376 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:18:01.882767 [debug] [MainThread]: On master: Close
[0m15:18:01.949585 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:18:01.951091 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.AccountIncremental' was properly closed.
[0m15:18:01.951569 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.BatchDate' was properly closed.
[0m15:18:01.951920 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.CashTransactionIncremental' was properly closed.
[0m15:18:01.952252 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.CustomerIncremental' was properly closed.
[0m15:18:01.952577 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.CustomerMgmtView' was properly closed.
[0m15:18:01.952907 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.DailyMarketHistorical' was properly closed.
[0m15:18:01.953174 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.DailyMarketIncremental' was properly closed.
[0m15:18:01.953430 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.DimBroker' was properly closed.
[0m15:18:01.953683 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.FinWire' was properly closed.
[0m15:18:01.953945 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.HoldingIncremental' was properly closed.
[0m15:18:01.954204 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.ProspectRaw' was properly closed.
[0m15:18:01.954457 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.TradeIncremental' was properly closed.
[0m15:18:01.954709 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.WatchIncremental' was properly closed.
[0m15:18:01.960018 [info ] [MainThread]: 
[0m15:18:01.960479 [info ] [MainThread]: Finished running 12 view models, 15 table models, 18 tests in 0 hours 0 minutes and 3.40 seconds (3.40s).
[0m15:18:01.963062 [debug] [MainThread]: Command end result
[0m15:18:02.014219 [info ] [MainThread]: 
[0m15:18:02.014573 [info ] [MainThread]: [31mCompleted with 13 errors and 0 warnings:[0m
[0m15:18:02.014787 [info ] [MainThread]: 
[0m15:18:02.014976 [error] [MainThread]: [33mRuntime Error in model CustomerIncremental (models/base/CustomerIncremental.sql)[0m
[0m15:18:02.015165 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`customerincrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:18:02.015355 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:18:02.015539 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:18:02.015716 [info ] [MainThread]: 
[0m15:18:02.015897 [error] [MainThread]: [33mRuntime Error in model DailyMarketHistorical (models/base/DailyMarketHistorical.sql)[0m
[0m15:18:02.016075 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketHistorical` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:18:02.016437 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:18:02.016692 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:18:02.016891 [info ] [MainThread]: 
[0m15:18:02.017090 [error] [MainThread]: [33mRuntime Error in model CashTransactionIncremental (models/base/CashTransactionIncremental.sql)[0m
[0m15:18:02.017281 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`CashTransactionIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:18:02.017472 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:18:02.017648 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:18:02.017825 [info ] [MainThread]: 
[0m15:18:02.018003 [error] [MainThread]: [33mRuntime Error in model BatchDate (models/base/BatchDate.sql)[0m
[0m15:18:02.018178 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`BatchDateuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:18:02.018361 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:18:02.018532 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:18:02.018704 [info ] [MainThread]: 
[0m15:18:02.018877 [error] [MainThread]: [33mRuntime Error in model DailyMarketIncremental (models/base/DailyMarketIncremental.sql)[0m
[0m15:18:02.019059 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:18:02.019238 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:18:02.019413 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:18:02.019586 [info ] [MainThread]: 
[0m15:18:02.019760 [error] [MainThread]: [33mRuntime Error in model AccountIncremental (models/base/AccountIncremental.sql)[0m
[0m15:18:02.019948 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`AccountIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:18:02.020120 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:18:02.020284 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:18:02.020445 [info ] [MainThread]: 
[0m15:18:02.020610 [error] [MainThread]: [33mRuntime Error in model HoldingIncremental (models/base/HoldingIncremental.sql)[0m
[0m15:18:02.020777 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HoldingIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:18:02.020944 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:18:02.021117 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:18:02.021285 [info ] [MainThread]: 
[0m15:18:02.021450 [error] [MainThread]: [33mRuntime Error in model WatchIncremental (models/base/WatchIncremental.sql)[0m
[0m15:18:02.021618 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`WatchIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:18:02.021784 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:18:02.021950 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:18:02.022111 [info ] [MainThread]: 
[0m15:18:02.022275 [error] [MainThread]: [33mRuntime Error in model ProspectRaw (models/base/ProspectRaw.sql)[0m
[0m15:18:02.022443 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`ProspectRawuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:18:02.022610 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:18:02.022779 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:18:02.022941 [info ] [MainThread]: 
[0m15:18:02.023107 [error] [MainThread]: [33mRuntime Error in model TradeIncremental (models/base/TradeIncremental.sql)[0m
[0m15:18:02.023277 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`TradeIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:18:02.023443 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:18:02.023610 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:18:02.023771 [info ] [MainThread]: 
[0m15:18:02.023937 [error] [MainThread]: [33mRuntime Error in model CustomerMgmtView (models/base/CustomerMgmtView.sql)[0m
[0m15:18:02.024274 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`roberto_salcido_tpcdi_stage`.`customermgmt10` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:18:02.024514 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:18:02.024698 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 10 pos 4
[0m15:18:02.024883 [info ] [MainThread]: 
[0m15:18:02.025073 [error] [MainThread]: [33mRuntime Error in model DimBroker (models/silver/DimBroker.sql)[0m
[0m15:18:02.025247 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HR` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:18:02.025422 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:18:02.025591 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 33 pos 6
[0m15:18:02.025762 [info ] [MainThread]: 
[0m15:18:02.025931 [error] [MainThread]: [33mRuntime Error in model FinWire (models/base/FinWire.sql)[0m
[0m15:18:02.026098 [error] [MainThread]:   [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: text; line 9 pos 0
[0m15:18:02.026300 [info ] [MainThread]: 
[0m15:18:02.026492 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=13 SKIP=32 TOTAL=45
[0m15:18:02.027139 [debug] [MainThread]: Command `dbt build` failed at 15:18:02.027077 after 4.35 seconds
[0m15:18:02.027356 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10708f220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1371c35e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ab7310>]}
[0m15:18:02.027561 [debug] [MainThread]: Flushing usage events
[0m15:25:55.135027 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108470670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1096cff10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109719ac0>]}


============================== 15:25:55.138776 | 99f9bed4-aa40-4fcf-85c0-76360e0e1894 ==============================
[0m15:25:55.138776 [info ] [MainThread]: Running with dbt=1.5.1
[0m15:25:55.139154 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:26:02.069706 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '99f9bed4-aa40-4fcf-85c0-76360e0e1894', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10500c040>]}
[0m15:26:02.077887 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '99f9bed4-aa40-4fcf-85c0-76360e0e1894', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x168d66cd0>]}
[0m15:26:02.105144 [debug] [MainThread]: checksum: 51f6b581eba8f8101bc020bf9faf8f96af641e2da86d581f66e2bdf0ff384b1c, vars: {}, profile: , target: , version: 1.5.1
[0m15:26:02.118797 [info ] [MainThread]: Unable to do partial parsing because of a version mismatch
[0m15:26:02.119117 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '99f9bed4-aa40-4fcf-85c0-76360e0e1894', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109738b20>]}
[0m15:26:03.002341 [debug] [MainThread]: 1699: static parser successfully parsed incremental/FactWatches.sql
[0m15:26:03.009483 [debug] [MainThread]: 1699: static parser successfully parsed incremental/Prospect.sql
[0m15:26:03.011258 [debug] [MainThread]: 1699: static parser successfully parsed incremental/FactCashBalances.sql
[0m15:26:03.012987 [debug] [MainThread]: 1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
[0m15:26:03.014793 [debug] [MainThread]: 1699: static parser successfully parsed incremental/FactHoldings.sql
[0m15:26:03.016702 [debug] [MainThread]: 1699: static parser successfully parsed incremental/FactMarketHistory.sql
[0m15:26:03.018425 [debug] [MainThread]: 1699: static parser successfully parsed incremental/DimCustomer.sql
[0m15:26:03.020122 [debug] [MainThread]: 1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
[0m15:26:03.022094 [debug] [MainThread]: 1699: static parser successfully parsed incremental/DimTrade.sql
[0m15:26:03.023878 [debug] [MainThread]: 1699: static parser successfully parsed incremental/DimAccount.sql
[0m15:26:03.025855 [debug] [MainThread]: 1699: static parser successfully parsed incremental/DimCustomerStg.sql
[0m15:26:03.027588 [debug] [MainThread]: 1699: static parser successfully parsed silver/DimCompany.sql
[0m15:26:03.029368 [debug] [MainThread]: 1699: static parser successfully parsed silver/DimBroker.sql
[0m15:26:03.031040 [debug] [MainThread]: 1699: static parser successfully parsed silver/Financial.sql
[0m15:26:03.032767 [debug] [MainThread]: 1699: static parser successfully parsed silver/DimSecurity.sql
[0m15:26:03.034551 [debug] [MainThread]: 1699: static parser successfully parsed base/CustomerIncremental.sql
[0m15:26:03.036155 [debug] [MainThread]: 1699: static parser successfully parsed base/CashTransactionIncremental.sql
[0m15:26:03.037706 [debug] [MainThread]: 1603: static parser failed on base/CustomerMgmtView.sql
[0m15:26:03.040337 [debug] [MainThread]: 1602: parser fallback to jinja rendering on base/CustomerMgmtView.sql
[0m15:26:03.041386 [debug] [MainThread]: 1699: static parser successfully parsed base/TradeIncremental.sql
[0m15:26:03.042974 [debug] [MainThread]: 1699: static parser successfully parsed base/HoldingIncremental.sql
[0m15:26:03.044533 [debug] [MainThread]: 1699: static parser successfully parsed base/DailyMarketIncremental.sql
[0m15:26:03.046179 [debug] [MainThread]: 1699: static parser successfully parsed base/BatchDate.sql
[0m15:26:03.047933 [debug] [MainThread]: 1699: static parser successfully parsed base/AccountIncremental.sql
[0m15:26:03.049544 [debug] [MainThread]: 1603: static parser failed on base/FinWire.sql
[0m15:26:03.052400 [debug] [MainThread]: 1602: parser fallback to jinja rendering on base/FinWire.sql
[0m15:26:03.053356 [debug] [MainThread]: 1699: static parser successfully parsed base/WatchIncremental.sql
[0m15:26:03.055141 [debug] [MainThread]: 1699: static parser successfully parsed base/ProspectRaw.sql
[0m15:26:03.056726 [debug] [MainThread]: 1699: static parser successfully parsed base/DailyMarketHistorical.sql
[0m15:26:03.299139 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '99f9bed4-aa40-4fcf-85c0-76360e0e1894', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x169241e50>]}
[0m15:26:03.343209 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '99f9bed4-aa40-4fcf-85c0-76360e0e1894', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1691d70d0>]}
[0m15:26:03.343532 [info ] [MainThread]: Found 27 models, 18 tests, 0 snapshots, 0 analyses, 669 macros, 0 operations, 0 seed files, 33 sources, 0 exposures, 0 metrics, 0 groups
[0m15:26:03.345560 [info ] [MainThread]: 
[0m15:26:03.346039 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m15:26:03.347727 [debug] [ThreadPool]: Acquiring new databricks connection 'list_main'
[0m15:26:03.348143 [debug] [ThreadPool]: Acquiring new databricks connection 'list_main'
[0m15:26:03.348374 [debug] [ThreadPool]: Using databricks connection "list_main"
[0m15:26:03.348598 [debug] [ThreadPool]: Using databricks connection "list_main"
[0m15:26:03.348789 [debug] [ThreadPool]: On list_main: GetSchemas(database=`main`, schema=None)
[0m15:26:03.348946 [debug] [ThreadPool]: On list_main: GetSchemas(database=`main`, schema=None)
[0m15:26:03.349110 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:26:03.349266 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:26:03.879122 [debug] [ThreadPool]: SQL status: OK in 0.5299999713897705 seconds
[0m15:26:03.885620 [debug] [ThreadPool]: On list_main: Close
[0m15:26:03.927939 [debug] [ThreadPool]: SQL status: OK in 0.5799999833106995 seconds
[0m15:26:03.930631 [debug] [ThreadPool]: On list_main: Close
[0m15:26:03.994319 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_main, now create_main_tpcdi)
[0m15:26:03.995617 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_main, now create_main_tpcdi_dbt_test__audit)
[0m15:26:03.996703 [debug] [ThreadPool]: Creating schema "database: "main"
schema: "tpcdi"
"
[0m15:26:03.997644 [debug] [ThreadPool]: Creating schema "database: "main"
schema: "tpcdi_dbt_test__audit"
"
[0m15:26:04.017574 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:26:04.018030 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:26:04.018279 [debug] [ThreadPool]: Using databricks connection "create_main_tpcdi"
[0m15:26:04.018523 [debug] [ThreadPool]: Using databricks connection "create_main_tpcdi_dbt_test__audit"
[0m15:26:04.018777 [debug] [ThreadPool]: On create_main_tpcdi: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "create_main_tpcdi"} */
create schema if not exists `main`.`tpcdi`
  
[0m15:26:04.019035 [debug] [ThreadPool]: On create_main_tpcdi_dbt_test__audit: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "create_main_tpcdi_dbt_test__audit"} */
create schema if not exists `main`.`tpcdi_dbt_test__audit`
  
[0m15:26:04.019282 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:26:04.019476 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:26:04.437195 [debug] [ThreadPool]: SQL status: OK in 0.41999998688697815 seconds
[0m15:26:04.437817 [debug] [ThreadPool]: SQL status: OK in 0.41999998688697815 seconds
[0m15:26:04.438723 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m15:26:04.439206 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m15:26:04.439401 [debug] [ThreadPool]: On create_main_tpcdi: ROLLBACK
[0m15:26:04.439553 [debug] [ThreadPool]: On create_main_tpcdi_dbt_test__audit: ROLLBACK
[0m15:26:04.439704 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m15:26:04.439843 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m15:26:04.439980 [debug] [ThreadPool]: On create_main_tpcdi: Close
[0m15:26:04.440109 [debug] [ThreadPool]: On create_main_tpcdi_dbt_test__audit: Close
[0m15:26:04.505380 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_main_tpcdi_dbt_test__audit, now list_main_tpcdi)
[0m15:26:04.506280 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_main_tpcdi, now list_main_tpcdi_dbt_test__audit)
[0m15:26:04.512611 [debug] [ThreadPool]: Using databricks connection "list_main_tpcdi"
[0m15:26:04.515229 [debug] [ThreadPool]: Using databricks connection "list_main_tpcdi_dbt_test__audit"
[0m15:26:04.515576 [debug] [ThreadPool]: On list_main_tpcdi: GetTables(database=main, schema=tpcdi, identifier=None)
[0m15:26:04.515857 [debug] [ThreadPool]: On list_main_tpcdi_dbt_test__audit: GetTables(database=main, schema=tpcdi_dbt_test__audit, identifier=None)
[0m15:26:04.516133 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:26:04.516396 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:26:04.927894 [debug] [ThreadPool]: SQL status: OK in 0.4099999964237213 seconds
[0m15:26:04.928431 [debug] [ThreadPool]: SQL status: OK in 0.4099999964237213 seconds
[0m15:26:04.932949 [debug] [ThreadPool]: On list_main_tpcdi_dbt_test__audit: Close
[0m15:26:04.934818 [debug] [ThreadPool]: On list_main_tpcdi: Close
[0m15:26:04.996813 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '99f9bed4-aa40-4fcf-85c0-76360e0e1894', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1691c4820>]}
[0m15:26:04.997886 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:26:04.998377 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:26:04.999356 [info ] [MainThread]: Concurrency: 25 threads (target='dev')
[0m15:26:04.999941 [info ] [MainThread]: 
[0m15:26:05.016925 [debug] [Thread-1  ]: Began running node model.dbsql_dbt_tpch.AccountIncremental
[0m15:26:05.017609 [debug] [Thread-2  ]: Began running node model.dbsql_dbt_tpch.BatchDate
[0m15:26:05.018064 [debug] [Thread-3  ]: Began running node model.dbsql_dbt_tpch.CashTransactionIncremental
[0m15:26:05.018926 [debug] [Thread-4  ]: Began running node model.dbsql_dbt_tpch.CustomerIncremental
[0m15:26:05.019343 [debug] [Thread-5  ]: Began running node model.dbsql_dbt_tpch.CustomerMgmtView
[0m15:26:05.018559 [info ] [Thread-1  ]: 1 of 45 START sql view model tpcdi.AccountIncremental .......................... [RUN]
[0m15:26:05.019861 [debug] [Thread-6  ]: Began running node model.dbsql_dbt_tpch.DailyMarketHistorical
[0m15:26:05.020634 [debug] [Thread-7  ]: Began running node model.dbsql_dbt_tpch.DailyMarketIncremental
[0m15:26:05.020272 [info ] [Thread-2  ]: 2 of 45 START sql view model tpcdi.BatchDate ................................... [RUN]
[0m15:26:05.021137 [debug] [Thread-8  ]: Began running node model.dbsql_dbt_tpch.DimBroker
[0m15:26:05.021490 [debug] [Thread-9  ]: Began running node model.dbsql_dbt_tpch.FinWire
[0m15:26:05.021879 [info ] [Thread-3  ]: 3 of 45 START sql view model tpcdi.CashTransactionIncremental .................. [RUN]
[0m15:26:05.022308 [debug] [Thread-10 ]: Began running node model.dbsql_dbt_tpch.HoldingIncremental
[0m15:26:05.022622 [debug] [Thread-11 ]: Began running node model.dbsql_dbt_tpch.ProspectRaw
[0m15:26:05.022944 [info ] [Thread-4  ]: 4 of 45 START sql view model tpcdi.CustomerIncremental ......................... [RUN]
[0m15:26:05.023315 [debug] [Thread-12 ]: Began running node model.dbsql_dbt_tpch.TradeIncremental
[0m15:26:05.023563 [debug] [Thread-13 ]: Began running node model.dbsql_dbt_tpch.WatchIncremental
[0m15:26:05.023854 [info ] [Thread-5  ]: 5 of 45 START sql view model tpcdi.CustomerMgmtView ............................ [RUN]
[0m15:26:05.024739 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_main_tpcdi, now model.dbsql_dbt_tpch.AccountIncremental)
[0m15:26:05.025281 [info ] [Thread-6  ]: 6 of 45 START sql view model tpcdi.DailyMarketHistorical ....................... [RUN]
[0m15:26:05.025832 [info ] [Thread-7  ]: 7 of 45 START sql view model tpcdi.DailyMarketIncremental ...................... [RUN]
[0m15:26:05.026504 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_main_tpcdi_dbt_test__audit, now model.dbsql_dbt_tpch.BatchDate)
[0m15:26:05.026898 [info ] [Thread-8  ]: 8 of 45 START sql table model tpcdi.DimBroker .................................. [RUN]
[0m15:26:05.027327 [info ] [Thread-9  ]: 9 of 45 START sql view model tpcdi.FinWire ..................................... [RUN]
[0m15:26:05.027979 [debug] [Thread-3  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.CashTransactionIncremental'
[0m15:26:05.028334 [info ] [Thread-10 ]: 10 of 45 START sql view model tpcdi.HoldingIncremental ......................... [RUN]
[0m15:26:05.028764 [info ] [Thread-11 ]: 11 of 45 START sql view model tpcdi.ProspectRaw ................................ [RUN]
[0m15:26:05.029402 [debug] [Thread-4  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.CustomerIncremental'
[0m15:26:05.029746 [info ] [Thread-12 ]: 12 of 45 START sql view model tpcdi.TradeIncremental ........................... [RUN]
[0m15:26:05.030150 [info ] [Thread-13 ]: 13 of 45 START sql view model tpcdi.WatchIncremental ........................... [RUN]
[0m15:26:05.030721 [debug] [Thread-5  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.CustomerMgmtView'
[0m15:26:05.030972 [debug] [Thread-1  ]: Began compiling node model.dbsql_dbt_tpch.AccountIncremental
[0m15:26:05.031436 [debug] [Thread-6  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.DailyMarketHistorical'
[0m15:26:05.031874 [debug] [Thread-7  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.DailyMarketIncremental'
[0m15:26:05.032109 [debug] [Thread-2  ]: Began compiling node model.dbsql_dbt_tpch.BatchDate
[0m15:26:05.032610 [debug] [Thread-8  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimBroker'
[0m15:26:05.033089 [debug] [Thread-9  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.FinWire'
[0m15:26:05.033346 [debug] [Thread-3  ]: Began compiling node model.dbsql_dbt_tpch.CashTransactionIncremental
[0m15:26:05.033809 [debug] [Thread-10 ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.HoldingIncremental'
[0m15:26:05.034272 [debug] [Thread-11 ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.ProspectRaw'
[0m15:26:05.034563 [debug] [Thread-4  ]: Began compiling node model.dbsql_dbt_tpch.CustomerIncremental
[0m15:26:05.035104 [debug] [Thread-12 ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.TradeIncremental'
[0m15:26:05.035556 [debug] [Thread-13 ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.WatchIncremental'
[0m15:26:05.035799 [debug] [Thread-5  ]: Began compiling node model.dbsql_dbt_tpch.CustomerMgmtView
[0m15:26:05.039397 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.AccountIncremental"
[0m15:26:05.039748 [debug] [Thread-6  ]: Began compiling node model.dbsql_dbt_tpch.DailyMarketHistorical
[0m15:26:05.040002 [debug] [Thread-7  ]: Began compiling node model.dbsql_dbt_tpch.DailyMarketIncremental
[0m15:26:05.042926 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.BatchDate"
[0m15:26:05.043209 [debug] [Thread-8  ]: Began compiling node model.dbsql_dbt_tpch.DimBroker
[0m15:26:05.043422 [debug] [Thread-9  ]: Began compiling node model.dbsql_dbt_tpch.FinWire
[0m15:26:05.045957 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.CashTransactionIncremental"
[0m15:26:05.046201 [debug] [Thread-10 ]: Began compiling node model.dbsql_dbt_tpch.HoldingIncremental
[0m15:26:05.046413 [debug] [Thread-11 ]: Began compiling node model.dbsql_dbt_tpch.ProspectRaw
[0m15:26:05.048890 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.CustomerIncremental"
[0m15:26:05.049125 [debug] [Thread-12 ]: Began compiling node model.dbsql_dbt_tpch.TradeIncremental
[0m15:26:05.049318 [debug] [Thread-13 ]: Began compiling node model.dbsql_dbt_tpch.WatchIncremental
[0m15:26:05.052496 [debug] [Thread-5  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.CustomerMgmtView"
[0m15:26:05.055419 [debug] [Thread-6  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.DailyMarketHistorical"
[0m15:26:05.058393 [debug] [Thread-7  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.DailyMarketIncremental"
[0m15:26:05.061041 [debug] [Thread-8  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.DimBroker"
[0m15:26:05.061332 [debug] [Thread-1  ]: Timing info for model.dbsql_dbt_tpch.AccountIncremental (compile): 15:26:05.035966 => 15:26:05.061204
[0m15:26:05.063408 [debug] [Thread-9  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.FinWire"
[0m15:26:05.066536 [debug] [Thread-10 ]: Writing injected SQL for node "model.dbsql_dbt_tpch.HoldingIncremental"
[0m15:26:05.066853 [debug] [Thread-2  ]: Timing info for model.dbsql_dbt_tpch.BatchDate (compile): 15:26:05.040175 => 15:26:05.066717
[0m15:26:05.069511 [debug] [Thread-11 ]: Writing injected SQL for node "model.dbsql_dbt_tpch.ProspectRaw"
[0m15:26:05.071681 [debug] [Thread-12 ]: Writing injected SQL for node "model.dbsql_dbt_tpch.TradeIncremental"
[0m15:26:05.071958 [debug] [Thread-3  ]: Timing info for model.dbsql_dbt_tpch.CashTransactionIncremental (compile): 15:26:05.043573 => 15:26:05.071829
[0m15:26:05.074042 [debug] [Thread-13 ]: Writing injected SQL for node "model.dbsql_dbt_tpch.WatchIncremental"
[0m15:26:05.074372 [debug] [Thread-4  ]: Timing info for model.dbsql_dbt_tpch.CustomerIncremental (compile): 15:26:05.046565 => 15:26:05.074248
[0m15:26:05.074688 [debug] [Thread-5  ]: Timing info for model.dbsql_dbt_tpch.CustomerMgmtView (compile): 15:26:05.049448 => 15:26:05.074581
[0m15:26:05.074948 [debug] [Thread-1  ]: Began executing node model.dbsql_dbt_tpch.AccountIncremental
[0m15:26:05.075195 [debug] [Thread-6  ]: Timing info for model.dbsql_dbt_tpch.DailyMarketHistorical (compile): 15:26:05.052898 => 15:26:05.075081
[0m15:26:05.075508 [debug] [Thread-7  ]: Timing info for model.dbsql_dbt_tpch.DailyMarketIncremental (compile): 15:26:05.055682 => 15:26:05.075409
[0m15:26:05.075715 [debug] [Thread-8  ]: Timing info for model.dbsql_dbt_tpch.DimBroker (compile): 15:26:05.058744 => 15:26:05.075617
[0m15:26:05.075933 [debug] [Thread-2  ]: Began executing node model.dbsql_dbt_tpch.BatchDate
[0m15:26:05.076146 [debug] [Thread-9  ]: Timing info for model.dbsql_dbt_tpch.FinWire (compile): 15:26:05.061477 => 15:26:05.076056
[0m15:26:05.076413 [debug] [Thread-3  ]: Began executing node model.dbsql_dbt_tpch.CashTransactionIncremental
[0m15:26:05.076628 [debug] [Thread-10 ]: Timing info for model.dbsql_dbt_tpch.HoldingIncremental (compile): 15:26:05.063612 => 15:26:05.076530
[0m15:26:05.076921 [debug] [Thread-11 ]: Timing info for model.dbsql_dbt_tpch.ProspectRaw (compile): 15:26:05.067036 => 15:26:05.076822
[0m15:26:05.077145 [debug] [Thread-12 ]: Timing info for model.dbsql_dbt_tpch.TradeIncremental (compile): 15:26:05.069747 => 15:26:05.077044
[0m15:26:05.077326 [debug] [Thread-4  ]: Began executing node model.dbsql_dbt_tpch.CustomerIncremental
[0m15:26:05.077506 [debug] [Thread-5  ]: Began executing node model.dbsql_dbt_tpch.CustomerMgmtView
[0m15:26:05.077704 [debug] [Thread-13 ]: Timing info for model.dbsql_dbt_tpch.WatchIncremental (compile): 15:26:05.072105 => 15:26:05.077616
[0m15:26:05.092078 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.AccountIncremental"
[0m15:26:05.092341 [debug] [Thread-6  ]: Began executing node model.dbsql_dbt_tpch.DailyMarketHistorical
[0m15:26:05.092531 [debug] [Thread-7  ]: Began executing node model.dbsql_dbt_tpch.DailyMarketIncremental
[0m15:26:05.092703 [debug] [Thread-8  ]: Began executing node model.dbsql_dbt_tpch.DimBroker
[0m15:26:05.095305 [debug] [Thread-2  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.BatchDate"
[0m15:26:05.095496 [debug] [Thread-9  ]: Began executing node model.dbsql_dbt_tpch.FinWire
[0m15:26:05.097412 [debug] [Thread-3  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.CashTransactionIncremental"
[0m15:26:05.097601 [debug] [Thread-10 ]: Began executing node model.dbsql_dbt_tpch.HoldingIncremental
[0m15:26:05.097767 [debug] [Thread-11 ]: Began executing node model.dbsql_dbt_tpch.ProspectRaw
[0m15:26:05.097929 [debug] [Thread-12 ]: Began executing node model.dbsql_dbt_tpch.TradeIncremental
[0m15:26:05.099709 [debug] [Thread-4  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.CustomerIncremental"
[0m15:26:05.101617 [debug] [Thread-5  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.CustomerMgmtView"
[0m15:26:05.101811 [debug] [Thread-13 ]: Began executing node model.dbsql_dbt_tpch.WatchIncremental
[0m15:26:05.103750 [debug] [Thread-6  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.DailyMarketHistorical"
[0m15:26:05.106106 [debug] [Thread-7  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.DailyMarketIncremental"
[0m15:26:05.119087 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:26:05.127763 [debug] [Thread-8  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.DimBroker"
[0m15:26:05.129742 [debug] [Thread-9  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.FinWire"
[0m15:26:05.131662 [debug] [Thread-10 ]: Writing runtime sql for node "model.dbsql_dbt_tpch.HoldingIncremental"
[0m15:26:05.131865 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:26:05.134253 [debug] [Thread-11 ]: Writing runtime sql for node "model.dbsql_dbt_tpch.ProspectRaw"
[0m15:26:05.136079 [debug] [Thread-12 ]: Writing runtime sql for node "model.dbsql_dbt_tpch.TradeIncremental"
[0m15:26:05.138108 [debug] [Thread-13 ]: Writing runtime sql for node "model.dbsql_dbt_tpch.WatchIncremental"
[0m15:26:05.138338 [debug] [Thread-3  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:26:05.138712 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:26:05.138897 [debug] [Thread-5  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:26:05.139061 [debug] [Thread-1  ]: Using databricks connection "model.dbsql_dbt_tpch.AccountIncremental"
[0m15:26:05.139252 [debug] [Thread-6  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:26:05.139485 [debug] [Thread-7  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:26:05.139708 [debug] [Thread-2  ]: Using databricks connection "model.dbsql_dbt_tpch.BatchDate"
[0m15:26:05.140007 [debug] [Thread-8  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:26:05.140202 [debug] [Thread-9  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:26:05.140377 [debug] [Thread-10 ]: Spark adapter: NotImplemented: add_begin_query
[0m15:26:05.140596 [debug] [Thread-11 ]: Spark adapter: NotImplemented: add_begin_query
[0m15:26:05.140760 [debug] [Thread-3  ]: Using databricks connection "model.dbsql_dbt_tpch.CashTransactionIncremental"
[0m15:26:05.140923 [debug] [Thread-4  ]: Using databricks connection "model.dbsql_dbt_tpch.CustomerIncremental"
[0m15:26:05.141119 [debug] [Thread-12 ]: Spark adapter: NotImplemented: add_begin_query
[0m15:26:05.141304 [debug] [Thread-13 ]: Spark adapter: NotImplemented: add_begin_query
[0m15:26:05.141424 [debug] [Thread-5  ]: Using databricks connection "model.dbsql_dbt_tpch.CustomerMgmtView"
[0m15:26:05.141589 [debug] [Thread-1  ]: On model.dbsql_dbt_tpch.AccountIncremental: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.AccountIncremental"} */
create or replace view `main`.`tpcdi`.`AccountIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`AccountIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`AccountIncrementaltres`

[0m15:26:05.141729 [debug] [Thread-6  ]: Using databricks connection "model.dbsql_dbt_tpch.DailyMarketHistorical"
[0m15:26:05.141852 [debug] [Thread-7  ]: Using databricks connection "model.dbsql_dbt_tpch.DailyMarketIncremental"
[0m15:26:05.142002 [debug] [Thread-2  ]: On model.dbsql_dbt_tpch.BatchDate: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.BatchDate"} */
create or replace view `main`.`tpcdi`.`BatchDate`
  
  
  as
    

select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`BatchDateuno`

 UNION ALL

select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`BatchDatedos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`BatchDatetres`

[0m15:26:05.142142 [debug] [Thread-8  ]: Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
[0m15:26:05.142264 [debug] [Thread-9  ]: Using databricks connection "model.dbsql_dbt_tpch.FinWire"
[0m15:26:05.142384 [debug] [Thread-10 ]: Using databricks connection "model.dbsql_dbt_tpch.HoldingIncremental"
[0m15:26:05.142499 [debug] [Thread-11 ]: Using databricks connection "model.dbsql_dbt_tpch.ProspectRaw"
[0m15:26:05.142647 [debug] [Thread-3  ]: On model.dbsql_dbt_tpch.CashTransactionIncremental: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CashTransactionIncremental"} */
create or replace view `main`.`tpcdi`.`CashTransactionIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`CashTransactionIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`CashTransactionIncrementaltres`

[0m15:26:05.142809 [debug] [Thread-4  ]: On model.dbsql_dbt_tpch.CustomerIncremental: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CustomerIncremental"} */
create or replace view `main`.`tpcdi`.`CustomerIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`customerincrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`customerincrementaltres`

[0m15:26:05.142947 [debug] [Thread-12 ]: Using databricks connection "model.dbsql_dbt_tpch.TradeIncremental"
[0m15:26:05.143068 [debug] [Thread-13 ]: Using databricks connection "model.dbsql_dbt_tpch.WatchIncremental"
[0m15:26:05.143208 [debug] [Thread-5  ]: On model.dbsql_dbt_tpch.CustomerMgmtView: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CustomerMgmtView"} */
create or replace view `main`.`tpcdi`.`CustomerMgmtView`
  
  
  as
    
select
    *
from
    hive_metastore.roberto_salcido_tpcdi_stage.customermgmt10

[0m15:26:05.143379 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:26:05.143521 [debug] [Thread-6  ]: On model.dbsql_dbt_tpch.DailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DailyMarketHistorical"} */
create or replace view `main`.`tpcdi`.`DailyMarketHistorical`
  
  
  as
    
select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`DailyMarketHistorical`

[0m15:26:05.143678 [debug] [Thread-7  ]: On model.dbsql_dbt_tpch.DailyMarketIncremental: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DailyMarketIncremental"} */
create or replace view `main`.`tpcdi`.`DailyMarketIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`DailyMarketIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`DailyMarketIncrementaltres`

[0m15:26:05.143844 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m15:26:05.144005 [debug] [Thread-8  ]: On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

  
    
        create or replace table `main`.`tpcdi`.`DimBroker`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT

  cast(employeeid as BIGINT) brokerid,
  cast(managerid as BIGINT) managerid,
  employeefirstname firstname,
  employeelastname lastname,
  employeemi middleinitial,
  employeebranch branch,
  employeeoffice office,
  employeephone phone,
  true iscurrent,
  1 batchid,
  (SELECT min(to_date(datevalue)) as effectivedate FROM `main`.`tpcdi`.`DimDate`) effectivedate,
  date('9999-12-31') enddate,
  bigint(concat(date_format(enddate, 'yyyyMMdd'), cast(brokerid as string))) as sk_brokerid
FROM  `main`.`tpcdi`.`HR`
WHERE employeejobcode = 314
  
[0m15:26:05.144172 [debug] [Thread-9  ]: On model.dbsql_dbt_tpch.FinWire: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.FinWire"} */
create or replace view `main`.`tpcdi`.`FinWire`
  
  
  as
    

select *, substring(value, 16, 3) rectype from 
text.`dbfs:/tmp/tpcdi/sf=10/Batch1/FINWIRE*`

[0m15:26:05.144328 [debug] [Thread-10 ]: On model.dbsql_dbt_tpch.HoldingIncremental: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.HoldingIncremental"} */
create or replace view `main`.`tpcdi`.`HoldingIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`HoldingIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`HoldingIncrementaltres`

[0m15:26:05.144488 [debug] [Thread-11 ]: On model.dbsql_dbt_tpch.ProspectRaw: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.ProspectRaw"} */
create or replace view `main`.`tpcdi`.`ProspectRaw`
  
  
  as
    

select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`ProspectRawuno`

 UNION ALL

select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`ProspectRawdos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`ProspectRawtres`

[0m15:26:05.144654 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m15:26:05.144810 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m15:26:05.144960 [debug] [Thread-12 ]: On model.dbsql_dbt_tpch.TradeIncremental: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.TradeIncremental"} */
create or replace view `main`.`tpcdi`.`TradeIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`TradeIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`TradeIncrementaltres`

[0m15:26:05.145117 [debug] [Thread-13 ]: On model.dbsql_dbt_tpch.WatchIncremental: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.WatchIncremental"} */
create or replace view `main`.`tpcdi`.`WatchIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`WatchIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`WatchIncrementaltres`

[0m15:26:05.145277 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m15:26:05.145505 [debug] [Thread-6  ]: Opening a new connection, currently in state init
[0m15:26:05.145674 [debug] [Thread-7  ]: Opening a new connection, currently in state init
[0m15:26:05.176318 [debug] [Thread-8  ]: Opening a new connection, currently in state init
[0m15:26:05.177024 [debug] [Thread-9  ]: Opening a new connection, currently in state init
[0m15:26:05.177198 [debug] [Thread-10 ]: Opening a new connection, currently in state init
[0m15:26:05.177350 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m15:26:05.177686 [debug] [Thread-12 ]: Opening a new connection, currently in state init
[0m15:26:05.209360 [debug] [Thread-13 ]: Opening a new connection, currently in state init
[0m15:26:05.690820 [debug] [Thread-2  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.BatchDate"} */
create or replace view `main`.`tpcdi`.`BatchDate`
  
  
  as
    

select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`BatchDateuno`

 UNION ALL

select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`BatchDatedos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`BatchDatetres`

[0m15:26:05.691393 [debug] [Thread-2  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`BatchDateuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:26:05.691912 [debug] [Thread-2  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`BatchDateuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`BatchDateuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:26:05.692393 [debug] [Thread-2  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xdf\x84S\x11\xd9\xbdn\x18\xac\xd8\x16\xfa\xcb'
[0m15:26:05.692867 [debug] [Thread-2  ]: Timing info for model.dbsql_dbt_tpch.BatchDate (execute): 15:26:05.092820 => 15:26:05.692698
[0m15:26:05.693158 [debug] [Thread-2  ]: On model.dbsql_dbt_tpch.BatchDate: ROLLBACK
[0m15:26:05.693425 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m15:26:05.693686 [debug] [Thread-2  ]: On model.dbsql_dbt_tpch.BatchDate: Close
[0m15:26:05.745466 [debug] [Thread-4  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CustomerIncremental"} */
create or replace view `main`.`tpcdi`.`CustomerIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`customerincrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`customerincrementaltres`

[0m15:26:05.745915 [debug] [Thread-4  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`customerincrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:26:05.746485 [debug] [Thread-4  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`customerincrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`customerincrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:26:05.747030 [debug] [Thread-4  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xdf\x84[\x1a\xbb\x98\xefsj\x8d\xb3\xf03'
[0m15:26:05.747538 [debug] [Thread-4  ]: Timing info for model.dbsql_dbt_tpch.CustomerIncremental (execute): 15:26:05.098041 => 15:26:05.747350
[0m15:26:05.747872 [debug] [Thread-4  ]: On model.dbsql_dbt_tpch.CustomerIncremental: ROLLBACK
[0m15:26:05.748184 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m15:26:05.748488 [debug] [Thread-4  ]: On model.dbsql_dbt_tpch.CustomerIncremental: Close
[0m15:26:05.760332 [debug] [Thread-2  ]: Runtime Error in model BatchDate (models/base/BatchDate.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`BatchDateuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:26:05.760846 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '99f9bed4-aa40-4fcf-85c0-76360e0e1894', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x168e8a820>]}
[0m15:26:05.762224 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.AccountIncremental"} */
create or replace view `main`.`tpcdi`.`AccountIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`AccountIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`AccountIncrementaltres`

[0m15:26:05.762845 [error] [Thread-2  ]: 2 of 45 ERROR creating sql view model tpcdi.BatchDate .......................... [[31mERROR[0m in 0.73s]
[0m15:26:05.763402 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`AccountIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:26:05.763865 [debug] [Thread-2  ]: Finished running node model.dbsql_dbt_tpch.BatchDate
[0m15:26:05.764437 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`AccountIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`AccountIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:26:05.765155 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xdf\x84[\x1eU\xaef\x11lus>\xa5'
[0m15:26:05.765636 [debug] [Thread-1  ]: Timing info for model.dbsql_dbt_tpch.AccountIncremental (execute): 15:26:05.077820 => 15:26:05.765461
[0m15:26:05.765956 [debug] [Thread-1  ]: On model.dbsql_dbt_tpch.AccountIncremental: ROLLBACK
[0m15:26:05.766248 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m15:26:05.766537 [debug] [Thread-1  ]: On model.dbsql_dbt_tpch.AccountIncremental: Close
[0m15:26:05.780977 [debug] [Thread-6  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DailyMarketHistorical"} */
create or replace view `main`.`tpcdi`.`DailyMarketHistorical`
  
  
  as
    
select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`DailyMarketHistorical`

[0m15:26:05.781400 [debug] [Thread-6  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketHistorical` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:26:05.781852 [debug] [Thread-6  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketHistorical` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketHistorical` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:26:05.782272 [debug] [Thread-6  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xdf\x84\\\x17\x10\x87hHrP\xb8\xfb\xfb'
[0m15:26:05.782687 [debug] [Thread-6  ]: Timing info for model.dbsql_dbt_tpch.DailyMarketHistorical (execute): 15:26:05.102036 => 15:26:05.782530
[0m15:26:05.782970 [debug] [Thread-6  ]: On model.dbsql_dbt_tpch.DailyMarketHistorical: ROLLBACK
[0m15:26:05.783225 [debug] [Thread-6  ]: Databricks adapter: NotImplemented: rollback
[0m15:26:05.783463 [debug] [Thread-6  ]: On model.dbsql_dbt_tpch.DailyMarketHistorical: Close
[0m15:26:05.791051 [debug] [Thread-7  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DailyMarketIncremental"} */
create or replace view `main`.`tpcdi`.`DailyMarketIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`DailyMarketIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`DailyMarketIncrementaltres`

[0m15:26:05.791407 [debug] [Thread-7  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:26:05.791861 [debug] [Thread-7  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:26:05.792278 [debug] [Thread-7  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xdf\x84a\x1b\x16\xa4\xc2tv\xf6[\xb5\xdd'
[0m15:26:05.792675 [debug] [Thread-7  ]: Timing info for model.dbsql_dbt_tpch.DailyMarketIncremental (execute): 15:26:05.103897 => 15:26:05.792526
[0m15:26:05.792939 [debug] [Thread-7  ]: On model.dbsql_dbt_tpch.DailyMarketIncremental: ROLLBACK
[0m15:26:05.793181 [debug] [Thread-7  ]: Databricks adapter: NotImplemented: rollback
[0m15:26:05.793413 [debug] [Thread-7  ]: On model.dbsql_dbt_tpch.DailyMarketIncremental: Close
[0m15:26:05.808627 [debug] [Thread-4  ]: Runtime Error in model CustomerIncremental (models/base/CustomerIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`customerincrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:26:05.809055 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '99f9bed4-aa40-4fcf-85c0-76360e0e1894', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1691b53a0>]}
[0m15:26:05.809508 [error] [Thread-4  ]: 4 of 45 ERROR creating sql view model tpcdi.CustomerIncremental ................ [[31mERROR[0m in 0.78s]
[0m15:26:05.809936 [debug] [Thread-4  ]: Finished running node model.dbsql_dbt_tpch.CustomerIncremental
[0m15:26:05.829592 [debug] [Thread-1  ]: Runtime Error in model AccountIncremental (models/base/AccountIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`AccountIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:26:05.830016 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '99f9bed4-aa40-4fcf-85c0-76360e0e1894', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x168eb78e0>]}
[0m15:26:05.830460 [error] [Thread-1  ]: 1 of 45 ERROR creating sql view model tpcdi.AccountIncremental ................. [[31mERROR[0m in 0.81s]
[0m15:26:05.830852 [debug] [Thread-1  ]: Finished running node model.dbsql_dbt_tpch.AccountIncremental
[0m15:26:05.839858 [debug] [Thread-3  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CashTransactionIncremental"} */
create or replace view `main`.`tpcdi`.`CashTransactionIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`CashTransactionIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`CashTransactionIncrementaltres`

[0m15:26:05.840248 [debug] [Thread-3  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`CashTransactionIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:26:05.840650 [debug] [Thread-3  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`CashTransactionIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`CashTransactionIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:26:05.841047 [debug] [Thread-3  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xdf\x84f\x1bx\xa3\x9e\xdf\xcc\x8bG\xe5\xab'
[0m15:26:05.841422 [debug] [Thread-3  ]: Timing info for model.dbsql_dbt_tpch.CashTransactionIncremental (execute): 15:26:05.095614 => 15:26:05.841279
[0m15:26:05.841674 [debug] [Thread-3  ]: On model.dbsql_dbt_tpch.CashTransactionIncremental: ROLLBACK
[0m15:26:05.841894 [debug] [Thread-3  ]: Databricks adapter: NotImplemented: rollback
[0m15:26:05.842108 [debug] [Thread-3  ]: On model.dbsql_dbt_tpch.CashTransactionIncremental: Close
[0m15:26:05.853451 [debug] [Thread-11 ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.ProspectRaw"} */
create or replace view `main`.`tpcdi`.`ProspectRaw`
  
  
  as
    

select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`ProspectRawuno`

 UNION ALL

select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`ProspectRawdos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`ProspectRawtres`

[0m15:26:05.854420 [debug] [Thread-11 ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`ProspectRawuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:26:05.854815 [debug] [Thread-11 ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`ProspectRawuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`ProspectRawuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:26:05.855169 [debug] [Thread-11 ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xdf\x84f\x1aY\x8e\xb1\x13s5\x9d6\xe2'
[0m15:26:05.855579 [debug] [Thread-11 ]: Timing info for model.dbsql_dbt_tpch.ProspectRaw (execute): 15:26:05.132020 => 15:26:05.855447
[0m15:26:05.855811 [debug] [Thread-11 ]: On model.dbsql_dbt_tpch.ProspectRaw: ROLLBACK
[0m15:26:05.856022 [debug] [Thread-11 ]: Databricks adapter: NotImplemented: rollback
[0m15:26:05.856223 [debug] [Thread-11 ]: On model.dbsql_dbt_tpch.ProspectRaw: Close
[0m15:26:05.857183 [debug] [Thread-6  ]: Runtime Error in model DailyMarketHistorical (models/base/DailyMarketHistorical.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketHistorical` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:26:05.857528 [debug] [Thread-6  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '99f9bed4-aa40-4fcf-85c0-76360e0e1894', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x168de1520>]}
[0m15:26:05.857916 [error] [Thread-6  ]: 6 of 45 ERROR creating sql view model tpcdi.DailyMarketHistorical .............. [[31mERROR[0m in 0.83s]
[0m15:26:05.858262 [debug] [Thread-6  ]: Finished running node model.dbsql_dbt_tpch.DailyMarketHistorical
[0m15:26:05.860308 [debug] [Thread-7  ]: Runtime Error in model DailyMarketIncremental (models/base/DailyMarketIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:26:05.860587 [debug] [Thread-7  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '99f9bed4-aa40-4fcf-85c0-76360e0e1894', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1691a3250>]}
[0m15:26:05.860917 [error] [Thread-7  ]: 7 of 45 ERROR creating sql view model tpcdi.DailyMarketIncremental ............. [[31mERROR[0m in 0.83s]
[0m15:26:05.861241 [debug] [Thread-7  ]: Finished running node model.dbsql_dbt_tpch.DailyMarketIncremental
[0m15:26:05.861597 [debug] [Thread-15 ]: Began running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
[0m15:26:05.861839 [info ] [Thread-15 ]: 14 of 45 SKIP relation tpcdi.tempDailyMarketHistorical ......................... [[33mSKIP[0m]
[0m15:26:05.862132 [debug] [Thread-15 ]: Finished running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
[0m15:26:05.883287 [debug] [Thread-10 ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.HoldingIncremental"} */
create or replace view `main`.`tpcdi`.`HoldingIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`HoldingIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`HoldingIncrementaltres`

[0m15:26:05.883653 [debug] [Thread-10 ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HoldingIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:26:05.884014 [debug] [Thread-10 ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HoldingIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HoldingIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:26:05.884348 [debug] [Thread-10 ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xdf\x84l\x15(\x84]\xe4\xb6\xeby\x8a\xaa'
[0m15:26:05.884689 [debug] [Thread-10 ]: Timing info for model.dbsql_dbt_tpch.HoldingIncremental (execute): 15:26:05.129973 => 15:26:05.884564
[0m15:26:05.884913 [debug] [Thread-10 ]: On model.dbsql_dbt_tpch.HoldingIncremental: ROLLBACK
[0m15:26:05.885131 [debug] [Thread-10 ]: Databricks adapter: NotImplemented: rollback
[0m15:26:05.885319 [debug] [Thread-10 ]: On model.dbsql_dbt_tpch.HoldingIncremental: Close
[0m15:26:05.890456 [debug] [Thread-5  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CustomerMgmtView"} */
create or replace view `main`.`tpcdi`.`CustomerMgmtView`
  
  
  as
    
select
    *
from
    hive_metastore.roberto_salcido_tpcdi_stage.customermgmt10

[0m15:26:05.890709 [debug] [Thread-5  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`roberto_salcido_tpcdi_stage`.`customermgmt10` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 10 pos 4
[0m15:26:05.891066 [debug] [Thread-5  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`roberto_salcido_tpcdi_stage`.`customermgmt10` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 10 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`roberto_salcido_tpcdi_stage`.`customermgmt10` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 10 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:26:05.891400 [debug] [Thread-5  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xdf\x84f\x1aT\x8d\x8a\x15v2\x10JB'
[0m15:26:05.891706 [debug] [Thread-5  ]: Timing info for model.dbsql_dbt_tpch.CustomerMgmtView (execute): 15:26:05.099840 => 15:26:05.891590
[0m15:26:05.891914 [debug] [Thread-5  ]: On model.dbsql_dbt_tpch.CustomerMgmtView: ROLLBACK
[0m15:26:05.892125 [debug] [Thread-5  ]: Databricks adapter: NotImplemented: rollback
[0m15:26:05.892310 [debug] [Thread-5  ]: On model.dbsql_dbt_tpch.CustomerMgmtView: Close
[0m15:26:05.903553 [debug] [Thread-12 ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.TradeIncremental"} */
create or replace view `main`.`tpcdi`.`TradeIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`TradeIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`TradeIncrementaltres`

[0m15:26:05.903871 [debug] [Thread-12 ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`TradeIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:26:05.904218 [debug] [Thread-12 ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`TradeIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`TradeIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:26:05.904555 [debug] [Thread-12 ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xdf\x84p\x1bT\x82\x1e\x01\xc1l\x88\xe8h'
[0m15:26:05.904893 [debug] [Thread-12 ]: Timing info for model.dbsql_dbt_tpch.TradeIncremental (execute): 15:26:05.134399 => 15:26:05.904773
[0m15:26:05.905106 [debug] [Thread-12 ]: On model.dbsql_dbt_tpch.TradeIncremental: ROLLBACK
[0m15:26:05.905301 [debug] [Thread-12 ]: Databricks adapter: NotImplemented: rollback
[0m15:26:05.905492 [debug] [Thread-12 ]: On model.dbsql_dbt_tpch.TradeIncremental: Close
[0m15:26:05.911850 [debug] [Thread-13 ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.WatchIncremental"} */
create or replace view `main`.`tpcdi`.`WatchIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`WatchIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`WatchIncrementaltres`

[0m15:26:05.912101 [debug] [Thread-13 ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`WatchIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:26:05.913017 [debug] [Thread-13 ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`WatchIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`WatchIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:26:05.913363 [debug] [Thread-13 ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xdf\x84m\x12\x8b\xbfz\xe8\xf7\x14VJ\x1b'
[0m15:26:05.913675 [debug] [Thread-13 ]: Timing info for model.dbsql_dbt_tpch.WatchIncremental (execute): 15:26:05.136335 => 15:26:05.913561
[0m15:26:05.913939 [debug] [Thread-13 ]: On model.dbsql_dbt_tpch.WatchIncremental: ROLLBACK
[0m15:26:05.914197 [debug] [Thread-13 ]: Databricks adapter: NotImplemented: rollback
[0m15:26:05.914462 [debug] [Thread-13 ]: On model.dbsql_dbt_tpch.WatchIncremental: Close
[0m15:26:05.915398 [debug] [Thread-3  ]: Runtime Error in model CashTransactionIncremental (models/base/CashTransactionIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`CashTransactionIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:26:05.915687 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '99f9bed4-aa40-4fcf-85c0-76360e0e1894', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1692cd2e0>]}
[0m15:26:05.916052 [error] [Thread-3  ]: 3 of 45 ERROR creating sql view model tpcdi.CashTransactionIncremental ......... [[31mERROR[0m in 0.89s]
[0m15:26:05.916382 [debug] [Thread-3  ]: Finished running node model.dbsql_dbt_tpch.CashTransactionIncremental
[0m15:26:05.919043 [debug] [Thread-11 ]: Runtime Error in model ProspectRaw (models/base/ProspectRaw.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`ProspectRawuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:26:05.919383 [debug] [Thread-11 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '99f9bed4-aa40-4fcf-85c0-76360e0e1894', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1690b4850>]}
[0m15:26:05.919752 [error] [Thread-11 ]: 11 of 45 ERROR creating sql view model tpcdi.ProspectRaw ....................... [[31mERROR[0m in 0.89s]
[0m15:26:05.920093 [debug] [Thread-11 ]: Finished running node model.dbsql_dbt_tpch.ProspectRaw
[0m15:26:05.937201 [debug] [Thread-8  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

  
    
        create or replace table `main`.`tpcdi`.`DimBroker`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT

  cast(employeeid as BIGINT) brokerid,
  cast(managerid as BIGINT) managerid,
  employeefirstname firstname,
  employeelastname lastname,
  employeemi middleinitial,
  employeebranch branch,
  employeeoffice office,
  employeephone phone,
  true iscurrent,
  1 batchid,
  (SELECT min(to_date(datevalue)) as effectivedate FROM `main`.`tpcdi`.`DimDate`) effectivedate,
  date('9999-12-31') enddate,
  bigint(concat(date_format(enddate, 'yyyyMMdd'), cast(brokerid as string))) as sk_brokerid
FROM  `main`.`tpcdi`.`HR`
WHERE employeejobcode = 314
  
[0m15:26:05.937558 [debug] [Thread-8  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HR` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 33 pos 6
[0m15:26:05.937896 [debug] [Thread-8  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HR` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 33 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HR` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 33 pos 6
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:26:05.938204 [debug] [Thread-8  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xdf\x84l\x15\x1b\xae\x84\xdc&\x17\x14\xbb\xe3'
[0m15:26:05.938521 [debug] [Thread-8  ]: Timing info for model.dbsql_dbt_tpch.DimBroker (execute): 15:26:05.106268 => 15:26:05.938406
[0m15:26:05.938722 [debug] [Thread-8  ]: On model.dbsql_dbt_tpch.DimBroker: ROLLBACK
[0m15:26:05.938900 [debug] [Thread-8  ]: Databricks adapter: NotImplemented: rollback
[0m15:26:05.939072 [debug] [Thread-8  ]: On model.dbsql_dbt_tpch.DimBroker: Close
[0m15:26:05.948371 [debug] [Thread-10 ]: Runtime Error in model HoldingIncremental (models/base/HoldingIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HoldingIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:26:05.948711 [debug] [Thread-10 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '99f9bed4-aa40-4fcf-85c0-76360e0e1894', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1692cd9a0>]}
[0m15:26:05.949077 [error] [Thread-10 ]: 10 of 45 ERROR creating sql view model tpcdi.HoldingIncremental ................ [[31mERROR[0m in 0.92s]
[0m15:26:05.949407 [debug] [Thread-10 ]: Finished running node model.dbsql_dbt_tpch.HoldingIncremental
[0m15:26:05.972339 [debug] [Thread-5  ]: Runtime Error in model CustomerMgmtView (models/base/CustomerMgmtView.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`roberto_salcido_tpcdi_stage`.`customermgmt10` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 10 pos 4
[0m15:26:05.973136 [debug] [Thread-12 ]: Runtime Error in model TradeIncremental (models/base/TradeIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`TradeIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:26:05.973532 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '99f9bed4-aa40-4fcf-85c0-76360e0e1894', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x169199bb0>]}
[0m15:26:05.973914 [debug] [Thread-12 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '99f9bed4-aa40-4fcf-85c0-76360e0e1894', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x169199ac0>]}
[0m15:26:05.974379 [error] [Thread-5  ]: 5 of 45 ERROR creating sql view model tpcdi.CustomerMgmtView ................... [[31mERROR[0m in 0.94s]
[0m15:26:05.974792 [error] [Thread-12 ]: 12 of 45 ERROR creating sql view model tpcdi.TradeIncremental .................. [[31mERROR[0m in 0.94s]
[0m15:26:05.975215 [debug] [Thread-5  ]: Finished running node model.dbsql_dbt_tpch.CustomerMgmtView
[0m15:26:05.975575 [debug] [Thread-12 ]: Finished running node model.dbsql_dbt_tpch.TradeIncremental
[0m15:26:05.976093 [debug] [Thread-17 ]: Began running node model.dbsql_dbt_tpch.DimCustomerStg
[0m15:26:05.976517 [info ] [Thread-17 ]: 15 of 45 SKIP relation tpcdi.DimCustomerStg .................................... [[33mSKIP[0m]
[0m15:26:05.976868 [debug] [Thread-17 ]: Finished running node model.dbsql_dbt_tpch.DimCustomerStg
[0m15:26:05.977226 [debug] [Thread-19 ]: Began running node test.dbsql_dbt_tpch.dateval_DimCustomerStg_effectivedate.8a3a23ae4e
[0m15:26:05.977513 [info ] [Thread-19 ]: 16 of 45 SKIP test dateval_DimCustomerStg_effectivedate ........................ [[33mSKIP[0m]
[0m15:26:05.977801 [debug] [Thread-19 ]: Finished running node test.dbsql_dbt_tpch.dateval_DimCustomerStg_effectivedate.8a3a23ae4e
[0m15:26:05.978130 [debug] [Thread-21 ]: Began running node model.dbsql_dbt_tpch.Prospect
[0m15:26:05.978378 [info ] [Thread-21 ]: 17 of 45 SKIP relation tpcdi.Prospect .......................................... [[33mSKIP[0m]
[0m15:26:05.978670 [debug] [Thread-21 ]: Finished running node model.dbsql_dbt_tpch.Prospect
[0m15:26:05.978985 [debug] [Thread-23 ]: Began running node model.dbsql_dbt_tpch.DimCustomer
[0m15:26:05.979220 [info ] [Thread-23 ]: 18 of 45 SKIP relation tpcdi.DimCustomer ....................................... [[33mSKIP[0m]
[0m15:26:05.979490 [debug] [Thread-23 ]: Finished running node model.dbsql_dbt_tpch.DimCustomer
[0m15:26:05.979802 [debug] [Thread-25 ]: Began running node test.dbsql_dbt_tpch.accepted_values_DimCustomer_tier__1__2__3.7be85b3b7a
[0m15:26:05.980240 [debug] [Thread-14 ]: Began running node test.dbsql_dbt_tpch.not_null_DimCustomer_tier.e0964b5cac
[0m15:26:05.980039 [info ] [Thread-25 ]: 19 of 45 SKIP test accepted_values_DimCustomer_tier__1__2__3 ................... [[33mSKIP[0m]
[0m15:26:05.981376 [debug] [Thread-25 ]: Finished running node test.dbsql_dbt_tpch.accepted_values_DimCustomer_tier__1__2__3.7be85b3b7a
[0m15:26:05.980466 [info ] [Thread-14 ]: 20 of 45 SKIP test not_null_DimCustomer_tier ................................... [[33mSKIP[0m]
[0m15:26:05.981802 [debug] [Thread-14 ]: Finished running node test.dbsql_dbt_tpch.not_null_DimCustomer_tier.e0964b5cac
[0m15:26:05.982615 [debug] [Thread-13 ]: Runtime Error in model WatchIncremental (models/base/WatchIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`WatchIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:26:05.982916 [debug] [Thread-13 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '99f9bed4-aa40-4fcf-85c0-76360e0e1894', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1690a9070>]}
[0m15:26:05.983251 [error] [Thread-13 ]: 13 of 45 ERROR creating sql view model tpcdi.WatchIncremental .................. [[31mERROR[0m in 0.95s]
[0m15:26:05.983542 [debug] [Thread-13 ]: Finished running node model.dbsql_dbt_tpch.WatchIncremental
[0m15:26:06.000491 [debug] [Thread-8  ]: Runtime Error in model DimBroker (models/silver/DimBroker.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HR` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 33 pos 6
[0m15:26:06.000911 [debug] [Thread-8  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '99f9bed4-aa40-4fcf-85c0-76360e0e1894', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x168e84dc0>]}
[0m15:26:06.001273 [error] [Thread-8  ]: 8 of 45 ERROR creating sql table model tpcdi.DimBroker ......................... [[31mERROR[0m in 0.97s]
[0m15:26:06.001596 [debug] [Thread-8  ]: Finished running node model.dbsql_dbt_tpch.DimBroker
[0m15:26:06.002124 [debug] [Thread-4  ]: Began running node model.dbsql_dbt_tpch.DimAccount
[0m15:26:06.002491 [info ] [Thread-4  ]: 21 of 45 SKIP relation tpcdi.DimAccount ........................................ [[33mSKIP[0m]
[0m15:26:06.002908 [debug] [Thread-4  ]: Finished running node model.dbsql_dbt_tpch.DimAccount
[0m15:26:06.003354 [debug] [Thread-6  ]: Began running node test.dbsql_dbt_tpch.not_null_DimAccount_sk_brokerid.dd6e992726
[0m15:26:06.003649 [info ] [Thread-6  ]: 22 of 45 SKIP test not_null_DimAccount_sk_brokerid ............................. [[33mSKIP[0m]
[0m15:26:06.003957 [debug] [Thread-7  ]: Began running node test.dbsql_dbt_tpch.not_null_DimAccount_sk_customerid.b8baa6ce87
[0m15:26:06.004197 [debug] [Thread-6  ]: Finished running node test.dbsql_dbt_tpch.not_null_DimAccount_sk_brokerid.dd6e992726
[0m15:26:06.004517 [info ] [Thread-7  ]: 23 of 45 SKIP test not_null_DimAccount_sk_customerid ........................... [[33mSKIP[0m]
[0m15:26:06.005158 [debug] [Thread-7  ]: Finished running node test.dbsql_dbt_tpch.not_null_DimAccount_sk_customerid.b8baa6ce87
[0m15:26:06.005659 [debug] [Thread-15 ]: Began running node model.dbsql_dbt_tpch.FactCashBalances
[0m15:26:06.005924 [info ] [Thread-15 ]: 24 of 45 SKIP relation tpcdi.FactCashBalances .................................. [[33mSKIP[0m]
[0m15:26:06.006198 [debug] [Thread-15 ]: Finished running node model.dbsql_dbt_tpch.FactCashBalances
[0m15:26:06.006514 [debug] [Thread-11 ]: Began running node test.dbsql_dbt_tpch.not_null_FactCashBalances_sk_accountid.878cf23033
[0m15:26:06.006763 [info ] [Thread-11 ]: 25 of 45 SKIP test not_null_FactCashBalances_sk_accountid ...................... [[33mSKIP[0m]
[0m15:26:06.007065 [debug] [Thread-11 ]: Finished running node test.dbsql_dbt_tpch.not_null_FactCashBalances_sk_accountid.878cf23033
[0m15:26:06.050649 [debug] [Thread-9  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.FinWire"} */
create or replace view `main`.`tpcdi`.`FinWire`
  
  
  as
    

select *, substring(value, 16, 3) rectype from 
text.`dbfs:/tmp/tpcdi/sf=10/Batch1/FINWIRE*`

[0m15:26:06.050995 [debug] [Thread-9  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: text; line 9 pos 0
[0m15:26:06.051583 [debug] [Thread-9  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: text; line 9 pos 0
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: text; line 9 pos 0
	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:76)
	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:171)
	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:93)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1306)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1305)
	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:81)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1335)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1332)
	at org.apache.spark.sql.catalyst.plans.logical.CreateView.mapChildren(v2Commands.scala:1350)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:79)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:78)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:93)
	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:51)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:229)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:229)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:226)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:218)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:296)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:296)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:296)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:383)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:376)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:283)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:376)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:304)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:379)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:355)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:156)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:348)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:380)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:817)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:380)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1040)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:377)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:150)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:140)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getRuntimeCategory$1(QueryRuntimePrediction.scala:300)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1040)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:299)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:349)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:345)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more
Caused by: org.apache.spark.sql.AnalysisException: [PATH_NOT_FOUND] Path does not exist: dbfs:/tmp/tpcdi/sf=10/Batch1/FINWIRE*.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:1592)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:844)
	at scala.collection.immutable.List.flatMap(List.scala:366)
	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:831)
	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:616)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:451)
	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:160)
	... 95 more

[0m15:26:06.052149 [debug] [Thread-9  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xdf\x84f\x1aZ\xa1S\xc4\x13C\xcac\xbc'
[0m15:26:06.052434 [debug] [Thread-9  ]: Timing info for model.dbsql_dbt_tpch.FinWire (execute): 15:26:05.127915 => 15:26:06.052328
[0m15:26:06.052618 [debug] [Thread-9  ]: On model.dbsql_dbt_tpch.FinWire: ROLLBACK
[0m15:26:06.052780 [debug] [Thread-9  ]: Databricks adapter: NotImplemented: rollback
[0m15:26:06.052928 [debug] [Thread-9  ]: On model.dbsql_dbt_tpch.FinWire: Close
[0m15:26:06.110465 [debug] [Thread-9  ]: Runtime Error in model FinWire (models/base/FinWire.sql)
  [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: text; line 9 pos 0
[0m15:26:06.110849 [debug] [Thread-9  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '99f9bed4-aa40-4fcf-85c0-76360e0e1894', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1690bb280>]}
[0m15:26:06.111190 [error] [Thread-9  ]: 9 of 45 ERROR creating sql view model tpcdi.FinWire ............................ [[31mERROR[0m in 1.08s]
[0m15:26:06.111491 [debug] [Thread-9  ]: Finished running node model.dbsql_dbt_tpch.FinWire
[0m15:26:06.111994 [debug] [Thread-5  ]: Began running node model.dbsql_dbt_tpch.DimCompany
[0m15:26:06.112245 [info ] [Thread-5  ]: 26 of 45 SKIP relation tpcdi.DimCompany ........................................ [[33mSKIP[0m]
[0m15:26:06.112637 [debug] [Thread-5  ]: Finished running node model.dbsql_dbt_tpch.DimCompany
[0m15:26:06.113231 [debug] [Thread-18 ]: Began running node test.dbsql_dbt_tpch.dateval_DimCompany_effectivedate.4df1621a42
[0m15:26:06.113527 [info ] [Thread-18 ]: 27 of 45 SKIP test dateval_DimCompany_effectivedate ............................ [[33mSKIP[0m]
[0m15:26:06.113824 [debug] [Thread-18 ]: Finished running node test.dbsql_dbt_tpch.dateval_DimCompany_effectivedate.4df1621a42
[0m15:26:06.114232 [debug] [Thread-20 ]: Began running node model.dbsql_dbt_tpch.DimSecurity
[0m15:26:06.114565 [debug] [Thread-19 ]: Began running node model.dbsql_dbt_tpch.Financial
[0m15:26:06.114789 [info ] [Thread-20 ]: 28 of 45 SKIP relation tpcdi.DimSecurity ....................................... [[33mSKIP[0m]
[0m15:26:06.115110 [info ] [Thread-19 ]: 29 of 45 SKIP relation tpcdi.Financial ......................................... [[33mSKIP[0m]
[0m15:26:06.115411 [debug] [Thread-20 ]: Finished running node model.dbsql_dbt_tpch.DimSecurity
[0m15:26:06.115617 [debug] [Thread-19 ]: Finished running node model.dbsql_dbt_tpch.Financial
[0m15:26:06.115970 [debug] [Thread-19 ]: Began running node test.dbsql_dbt_tpch.not_null_DimSecurity_sk_companyid.e728df82fe
[0m15:26:06.116204 [info ] [Thread-19 ]: 30 of 45 SKIP test not_null_DimSecurity_sk_companyid ........................... [[33mSKIP[0m]
[0m15:26:06.116493 [debug] [Thread-19 ]: Finished running node test.dbsql_dbt_tpch.not_null_DimSecurity_sk_companyid.e728df82fe
[0m15:26:06.117026 [debug] [Thread-24 ]: Began running node test.dbsql_dbt_tpch.not_null_Financial_sk_companyid.529c7c2c12
[0m15:26:06.117483 [debug] [Thread-23 ]: Began running node model.dbsql_dbt_tpch.DimTrade
[0m15:26:06.117778 [debug] [Thread-2  ]: Began running node model.dbsql_dbt_tpch.FactWatches
[0m15:26:06.117282 [info ] [Thread-24 ]: 31 of 45 SKIP test not_null_Financial_sk_companyid ............................. [[33mSKIP[0m]
[0m15:26:06.118179 [info ] [Thread-23 ]: 32 of 45 SKIP relation tpcdi.DimTrade .......................................... [[33mSKIP[0m]
[0m15:26:06.118552 [info ] [Thread-2  ]: 33 of 45 SKIP relation tpcdi.FactWatches ....................................... [[33mSKIP[0m]
[0m15:26:06.118978 [debug] [Thread-24 ]: Finished running node test.dbsql_dbt_tpch.not_null_Financial_sk_companyid.529c7c2c12
[0m15:26:06.119214 [debug] [Thread-23 ]: Finished running node model.dbsql_dbt_tpch.DimTrade
[0m15:26:06.119446 [debug] [Thread-2  ]: Finished running node model.dbsql_dbt_tpch.FactWatches
[0m15:26:06.119931 [debug] [Thread-2  ]: Began running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
[0m15:26:06.120188 [info ] [Thread-2  ]: 34 of 45 SKIP relation tpcdi.tempSumpFiBasicEps ................................ [[33mSKIP[0m]
[0m15:26:06.120516 [debug] [Thread-14 ]: Began running node test.dbsql_dbt_tpch.not_null_DimTrade_sk_accountid.4db26dca2c
[0m15:26:06.120717 [debug] [Thread-2  ]: Finished running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
[0m15:26:06.120891 [debug] [Thread-13 ]: Began running node test.dbsql_dbt_tpch.not_null_DimTrade_sk_securityid.13bd8f9da6
[0m15:26:06.121110 [debug] [Thread-8  ]: Began running node test.dbsql_dbt_tpch.tradecom_DimTrade_commission.ad38ac2d42
[0m15:26:06.121532 [debug] [Thread-1  ]: Began running node test.dbsql_dbt_tpch.tradefee_DimTrade_fee.5ee05431ee
[0m15:26:06.121326 [info ] [Thread-14 ]: 35 of 45 SKIP test not_null_DimTrade_sk_accountid .............................. [[33mSKIP[0m]
[0m15:26:06.121816 [debug] [Thread-4  ]: Began running node test.dbsql_dbt_tpch.not_null_FactWatches_sk_customerid.1c7356b8c9
[0m15:26:06.122014 [debug] [Thread-2  ]: Began running node test.dbsql_dbt_tpch.not_null_FactWatches_sk_securityid.8a6b5e97b3
[0m15:26:06.122286 [info ] [Thread-13 ]: 36 of 45 SKIP test not_null_DimTrade_sk_securityid ............................. [[33mSKIP[0m]
[0m15:26:06.122549 [debug] [Thread-6  ]: Began running node model.dbsql_dbt_tpch.FactMarketHistory
[0m15:26:06.122747 [info ] [Thread-8  ]: 37 of 45 SKIP test tradecom_DimTrade_commission ................................ [[33mSKIP[0m]
[0m15:26:06.123035 [info ] [Thread-1  ]: 38 of 45 SKIP test tradefee_DimTrade_fee ....................................... [[33mSKIP[0m]
[0m15:26:06.123326 [debug] [Thread-14 ]: Finished running node test.dbsql_dbt_tpch.not_null_DimTrade_sk_accountid.4db26dca2c
[0m15:26:06.123540 [info ] [Thread-4  ]: 39 of 45 SKIP test not_null_FactWatches_sk_customerid .......................... [[33mSKIP[0m]
[0m15:26:06.123810 [info ] [Thread-2  ]: 40 of 45 SKIP test not_null_FactWatches_sk_securityid .......................... [[33mSKIP[0m]
[0m15:26:06.124080 [debug] [Thread-13 ]: Finished running node test.dbsql_dbt_tpch.not_null_DimTrade_sk_securityid.13bd8f9da6
[0m15:26:06.124286 [info ] [Thread-6  ]: 41 of 45 SKIP relation tpcdi.FactMarketHistory ................................. [[33mSKIP[0m]
[0m15:26:06.124537 [debug] [Thread-8  ]: Finished running node test.dbsql_dbt_tpch.tradecom_DimTrade_commission.ad38ac2d42
[0m15:26:06.124733 [debug] [Thread-1  ]: Finished running node test.dbsql_dbt_tpch.tradefee_DimTrade_fee.5ee05431ee
[0m15:26:06.125020 [debug] [Thread-4  ]: Finished running node test.dbsql_dbt_tpch.not_null_FactWatches_sk_customerid.1c7356b8c9
[0m15:26:06.125236 [debug] [Thread-2  ]: Finished running node test.dbsql_dbt_tpch.not_null_FactWatches_sk_securityid.8a6b5e97b3
[0m15:26:06.125490 [debug] [Thread-6  ]: Finished running node model.dbsql_dbt_tpch.FactMarketHistory
[0m15:26:06.126105 [debug] [Thread-3  ]: Began running node model.dbsql_dbt_tpch.FactHoldings
[0m15:26:06.126405 [debug] [Thread-15 ]: Began running node test.dbsql_dbt_tpch.not_null_FactMarketHistory_PERatio.83761a27d0
[0m15:26:06.126664 [info ] [Thread-3  ]: 42 of 45 SKIP relation tpcdi.FactHoldings ...................................... [[33mSKIP[0m]
[0m15:26:06.126970 [debug] [Thread-10 ]: Began running node test.dbsql_dbt_tpch.not_null_FactMarketHistory_sk_securityid.16a0e9b8b2
[0m15:26:06.127181 [info ] [Thread-15 ]: 43 of 45 SKIP test not_null_FactMarketHistory_PERatio .......................... [[33mSKIP[0m]
[0m15:26:06.127519 [debug] [Thread-3  ]: Finished running node model.dbsql_dbt_tpch.FactHoldings
[0m15:26:06.127801 [info ] [Thread-10 ]: 44 of 45 SKIP test not_null_FactMarketHistory_sk_securityid .................... [[33mSKIP[0m]
[0m15:26:06.128113 [debug] [Thread-15 ]: Finished running node test.dbsql_dbt_tpch.not_null_FactMarketHistory_PERatio.83761a27d0
[0m15:26:06.128534 [debug] [Thread-10 ]: Finished running node test.dbsql_dbt_tpch.not_null_FactMarketHistory_sk_securityid.16a0e9b8b2
[0m15:26:06.128732 [debug] [Thread-9  ]: Began running node test.dbsql_dbt_tpch.not_null_FactHoldings_currentprice.98a44eac00
[0m15:26:06.129052 [info ] [Thread-9  ]: 45 of 45 SKIP test not_null_FactHoldings_currentprice .......................... [[33mSKIP[0m]
[0m15:26:06.129308 [debug] [Thread-9  ]: Finished running node test.dbsql_dbt_tpch.not_null_FactHoldings_currentprice.98a44eac00
[0m15:26:06.130629 [debug] [MainThread]: On master: ROLLBACK
[0m15:26:06.130806 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:26:06.319845 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:26:06.320409 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:26:06.320667 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:26:06.320929 [debug] [MainThread]: On master: ROLLBACK
[0m15:26:06.321165 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:26:06.321391 [debug] [MainThread]: On master: Close
[0m15:26:06.381230 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:26:06.381509 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.AccountIncremental' was properly closed.
[0m15:26:06.381670 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.BatchDate' was properly closed.
[0m15:26:06.381817 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.CashTransactionIncremental' was properly closed.
[0m15:26:06.381958 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.CustomerIncremental' was properly closed.
[0m15:26:06.382091 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.CustomerMgmtView' was properly closed.
[0m15:26:06.382219 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.DailyMarketHistorical' was properly closed.
[0m15:26:06.382346 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.DailyMarketIncremental' was properly closed.
[0m15:26:06.382471 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.DimBroker' was properly closed.
[0m15:26:06.382594 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.FinWire' was properly closed.
[0m15:26:06.382719 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.HoldingIncremental' was properly closed.
[0m15:26:06.382842 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.ProspectRaw' was properly closed.
[0m15:26:06.382967 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.TradeIncremental' was properly closed.
[0m15:26:06.383089 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.WatchIncremental' was properly closed.
[0m15:26:06.385017 [info ] [MainThread]: 
[0m15:26:06.385241 [info ] [MainThread]: Finished running 12 view models, 15 table models, 18 tests in 0 hours 0 minutes and 3.04 seconds (3.04s).
[0m15:26:06.386559 [debug] [MainThread]: Command end result
[0m15:26:06.429619 [info ] [MainThread]: 
[0m15:26:06.430234 [info ] [MainThread]: [31mCompleted with 13 errors and 0 warnings:[0m
[0m15:26:06.430457 [info ] [MainThread]: 
[0m15:26:06.430653 [error] [MainThread]: [33mRuntime Error in model BatchDate (models/base/BatchDate.sql)[0m
[0m15:26:06.430844 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`BatchDateuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:26:06.431032 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:26:06.431208 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:26:06.431386 [info ] [MainThread]: 
[0m15:26:06.431566 [error] [MainThread]: [33mRuntime Error in model CustomerIncremental (models/base/CustomerIncremental.sql)[0m
[0m15:26:06.431740 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`customerincrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:26:06.431918 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:26:06.432091 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:26:06.432265 [info ] [MainThread]: 
[0m15:26:06.432439 [error] [MainThread]: [33mRuntime Error in model AccountIncremental (models/base/AccountIncremental.sql)[0m
[0m15:26:06.432615 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`AccountIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:26:06.432789 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:26:06.432958 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:26:06.433129 [info ] [MainThread]: 
[0m15:26:06.433302 [error] [MainThread]: [33mRuntime Error in model DailyMarketHistorical (models/base/DailyMarketHistorical.sql)[0m
[0m15:26:06.433480 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketHistorical` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:26:06.433659 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:26:06.433833 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:26:06.434005 [info ] [MainThread]: 
[0m15:26:06.434180 [error] [MainThread]: [33mRuntime Error in model DailyMarketIncremental (models/base/DailyMarketIncremental.sql)[0m
[0m15:26:06.434606 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:26:06.434955 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:26:06.435168 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:26:06.435475 [info ] [MainThread]: 
[0m15:26:06.435699 [error] [MainThread]: [33mRuntime Error in model CashTransactionIncremental (models/base/CashTransactionIncremental.sql)[0m
[0m15:26:06.435882 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`CashTransactionIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:26:06.436055 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:26:06.436231 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:26:06.436400 [info ] [MainThread]: 
[0m15:26:06.436572 [error] [MainThread]: [33mRuntime Error in model ProspectRaw (models/base/ProspectRaw.sql)[0m
[0m15:26:06.436742 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`ProspectRawuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:26:06.436916 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:26:06.437087 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:26:06.437253 [info ] [MainThread]: 
[0m15:26:06.437423 [error] [MainThread]: [33mRuntime Error in model HoldingIncremental (models/base/HoldingIncremental.sql)[0m
[0m15:26:06.437593 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HoldingIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:26:06.437875 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:26:06.438086 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:26:06.438274 [info ] [MainThread]: 
[0m15:26:06.438458 [error] [MainThread]: [33mRuntime Error in model CustomerMgmtView (models/base/CustomerMgmtView.sql)[0m
[0m15:26:06.438713 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`roberto_salcido_tpcdi_stage`.`customermgmt10` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:26:06.438943 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:26:06.439139 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 10 pos 4
[0m15:26:06.439324 [info ] [MainThread]: 
[0m15:26:06.439506 [error] [MainThread]: [33mRuntime Error in model TradeIncremental (models/base/TradeIncremental.sql)[0m
[0m15:26:06.439681 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`TradeIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:26:06.439853 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:26:06.440022 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:26:06.440191 [info ] [MainThread]: 
[0m15:26:06.440360 [error] [MainThread]: [33mRuntime Error in model WatchIncremental (models/base/WatchIncremental.sql)[0m
[0m15:26:06.440529 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`WatchIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:26:06.440699 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:26:06.440867 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:26:06.441034 [info ] [MainThread]: 
[0m15:26:06.441199 [error] [MainThread]: [33mRuntime Error in model DimBroker (models/silver/DimBroker.sql)[0m
[0m15:26:06.441361 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HR` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:26:06.441524 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:26:06.441689 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 33 pos 6
[0m15:26:06.441854 [info ] [MainThread]: 
[0m15:26:06.442021 [error] [MainThread]: [33mRuntime Error in model FinWire (models/base/FinWire.sql)[0m
[0m15:26:06.442185 [error] [MainThread]:   [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: text; line 9 pos 0
[0m15:26:06.442382 [info ] [MainThread]: 
[0m15:26:06.442574 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=13 SKIP=32 TOTAL=45
[0m15:26:06.442955 [debug] [MainThread]: Command `dbt build` failed at 15:26:06.442898 after 11.34 seconds
[0m15:26:06.443150 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108470670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x168dc0790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x168dc6940>]}
[0m15:26:06.443344 [debug] [MainThread]: Flushing usage events
[0m15:30:02.158311 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110c395b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111e90e80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111ed9a00>]}


============================== 15:30:02.162285 | 45e836c6-6b38-41c0-a54e-547104f046cc ==============================
[0m15:30:02.162285 [info ] [MainThread]: Running with dbt=1.5.1
[0m15:30:02.162632 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:30:02.961862 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '45e836c6-6b38-41c0-a54e-547104f046cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105389f10>]}
[0m15:30:02.970970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '45e836c6-6b38-41c0-a54e-547104f046cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x147165d90>]}
[0m15:30:03.000696 [debug] [MainThread]: checksum: 51f6b581eba8f8101bc020bf9faf8f96af641e2da86d581f66e2bdf0ff384b1c, vars: {}, profile: , target: , version: 1.5.1
[0m15:30:03.076020 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:30:03.076322 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:30:03.083630 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '45e836c6-6b38-41c0-a54e-547104f046cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14764a970>]}
[0m15:30:03.097223 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '45e836c6-6b38-41c0-a54e-547104f046cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x147227340>]}
[0m15:30:03.097552 [info ] [MainThread]: Found 27 models, 18 tests, 0 snapshots, 0 analyses, 669 macros, 0 operations, 0 seed files, 33 sources, 0 exposures, 0 metrics, 0 groups
[0m15:30:03.099669 [info ] [MainThread]: 
[0m15:30:03.100116 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m15:30:03.101694 [debug] [ThreadPool]: Acquiring new databricks connection 'list_main'
[0m15:30:03.101854 [debug] [ThreadPool]: Using databricks connection "list_main"
[0m15:30:03.102157 [debug] [ThreadPool]: On list_main: GetSchemas(database=`main`, schema=None)
[0m15:30:03.102643 [debug] [ThreadPool]: Acquiring new databricks connection 'list_main'
[0m15:30:03.102909 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:30:03.103118 [debug] [ThreadPool]: Using databricks connection "list_main"
[0m15:30:03.103518 [debug] [ThreadPool]: On list_main: GetSchemas(database=`main`, schema=None)
[0m15:30:03.103671 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:30:03.742581 [debug] [ThreadPool]: SQL status: OK in 0.6399999856948853 seconds
[0m15:30:03.744873 [debug] [ThreadPool]: SQL status: OK in 0.6399999856948853 seconds
[0m15:30:03.755941 [debug] [ThreadPool]: On list_main: Close
[0m15:30:03.756967 [debug] [ThreadPool]: On list_main: Close
[0m15:30:03.839218 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_main, now create_main_tpcdi_dbt_test__audit)
[0m15:30:03.840078 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_main, now create_main_tpcdi)
[0m15:30:03.842373 [debug] [ThreadPool]: Creating schema "database: "main"
schema: "tpcdi_dbt_test__audit"
"
[0m15:30:03.843156 [debug] [ThreadPool]: Creating schema "database: "main"
schema: "tpcdi"
"
[0m15:30:03.855128 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:30:03.857101 [debug] [ThreadPool]: Using databricks connection "create_main_tpcdi_dbt_test__audit"
[0m15:30:03.858661 [debug] [ThreadPool]: On create_main_tpcdi_dbt_test__audit: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "create_main_tpcdi_dbt_test__audit"} */
create schema if not exists `main`.`tpcdi_dbt_test__audit`
  
[0m15:30:03.863763 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:30:03.864026 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:30:03.864263 [debug] [ThreadPool]: Using databricks connection "create_main_tpcdi"
[0m15:30:03.864657 [debug] [ThreadPool]: On create_main_tpcdi: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "create_main_tpcdi"} */
create schema if not exists `main`.`tpcdi`
  
[0m15:30:03.864882 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:30:04.303738 [debug] [ThreadPool]: SQL status: OK in 0.4399999976158142 seconds
[0m15:30:04.304436 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m15:30:04.304617 [debug] [ThreadPool]: On create_main_tpcdi: ROLLBACK
[0m15:30:04.304768 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m15:30:04.304901 [debug] [ThreadPool]: On create_main_tpcdi: Close
[0m15:30:04.326497 [debug] [ThreadPool]: SQL status: OK in 0.46000000834465027 seconds
[0m15:30:04.327240 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m15:30:04.327439 [debug] [ThreadPool]: On create_main_tpcdi_dbt_test__audit: ROLLBACK
[0m15:30:04.327596 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m15:30:04.327739 [debug] [ThreadPool]: On create_main_tpcdi_dbt_test__audit: Close
[0m15:30:04.392863 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_main_tpcdi_dbt_test__audit, now list_main_tpcdi_dbt_test__audit)
[0m15:30:04.393708 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_main_tpcdi, now list_main_tpcdi)
[0m15:30:04.400717 [debug] [ThreadPool]: Using databricks connection "list_main_tpcdi"
[0m15:30:04.407533 [debug] [ThreadPool]: On list_main_tpcdi: GetTables(database=main, schema=tpcdi, identifier=None)
[0m15:30:04.407185 [debug] [ThreadPool]: Using databricks connection "list_main_tpcdi_dbt_test__audit"
[0m15:30:04.407892 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:30:04.408168 [debug] [ThreadPool]: On list_main_tpcdi_dbt_test__audit: GetTables(database=main, schema=tpcdi_dbt_test__audit, identifier=None)
[0m15:30:04.408576 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:30:04.830780 [debug] [ThreadPool]: SQL status: OK in 0.41999998688697815 seconds
[0m15:30:04.831690 [debug] [ThreadPool]: SQL status: OK in 0.41999998688697815 seconds
[0m15:30:04.836257 [debug] [ThreadPool]: On list_main_tpcdi: Close
[0m15:30:04.837967 [debug] [ThreadPool]: On list_main_tpcdi_dbt_test__audit: Close
[0m15:30:04.908927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '45e836c6-6b38-41c0-a54e-547104f046cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x147622eb0>]}
[0m15:30:04.910508 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:30:04.911009 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:30:04.912195 [info ] [MainThread]: Concurrency: 25 threads (target='dev')
[0m15:30:04.912938 [info ] [MainThread]: 
[0m15:30:04.922626 [debug] [Thread-1  ]: Began running node model.dbsql_dbt_tpch.AccountIncremental
[0m15:30:04.923270 [debug] [Thread-2  ]: Began running node model.dbsql_dbt_tpch.BatchDate
[0m15:30:04.923764 [debug] [Thread-3  ]: Began running node model.dbsql_dbt_tpch.CashTransactionIncremental
[0m15:30:04.924763 [debug] [Thread-4  ]: Began running node model.dbsql_dbt_tpch.CustomerIncremental
[0m15:30:04.924316 [info ] [Thread-1  ]: 1 of 45 START sql view model tpcdi.AccountIncremental .......................... [RUN]
[0m15:30:04.925295 [debug] [Thread-5  ]: Began running node model.dbsql_dbt_tpch.CustomerMgmtView
[0m15:30:04.925745 [debug] [Thread-6  ]: Began running node model.dbsql_dbt_tpch.DailyMarketHistorical
[0m15:30:04.926103 [debug] [Thread-7  ]: Began running node model.dbsql_dbt_tpch.DailyMarketIncremental
[0m15:30:04.926516 [info ] [Thread-2  ]: 2 of 45 START sql view model tpcdi.BatchDate ................................... [RUN]
[0m15:30:04.927059 [debug] [Thread-8  ]: Began running node model.dbsql_dbt_tpch.DimBroker
[0m15:30:04.927538 [debug] [Thread-9  ]: Began running node model.dbsql_dbt_tpch.FinWire
[0m15:30:04.927933 [debug] [Thread-10 ]: Began running node model.dbsql_dbt_tpch.HoldingIncremental
[0m15:30:04.928377 [info ] [Thread-3  ]: 3 of 45 START sql view model tpcdi.CashTransactionIncremental .................. [RUN]
[0m15:30:04.928886 [debug] [Thread-11 ]: Began running node model.dbsql_dbt_tpch.ProspectRaw
[0m15:30:04.929264 [debug] [Thread-12 ]: Began running node model.dbsql_dbt_tpch.TradeIncremental
[0m15:30:04.929666 [info ] [Thread-4  ]: 4 of 45 START sql view model tpcdi.CustomerIncremental ......................... [RUN]
[0m15:30:04.930128 [debug] [Thread-13 ]: Began running node model.dbsql_dbt_tpch.WatchIncremental
[0m15:30:04.930830 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_main_tpcdi_dbt_test__audit, now model.dbsql_dbt_tpch.AccountIncremental)
[0m15:30:04.931291 [info ] [Thread-5  ]: 5 of 45 START sql view model tpcdi.CustomerMgmtView ............................ [RUN]
[0m15:30:04.931775 [info ] [Thread-6  ]: 6 of 45 START sql view model tpcdi.DailyMarketHistorical ....................... [RUN]
[0m15:30:04.932236 [info ] [Thread-7  ]: 7 of 45 START sql view model tpcdi.DailyMarketIncremental ...................... [RUN]
[0m15:30:04.932918 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_main_tpcdi, now model.dbsql_dbt_tpch.BatchDate)
[0m15:30:04.933329 [info ] [Thread-8  ]: 8 of 45 START sql table model tpcdi.DimBroker .................................. [RUN]
[0m15:30:04.933884 [info ] [Thread-9  ]: 9 of 45 START sql view model tpcdi.FinWire ..................................... [RUN]
[0m15:30:04.934588 [info ] [Thread-10 ]: 10 of 45 START sql view model tpcdi.HoldingIncremental ......................... [RUN]
[0m15:30:04.935468 [debug] [Thread-3  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.CashTransactionIncremental'
[0m15:30:04.935924 [info ] [Thread-11 ]: 11 of 45 START sql view model tpcdi.ProspectRaw ................................ [RUN]
[0m15:30:04.936425 [info ] [Thread-12 ]: 12 of 45 START sql view model tpcdi.TradeIncremental ........................... [RUN]
[0m15:30:04.937119 [debug] [Thread-4  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.CustomerIncremental'
[0m15:30:04.937482 [info ] [Thread-13 ]: 13 of 45 START sql view model tpcdi.WatchIncremental ........................... [RUN]
[0m15:30:04.937835 [debug] [Thread-1  ]: Began compiling node model.dbsql_dbt_tpch.AccountIncremental
[0m15:30:04.938363 [debug] [Thread-5  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.CustomerMgmtView'
[0m15:30:04.938881 [debug] [Thread-6  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.DailyMarketHistorical'
[0m15:30:04.939372 [debug] [Thread-7  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.DailyMarketIncremental'
[0m15:30:04.939633 [debug] [Thread-2  ]: Began compiling node model.dbsql_dbt_tpch.BatchDate
[0m15:30:04.940134 [debug] [Thread-8  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimBroker'
[0m15:30:04.940638 [debug] [Thread-9  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.FinWire'
[0m15:30:04.941128 [debug] [Thread-10 ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.HoldingIncremental'
[0m15:30:04.941393 [debug] [Thread-3  ]: Began compiling node model.dbsql_dbt_tpch.CashTransactionIncremental
[0m15:30:04.941884 [debug] [Thread-11 ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.ProspectRaw'
[0m15:30:04.942379 [debug] [Thread-12 ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.TradeIncremental'
[0m15:30:04.942636 [debug] [Thread-4  ]: Began compiling node model.dbsql_dbt_tpch.CustomerIncremental
[0m15:30:04.943105 [debug] [Thread-13 ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.WatchIncremental'
[0m15:30:04.946781 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.AccountIncremental"
[0m15:30:04.947063 [debug] [Thread-5  ]: Began compiling node model.dbsql_dbt_tpch.CustomerMgmtView
[0m15:30:04.947299 [debug] [Thread-6  ]: Began compiling node model.dbsql_dbt_tpch.DailyMarketHistorical
[0m15:30:04.947521 [debug] [Thread-7  ]: Began compiling node model.dbsql_dbt_tpch.DailyMarketIncremental
[0m15:30:04.950386 [debug] [Thread-8  ]: Began compiling node model.dbsql_dbt_tpch.DimBroker
[0m15:30:04.955805 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.BatchDate"
[0m15:30:04.956154 [debug] [Thread-9  ]: Began compiling node model.dbsql_dbt_tpch.FinWire
[0m15:30:04.956393 [debug] [Thread-10 ]: Began compiling node model.dbsql_dbt_tpch.HoldingIncremental
[0m15:30:04.959088 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.CashTransactionIncremental"
[0m15:30:04.959348 [debug] [Thread-11 ]: Began compiling node model.dbsql_dbt_tpch.ProspectRaw
[0m15:30:04.959571 [debug] [Thread-12 ]: Began compiling node model.dbsql_dbt_tpch.TradeIncremental
[0m15:30:04.962630 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.CustomerIncremental"
[0m15:30:04.962996 [debug] [Thread-13 ]: Began compiling node model.dbsql_dbt_tpch.WatchIncremental
[0m15:30:04.965796 [debug] [Thread-5  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.CustomerMgmtView"
[0m15:30:04.968384 [debug] [Thread-6  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.DailyMarketHistorical"
[0m15:30:04.970918 [debug] [Thread-7  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.DailyMarketIncremental"
[0m15:30:04.971275 [debug] [Thread-1  ]: Timing info for model.dbsql_dbt_tpch.AccountIncremental (compile): 15:30:04.943298 => 15:30:04.971101
[0m15:30:04.974425 [debug] [Thread-8  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.DimBroker"
[0m15:30:05.013624 [debug] [Thread-9  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.FinWire"
[0m15:30:05.015798 [debug] [Thread-10 ]: Writing injected SQL for node "model.dbsql_dbt_tpch.HoldingIncremental"
[0m15:30:05.018073 [debug] [Thread-11 ]: Writing injected SQL for node "model.dbsql_dbt_tpch.ProspectRaw"
[0m15:30:05.018361 [debug] [Thread-2  ]: Timing info for model.dbsql_dbt_tpch.BatchDate (compile): 15:30:04.947679 => 15:30:05.018235
[0m15:30:05.020459 [debug] [Thread-12 ]: Writing injected SQL for node "model.dbsql_dbt_tpch.TradeIncremental"
[0m15:30:05.022602 [debug] [Thread-13 ]: Writing injected SQL for node "model.dbsql_dbt_tpch.WatchIncremental"
[0m15:30:05.022880 [debug] [Thread-3  ]: Timing info for model.dbsql_dbt_tpch.CashTransactionIncremental (compile): 15:30:04.956544 => 15:30:05.022771
[0m15:30:05.023224 [debug] [Thread-4  ]: Timing info for model.dbsql_dbt_tpch.CustomerIncremental (compile): 15:30:04.959719 => 15:30:05.023105
[0m15:30:05.023426 [debug] [Thread-1  ]: Began executing node model.dbsql_dbt_tpch.AccountIncremental
[0m15:30:05.023701 [debug] [Thread-6  ]: Timing info for model.dbsql_dbt_tpch.DailyMarketHistorical (compile): 15:30:04.966090 => 15:30:05.023604
[0m15:30:05.023998 [debug] [Thread-5  ]: Timing info for model.dbsql_dbt_tpch.CustomerMgmtView (compile): 15:30:04.963266 => 15:30:05.023899
[0m15:30:05.024209 [debug] [Thread-7  ]: Timing info for model.dbsql_dbt_tpch.DailyMarketIncremental (compile): 15:30:04.968613 => 15:30:05.024120
[0m15:30:05.024467 [debug] [Thread-2  ]: Began executing node model.dbsql_dbt_tpch.BatchDate
[0m15:30:05.024736 [debug] [Thread-9  ]: Timing info for model.dbsql_dbt_tpch.FinWire (compile): 15:30:04.974667 => 15:30:05.024638
[0m15:30:05.024957 [debug] [Thread-8  ]: Timing info for model.dbsql_dbt_tpch.DimBroker (compile): 15:30:04.971422 => 15:30:05.024868
[0m15:30:05.025192 [debug] [Thread-11 ]: Timing info for model.dbsql_dbt_tpch.ProspectRaw (compile): 15:30:05.016046 => 15:30:05.025100
[0m15:30:05.025392 [debug] [Thread-10 ]: Timing info for model.dbsql_dbt_tpch.HoldingIncremental (compile): 15:30:05.013865 => 15:30:05.025293
[0m15:30:05.025618 [debug] [Thread-3  ]: Began executing node model.dbsql_dbt_tpch.CashTransactionIncremental
[0m15:30:05.025831 [debug] [Thread-12 ]: Timing info for model.dbsql_dbt_tpch.TradeIncremental (compile): 15:30:05.018489 => 15:30:05.025738
[0m15:30:05.026002 [debug] [Thread-4  ]: Began executing node model.dbsql_dbt_tpch.CustomerIncremental
[0m15:30:05.038709 [debug] [Thread-6  ]: Began executing node model.dbsql_dbt_tpch.DailyMarketHistorical
[0m15:30:05.038955 [debug] [Thread-13 ]: Timing info for model.dbsql_dbt_tpch.WatchIncremental (compile): 15:30:05.020684 => 15:30:05.038836
[0m15:30:05.041288 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.AccountIncremental"
[0m15:30:05.041489 [debug] [Thread-5  ]: Began executing node model.dbsql_dbt_tpch.CustomerMgmtView
[0m15:30:05.041675 [debug] [Thread-7  ]: Began executing node model.dbsql_dbt_tpch.DailyMarketIncremental
[0m15:30:05.043598 [debug] [Thread-2  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.BatchDate"
[0m15:30:05.043792 [debug] [Thread-9  ]: Began executing node model.dbsql_dbt_tpch.FinWire
[0m15:30:05.043973 [debug] [Thread-8  ]: Began executing node model.dbsql_dbt_tpch.DimBroker
[0m15:30:05.044140 [debug] [Thread-11 ]: Began executing node model.dbsql_dbt_tpch.ProspectRaw
[0m15:30:05.044308 [debug] [Thread-10 ]: Began executing node model.dbsql_dbt_tpch.HoldingIncremental
[0m15:30:05.046148 [debug] [Thread-3  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.CashTransactionIncremental"
[0m15:30:05.046331 [debug] [Thread-12 ]: Began executing node model.dbsql_dbt_tpch.TradeIncremental
[0m15:30:05.048212 [debug] [Thread-4  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.CustomerIncremental"
[0m15:30:05.050378 [debug] [Thread-6  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.DailyMarketHistorical"
[0m15:30:05.050569 [debug] [Thread-13 ]: Began executing node model.dbsql_dbt_tpch.WatchIncremental
[0m15:30:05.052509 [debug] [Thread-5  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.CustomerMgmtView"
[0m15:30:05.054365 [debug] [Thread-7  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.DailyMarketIncremental"
[0m15:30:05.056288 [debug] [Thread-9  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.FinWire"
[0m15:30:05.075589 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:30:05.078273 [debug] [Thread-8  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.DimBroker"
[0m15:30:05.080222 [debug] [Thread-11 ]: Writing runtime sql for node "model.dbsql_dbt_tpch.ProspectRaw"
[0m15:30:05.082094 [debug] [Thread-10 ]: Writing runtime sql for node "model.dbsql_dbt_tpch.HoldingIncremental"
[0m15:30:05.082288 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:30:05.084230 [debug] [Thread-12 ]: Writing runtime sql for node "model.dbsql_dbt_tpch.TradeIncremental"
[0m15:30:05.086236 [debug] [Thread-13 ]: Writing runtime sql for node "model.dbsql_dbt_tpch.WatchIncremental"
[0m15:30:05.086432 [debug] [Thread-3  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:30:05.086785 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:30:05.086974 [debug] [Thread-6  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:30:05.087159 [debug] [Thread-1  ]: Using databricks connection "model.dbsql_dbt_tpch.AccountIncremental"
[0m15:30:05.087372 [debug] [Thread-5  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:30:05.087617 [debug] [Thread-7  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:30:05.087859 [debug] [Thread-9  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:30:05.088074 [debug] [Thread-2  ]: Using databricks connection "model.dbsql_dbt_tpch.BatchDate"
[0m15:30:05.088310 [debug] [Thread-8  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:30:05.088597 [debug] [Thread-10 ]: Spark adapter: NotImplemented: add_begin_query
[0m15:30:05.088799 [debug] [Thread-11 ]: Spark adapter: NotImplemented: add_begin_query
[0m15:30:05.088954 [debug] [Thread-3  ]: Using databricks connection "model.dbsql_dbt_tpch.CashTransactionIncremental"
[0m15:30:05.089105 [debug] [Thread-4  ]: Using databricks connection "model.dbsql_dbt_tpch.CustomerIncremental"
[0m15:30:05.089288 [debug] [Thread-12 ]: Spark adapter: NotImplemented: add_begin_query
[0m15:30:05.089475 [debug] [Thread-13 ]: Spark adapter: NotImplemented: add_begin_query
[0m15:30:05.089605 [debug] [Thread-6  ]: Using databricks connection "model.dbsql_dbt_tpch.DailyMarketHistorical"
[0m15:30:05.089773 [debug] [Thread-1  ]: On model.dbsql_dbt_tpch.AccountIncremental: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.AccountIncremental"} */
create or replace view `main`.`tpcdi`.`AccountIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`AccountIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`AccountIncrementaltres`

[0m15:30:05.089919 [debug] [Thread-5  ]: Using databricks connection "model.dbsql_dbt_tpch.CustomerMgmtView"
[0m15:30:05.090042 [debug] [Thread-7  ]: Using databricks connection "model.dbsql_dbt_tpch.DailyMarketIncremental"
[0m15:30:05.090169 [debug] [Thread-9  ]: Using databricks connection "model.dbsql_dbt_tpch.FinWire"
[0m15:30:05.090317 [debug] [Thread-2  ]: On model.dbsql_dbt_tpch.BatchDate: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.BatchDate"} */
create or replace view `main`.`tpcdi`.`BatchDate`
  
  
  as
    

select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`BatchDateuno`

 UNION ALL

select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`BatchDatedos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`BatchDatetres`

[0m15:30:05.090453 [debug] [Thread-8  ]: Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
[0m15:30:05.090572 [debug] [Thread-10 ]: Using databricks connection "model.dbsql_dbt_tpch.HoldingIncremental"
[0m15:30:05.090690 [debug] [Thread-11 ]: Using databricks connection "model.dbsql_dbt_tpch.ProspectRaw"
[0m15:30:05.090832 [debug] [Thread-3  ]: On model.dbsql_dbt_tpch.CashTransactionIncremental: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CashTransactionIncremental"} */
create or replace view `main`.`tpcdi`.`CashTransactionIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`CashTransactionIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`CashTransactionIncrementaltres`

[0m15:30:05.090995 [debug] [Thread-4  ]: On model.dbsql_dbt_tpch.CustomerIncremental: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CustomerIncremental"} */
create or replace view `main`.`tpcdi`.`CustomerIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`customerincrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`customerincrementaltres`

[0m15:30:05.091133 [debug] [Thread-12 ]: Using databricks connection "model.dbsql_dbt_tpch.TradeIncremental"
[0m15:30:05.091252 [debug] [Thread-13 ]: Using databricks connection "model.dbsql_dbt_tpch.WatchIncremental"
[0m15:30:05.091392 [debug] [Thread-6  ]: On model.dbsql_dbt_tpch.DailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DailyMarketHistorical"} */
create or replace view `main`.`tpcdi`.`DailyMarketHistorical`
  
  
  as
    
select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`DailyMarketHistorical`

[0m15:30:05.091564 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:30:05.091707 [debug] [Thread-5  ]: On model.dbsql_dbt_tpch.CustomerMgmtView: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CustomerMgmtView"} */
create or replace view `main`.`tpcdi`.`CustomerMgmtView`
  
  
  as
    
select
    *
from
    hive_metastore.roberto_salcido_tpcdi_stage.customermgmt10

[0m15:30:05.091865 [debug] [Thread-7  ]: On model.dbsql_dbt_tpch.DailyMarketIncremental: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DailyMarketIncremental"} */
create or replace view `main`.`tpcdi`.`DailyMarketIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`DailyMarketIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`DailyMarketIncrementaltres`

[0m15:30:05.092020 [debug] [Thread-9  ]: On model.dbsql_dbt_tpch.FinWire: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.FinWire"} */
create or replace view `main`.`tpcdi`.`FinWire`
  
  
  as
    

select *, substring(value, 16, 3) rectype from 
text.`dbfs:/tmp/tpcdi/sf=10/Batch1/FINWIRE*`

[0m15:30:05.092181 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m15:30:05.092345 [debug] [Thread-8  ]: On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

  
    
        create or replace table `main`.`tpcdi`.`DimBroker`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT

  cast(employeeid as BIGINT) brokerid,
  cast(managerid as BIGINT) managerid,
  employeefirstname firstname,
  employeelastname lastname,
  employeemi middleinitial,
  employeebranch branch,
  employeeoffice office,
  employeephone phone,
  true iscurrent,
  1 batchid,
  (SELECT min(to_date(datevalue)) as effectivedate FROM `main`.`tpcdi`.`DimDate`) effectivedate,
  date('9999-12-31') enddate,
  bigint(concat(date_format(enddate, 'yyyyMMdd'), cast(brokerid as string))) as sk_brokerid
FROM  `main`.`tpcdi`.`HR`
WHERE employeejobcode = 314
  
[0m15:30:05.092517 [debug] [Thread-10 ]: On model.dbsql_dbt_tpch.HoldingIncremental: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.HoldingIncremental"} */
create or replace view `main`.`tpcdi`.`HoldingIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`HoldingIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`HoldingIncrementaltres`

[0m15:30:05.092676 [debug] [Thread-11 ]: On model.dbsql_dbt_tpch.ProspectRaw: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.ProspectRaw"} */
create or replace view `main`.`tpcdi`.`ProspectRaw`
  
  
  as
    

select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`ProspectRawuno`

 UNION ALL

select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`ProspectRawdos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`ProspectRawtres`

[0m15:30:05.092845 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m15:30:05.092999 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m15:30:05.093145 [debug] [Thread-12 ]: On model.dbsql_dbt_tpch.TradeIncremental: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.TradeIncremental"} */
create or replace view `main`.`tpcdi`.`TradeIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`TradeIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`TradeIncrementaltres`

[0m15:30:05.093302 [debug] [Thread-13 ]: On model.dbsql_dbt_tpch.WatchIncremental: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.WatchIncremental"} */
create or replace view `main`.`tpcdi`.`WatchIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`WatchIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`WatchIncrementaltres`

[0m15:30:05.093468 [debug] [Thread-6  ]: Opening a new connection, currently in state init
[0m15:30:05.093687 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m15:30:05.093852 [debug] [Thread-7  ]: Opening a new connection, currently in state init
[0m15:30:05.109481 [debug] [Thread-9  ]: Opening a new connection, currently in state init
[0m15:30:05.109868 [debug] [Thread-8  ]: Opening a new connection, currently in state init
[0m15:30:05.110038 [debug] [Thread-10 ]: Opening a new connection, currently in state init
[0m15:30:05.125838 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m15:30:05.126424 [debug] [Thread-12 ]: Opening a new connection, currently in state init
[0m15:30:05.126605 [debug] [Thread-13 ]: Opening a new connection, currently in state init
[0m15:30:05.609041 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.AccountIncremental"} */
create or replace view `main`.`tpcdi`.`AccountIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`AccountIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`AccountIncrementaltres`

[0m15:30:05.609465 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`AccountIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:30:05.609890 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`AccountIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`AccountIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:30:05.610279 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe0\x13V\x146\xa95!l~3\xdc~'
[0m15:30:05.610674 [debug] [Thread-1  ]: Timing info for model.dbsql_dbt_tpch.AccountIncremental (execute): 15:30:05.026136 => 15:30:05.610528
[0m15:30:05.610932 [debug] [Thread-1  ]: On model.dbsql_dbt_tpch.AccountIncremental: ROLLBACK
[0m15:30:05.611162 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m15:30:05.611371 [debug] [Thread-1  ]: On model.dbsql_dbt_tpch.AccountIncremental: Close
[0m15:30:05.718537 [debug] [Thread-6  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DailyMarketHistorical"} */
create or replace view `main`.`tpcdi`.`DailyMarketHistorical`
  
  
  as
    
select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`DailyMarketHistorical`

[0m15:30:05.719134 [debug] [Thread-6  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketHistorical` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:30:05.719706 [debug] [Thread-6  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketHistorical` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketHistorical` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:30:05.720214 [debug] [Thread-6  ]: Databricks adapter: operation-id: b"\x01\xee\x0e\xe0\x13c\x1ed\x89\xba'\x84\xf0K\xcc\xda"
[0m15:30:05.720721 [debug] [Thread-6  ]: Timing info for model.dbsql_dbt_tpch.DailyMarketHistorical (execute): 15:30:05.048346 => 15:30:05.720533
[0m15:30:05.721068 [debug] [Thread-6  ]: On model.dbsql_dbt_tpch.DailyMarketHistorical: ROLLBACK
[0m15:30:05.721434 [debug] [Thread-6  ]: Databricks adapter: NotImplemented: rollback
[0m15:30:05.721777 [debug] [Thread-6  ]: On model.dbsql_dbt_tpch.DailyMarketHistorical: Close
[0m15:30:05.725587 [debug] [Thread-1  ]: Runtime Error in model AccountIncremental (models/base/AccountIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`AccountIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:30:05.726119 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45e836c6-6b38-41c0-a54e-547104f046cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x147a65fa0>]}
[0m15:30:05.726753 [error] [Thread-1  ]: 1 of 45 ERROR creating sql view model tpcdi.AccountIncremental ................. [[31mERROR[0m in 0.80s]
[0m15:30:05.727225 [debug] [Thread-1  ]: Finished running node model.dbsql_dbt_tpch.AccountIncremental
[0m15:30:05.747691 [debug] [Thread-4  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CustomerIncremental"} */
create or replace view `main`.`tpcdi`.`CustomerIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`customerincrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`customerincrementaltres`

[0m15:30:05.748111 [debug] [Thread-4  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`customerincrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:30:05.749425 [debug] [Thread-2  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.BatchDate"} */
create or replace view `main`.`tpcdi`.`BatchDate`
  
  
  as
    

select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`BatchDateuno`

 UNION ALL

select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`BatchDatedos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`BatchDatetres`

[0m15:30:05.749967 [debug] [Thread-4  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`customerincrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`customerincrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:30:05.751166 [debug] [Thread-3  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CashTransactionIncremental"} */
create or replace view `main`.`tpcdi`.`CashTransactionIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`CashTransactionIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`CashTransactionIncrementaltres`

[0m15:30:05.751505 [debug] [Thread-2  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`BatchDateuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:30:05.751786 [debug] [Thread-4  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe0\x13d\x10$\xb8\xc1\x1fza\xbf\xbcx'
[0m15:30:05.752062 [debug] [Thread-3  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`CashTransactionIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:30:05.752455 [debug] [Thread-2  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`BatchDateuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`BatchDateuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:30:05.752977 [debug] [Thread-4  ]: Timing info for model.dbsql_dbt_tpch.CustomerIncremental (execute): 15:30:05.046452 => 15:30:05.752840
[0m15:30:05.753364 [debug] [Thread-3  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`CashTransactionIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`CashTransactionIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:30:05.753744 [debug] [Thread-2  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe0\x13c\x1d`\xb3\xe8\x08`\x96d\x03\xcf'
[0m15:30:05.753991 [debug] [Thread-4  ]: On model.dbsql_dbt_tpch.CustomerIncremental: ROLLBACK
[0m15:30:05.754227 [debug] [Thread-3  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe0\x13c\x1d5\x9a+&_"\x80;\xf7'
[0m15:30:05.754569 [debug] [Thread-2  ]: Timing info for model.dbsql_dbt_tpch.BatchDate (execute): 15:30:05.041802 => 15:30:05.754447
[0m15:30:05.754800 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m15:30:05.755138 [debug] [Thread-3  ]: Timing info for model.dbsql_dbt_tpch.CashTransactionIncremental (execute): 15:30:05.044422 => 15:30:05.755015
[0m15:30:05.755365 [debug] [Thread-2  ]: On model.dbsql_dbt_tpch.BatchDate: ROLLBACK
[0m15:30:05.755585 [debug] [Thread-4  ]: On model.dbsql_dbt_tpch.CustomerIncremental: Close
[0m15:30:05.755813 [debug] [Thread-3  ]: On model.dbsql_dbt_tpch.CashTransactionIncremental: ROLLBACK
[0m15:30:05.756034 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m15:30:05.756644 [debug] [Thread-3  ]: Databricks adapter: NotImplemented: rollback
[0m15:30:05.757068 [debug] [Thread-2  ]: On model.dbsql_dbt_tpch.BatchDate: Close
[0m15:30:05.757360 [debug] [Thread-3  ]: On model.dbsql_dbt_tpch.CashTransactionIncremental: Close
[0m15:30:05.780278 [debug] [Thread-5  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CustomerMgmtView"} */
create or replace view `main`.`tpcdi`.`CustomerMgmtView`
  
  
  as
    
select
    *
from
    hive_metastore.roberto_salcido_tpcdi_stage.customermgmt10

[0m15:30:05.781375 [debug] [Thread-5  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`roberto_salcido_tpcdi_stage`.`customermgmt10` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 10 pos 4
[0m15:30:05.781824 [debug] [Thread-5  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`roberto_salcido_tpcdi_stage`.`customermgmt10` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 10 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`roberto_salcido_tpcdi_stage`.`customermgmt10` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 10 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:30:05.782221 [debug] [Thread-5  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe0\x13c\x1f\xa0\x91t\xa7\xb6\x8d\xb8\x08\xc7'
[0m15:30:05.782616 [debug] [Thread-5  ]: Timing info for model.dbsql_dbt_tpch.CustomerMgmtView (execute): 15:30:05.050739 => 15:30:05.782472
[0m15:30:05.782867 [debug] [Thread-5  ]: On model.dbsql_dbt_tpch.CustomerMgmtView: ROLLBACK
[0m15:30:05.783106 [debug] [Thread-5  ]: Databricks adapter: NotImplemented: rollback
[0m15:30:05.783320 [debug] [Thread-5  ]: On model.dbsql_dbt_tpch.CustomerMgmtView: Close
[0m15:30:05.784567 [debug] [Thread-8  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

  
    
        create or replace table `main`.`tpcdi`.`DimBroker`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT

  cast(employeeid as BIGINT) brokerid,
  cast(managerid as BIGINT) managerid,
  employeefirstname firstname,
  employeelastname lastname,
  employeemi middleinitial,
  employeebranch branch,
  employeeoffice office,
  employeephone phone,
  true iscurrent,
  1 batchid,
  (SELECT min(to_date(datevalue)) as effectivedate FROM `main`.`tpcdi`.`DimDate`) effectivedate,
  date('9999-12-31') enddate,
  bigint(concat(date_format(enddate, 'yyyyMMdd'), cast(brokerid as string))) as sk_brokerid
FROM  `main`.`tpcdi`.`HR`
WHERE employeejobcode = 314
  
[0m15:30:05.784900 [debug] [Thread-8  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HR` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 33 pos 6
[0m15:30:05.785302 [debug] [Thread-8  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HR` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 33 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HR` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 33 pos 6
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:30:05.785669 [debug] [Thread-8  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe0\x13c\x1eU\x98\x07U\x8c\xf0?6\xaf'
[0m15:30:05.785994 [debug] [Thread-8  ]: Timing info for model.dbsql_dbt_tpch.DimBroker (execute): 15:30:05.056430 => 15:30:05.785870
[0m15:30:05.786211 [debug] [Thread-8  ]: On model.dbsql_dbt_tpch.DimBroker: ROLLBACK
[0m15:30:05.786408 [debug] [Thread-8  ]: Databricks adapter: NotImplemented: rollback
[0m15:30:05.786596 [debug] [Thread-8  ]: On model.dbsql_dbt_tpch.DimBroker: Close
[0m15:30:05.787732 [debug] [Thread-6  ]: Runtime Error in model DailyMarketHistorical (models/base/DailyMarketHistorical.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketHistorical` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:30:05.788036 [debug] [Thread-6  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45e836c6-6b38-41c0-a54e-547104f046cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x147aa6790>]}
[0m15:30:05.788417 [error] [Thread-6  ]: 6 of 45 ERROR creating sql view model tpcdi.DailyMarketHistorical .............. [[31mERROR[0m in 0.85s]
[0m15:30:05.788761 [debug] [Thread-6  ]: Finished running node model.dbsql_dbt_tpch.DailyMarketHistorical
[0m15:30:05.813792 [debug] [Thread-4  ]: Runtime Error in model CustomerIncremental (models/base/CustomerIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`customerincrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:30:05.814259 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45e836c6-6b38-41c0-a54e-547104f046cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x147ad4040>]}
[0m15:30:05.814675 [error] [Thread-4  ]: 4 of 45 ERROR creating sql view model tpcdi.CustomerIncremental ................ [[31mERROR[0m in 0.88s]
[0m15:30:05.815052 [debug] [Thread-4  ]: Finished running node model.dbsql_dbt_tpch.CustomerIncremental
[0m15:30:05.818984 [debug] [Thread-3  ]: Runtime Error in model CashTransactionIncremental (models/base/CashTransactionIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`CashTransactionIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:30:05.819271 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45e836c6-6b38-41c0-a54e-547104f046cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x147afb760>]}
[0m15:30:05.819611 [error] [Thread-3  ]: 3 of 45 ERROR creating sql view model tpcdi.CashTransactionIncremental ......... [[31mERROR[0m in 0.88s]
[0m15:30:05.819948 [debug] [Thread-3  ]: Finished running node model.dbsql_dbt_tpch.CashTransactionIncremental
[0m15:30:05.820666 [debug] [Thread-2  ]: Runtime Error in model BatchDate (models/base/BatchDate.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`BatchDateuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:30:05.820922 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45e836c6-6b38-41c0-a54e-547104f046cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x147a65280>]}
[0m15:30:05.821232 [error] [Thread-2  ]: 2 of 45 ERROR creating sql view model tpcdi.BatchDate .......................... [[31mERROR[0m in 0.89s]
[0m15:30:05.821540 [debug] [Thread-2  ]: Finished running node model.dbsql_dbt_tpch.BatchDate
[0m15:30:05.838179 [debug] [Thread-10 ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.HoldingIncremental"} */
create or replace view `main`.`tpcdi`.`HoldingIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`HoldingIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`HoldingIncrementaltres`

[0m15:30:05.838576 [debug] [Thread-10 ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HoldingIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:30:05.838958 [debug] [Thread-10 ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HoldingIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HoldingIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:30:05.839306 [debug] [Thread-10 ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe0\x13t\x16\xb6\x8c\x985\x0f\xbb\xda!n'
[0m15:30:05.839654 [debug] [Thread-10 ]: Timing info for model.dbsql_dbt_tpch.HoldingIncremental (execute): 15:30:05.080397 => 15:30:05.839525
[0m15:30:05.839882 [debug] [Thread-10 ]: On model.dbsql_dbt_tpch.HoldingIncremental: ROLLBACK
[0m15:30:05.840094 [debug] [Thread-10 ]: Databricks adapter: NotImplemented: rollback
[0m15:30:05.840303 [debug] [Thread-10 ]: On model.dbsql_dbt_tpch.HoldingIncremental: Close
[0m15:30:05.841395 [debug] [Thread-7  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DailyMarketIncremental"} */
create or replace view `main`.`tpcdi`.`DailyMarketIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`DailyMarketIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`DailyMarketIncrementaltres`

[0m15:30:05.841737 [debug] [Thread-7  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:30:05.842097 [debug] [Thread-7  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:30:05.842458 [debug] [Thread-7  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe0\x13u\x11\xf7\xb3\xa6\xdf\xf5\x8a\x08\xdf\xd1'
[0m15:30:05.842873 [debug] [Thread-7  ]: Timing info for model.dbsql_dbt_tpch.DailyMarketIncremental (execute): 15:30:05.052658 => 15:30:05.842713
[0m15:30:05.843127 [debug] [Thread-7  ]: On model.dbsql_dbt_tpch.DailyMarketIncremental: ROLLBACK
[0m15:30:05.843348 [debug] [Thread-7  ]: Databricks adapter: NotImplemented: rollback
[0m15:30:05.843548 [debug] [Thread-7  ]: On model.dbsql_dbt_tpch.DailyMarketIncremental: Close
[0m15:30:05.850406 [debug] [Thread-8  ]: Runtime Error in model DimBroker (models/silver/DimBroker.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HR` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 33 pos 6
[0m15:30:05.851176 [debug] [Thread-5  ]: Runtime Error in model CustomerMgmtView (models/base/CustomerMgmtView.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`roberto_salcido_tpcdi_stage`.`customermgmt10` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 10 pos 4
[0m15:30:05.851471 [debug] [Thread-8  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45e836c6-6b38-41c0-a54e-547104f046cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x147a68520>]}
[0m15:30:05.851714 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45e836c6-6b38-41c0-a54e-547104f046cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x147ae68b0>]}
[0m15:30:05.852095 [error] [Thread-8  ]: 8 of 45 ERROR creating sql table model tpcdi.DimBroker ......................... [[31mERROR[0m in 0.91s]
[0m15:30:05.852557 [error] [Thread-5  ]: 5 of 45 ERROR creating sql view model tpcdi.CustomerMgmtView ................... [[31mERROR[0m in 0.91s]
[0m15:30:05.852907 [debug] [Thread-8  ]: Finished running node model.dbsql_dbt_tpch.DimBroker
[0m15:30:05.853145 [debug] [Thread-5  ]: Finished running node model.dbsql_dbt_tpch.CustomerMgmtView
[0m15:30:05.853601 [debug] [Thread-15 ]: Began running node model.dbsql_dbt_tpch.DimCustomerStg
[0m15:30:05.853828 [info ] [Thread-15 ]: 14 of 45 SKIP relation tpcdi.DimCustomerStg .................................... [[33mSKIP[0m]
[0m15:30:05.854095 [debug] [Thread-15 ]: Finished running node model.dbsql_dbt_tpch.DimCustomerStg
[0m15:30:05.854395 [debug] [Thread-17 ]: Began running node test.dbsql_dbt_tpch.dateval_DimCustomerStg_effectivedate.8a3a23ae4e
[0m15:30:05.854629 [info ] [Thread-17 ]: 15 of 45 SKIP test dateval_DimCustomerStg_effectivedate ........................ [[33mSKIP[0m]
[0m15:30:05.854903 [debug] [Thread-17 ]: Finished running node test.dbsql_dbt_tpch.dateval_DimCustomerStg_effectivedate.8a3a23ae4e
[0m15:30:05.855203 [debug] [Thread-19 ]: Began running node model.dbsql_dbt_tpch.DimAccount
[0m15:30:05.855432 [info ] [Thread-19 ]: 16 of 45 SKIP relation tpcdi.DimAccount ........................................ [[33mSKIP[0m]
[0m15:30:05.855703 [debug] [Thread-19 ]: Finished running node model.dbsql_dbt_tpch.DimAccount
[0m15:30:05.856009 [debug] [Thread-21 ]: Began running node test.dbsql_dbt_tpch.not_null_DimAccount_sk_brokerid.dd6e992726
[0m15:30:05.856240 [debug] [Thread-22 ]: Began running node test.dbsql_dbt_tpch.not_null_DimAccount_sk_customerid.b8baa6ce87
[0m15:30:05.856453 [info ] [Thread-21 ]: 17 of 45 SKIP test not_null_DimAccount_sk_brokerid ............................. [[33mSKIP[0m]
[0m15:30:05.856730 [info ] [Thread-22 ]: 18 of 45 SKIP test not_null_DimAccount_sk_customerid ........................... [[33mSKIP[0m]
[0m15:30:05.857007 [debug] [Thread-21 ]: Finished running node test.dbsql_dbt_tpch.not_null_DimAccount_sk_brokerid.dd6e992726
[0m15:30:05.857215 [debug] [Thread-22 ]: Finished running node test.dbsql_dbt_tpch.not_null_DimAccount_sk_customerid.b8baa6ce87
[0m15:30:05.857534 [debug] [Thread-24 ]: Began running node model.dbsql_dbt_tpch.FactCashBalances
[0m15:30:05.857750 [info ] [Thread-24 ]: 19 of 45 SKIP relation tpcdi.FactCashBalances .................................. [[33mSKIP[0m]
[0m15:30:05.858012 [debug] [Thread-24 ]: Finished running node model.dbsql_dbt_tpch.FactCashBalances
[0m15:30:05.858285 [debug] [Thread-14 ]: Began running node test.dbsql_dbt_tpch.not_null_FactCashBalances_sk_accountid.878cf23033
[0m15:30:05.858503 [info ] [Thread-14 ]: 20 of 45 SKIP test not_null_FactCashBalances_sk_accountid ...................... [[33mSKIP[0m]
[0m15:30:05.858767 [debug] [Thread-14 ]: Finished running node test.dbsql_dbt_tpch.not_null_FactCashBalances_sk_accountid.878cf23033
[0m15:30:05.887293 [debug] [Thread-13 ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.WatchIncremental"} */
create or replace view `main`.`tpcdi`.`WatchIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`WatchIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`WatchIncrementaltres`

[0m15:30:05.887659 [debug] [Thread-13 ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`WatchIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:30:05.888006 [debug] [Thread-13 ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`WatchIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`WatchIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:30:05.888322 [debug] [Thread-13 ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe0\x13~\x1c1\x97\x0c\x15f\x8e#j\x02'
[0m15:30:05.888634 [debug] [Thread-13 ]: Timing info for model.dbsql_dbt_tpch.WatchIncremental (execute): 15:30:05.084480 => 15:30:05.888519
[0m15:30:05.888840 [debug] [Thread-13 ]: On model.dbsql_dbt_tpch.WatchIncremental: ROLLBACK
[0m15:30:05.889035 [debug] [Thread-13 ]: Databricks adapter: NotImplemented: rollback
[0m15:30:05.889205 [debug] [Thread-13 ]: On model.dbsql_dbt_tpch.WatchIncremental: Close
[0m15:30:05.925110 [debug] [Thread-12 ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.TradeIncremental"} */
create or replace view `main`.`tpcdi`.`TradeIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`TradeIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`TradeIncrementaltres`

[0m15:30:05.925474 [debug] [Thread-12 ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`TradeIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:30:05.925835 [debug] [Thread-12 ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`TradeIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`TradeIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:30:05.926180 [debug] [Thread-12 ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe0\x13|\x16\xfa\xb4\xa0\x83\xa8\x0e\x8e\xa4\t'
[0m15:30:05.926514 [debug] [Thread-12 ]: Timing info for model.dbsql_dbt_tpch.TradeIncremental (execute): 15:30:05.082484 => 15:30:05.926391
[0m15:30:05.926738 [debug] [Thread-12 ]: On model.dbsql_dbt_tpch.TradeIncremental: ROLLBACK
[0m15:30:05.926941 [debug] [Thread-12 ]: Databricks adapter: NotImplemented: rollback
[0m15:30:05.927130 [debug] [Thread-12 ]: On model.dbsql_dbt_tpch.TradeIncremental: Close
[0m15:30:05.929400 [debug] [Thread-11 ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.ProspectRaw"} */
create or replace view `main`.`tpcdi`.`ProspectRaw`
  
  
  as
    

select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`ProspectRawuno`

 UNION ALL

select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`ProspectRawdos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`ProspectRawtres`

[0m15:30:05.929781 [debug] [Thread-11 ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`ProspectRawuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:30:05.930123 [debug] [Thread-11 ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`ProspectRawuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`ProspectRawuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:30:05.930518 [debug] [Thread-11 ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe0\x13}\x160\xbe!\x97\x01\xf1\xd0\x96)'
[0m15:30:05.930869 [debug] [Thread-11 ]: Timing info for model.dbsql_dbt_tpch.ProspectRaw (execute): 15:30:05.078419 => 15:30:05.930750
[0m15:30:05.931074 [debug] [Thread-11 ]: On model.dbsql_dbt_tpch.ProspectRaw: ROLLBACK
[0m15:30:05.931294 [debug] [Thread-11 ]: Databricks adapter: NotImplemented: rollback
[0m15:30:05.931494 [debug] [Thread-11 ]: On model.dbsql_dbt_tpch.ProspectRaw: Close
[0m15:30:05.932104 [debug] [Thread-10 ]: Runtime Error in model HoldingIncremental (models/base/HoldingIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HoldingIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:30:05.932507 [debug] [Thread-10 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45e836c6-6b38-41c0-a54e-547104f046cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x147a40760>]}
[0m15:30:05.932934 [error] [Thread-10 ]: 10 of 45 ERROR creating sql view model tpcdi.HoldingIncremental ................ [[31mERROR[0m in 0.99s]
[0m15:30:05.933358 [debug] [Thread-10 ]: Finished running node model.dbsql_dbt_tpch.HoldingIncremental
[0m15:30:05.934241 [debug] [Thread-7  ]: Runtime Error in model DailyMarketIncremental (models/base/DailyMarketIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:30:05.934545 [debug] [Thread-7  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45e836c6-6b38-41c0-a54e-547104f046cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x147c2a9d0>]}
[0m15:30:05.934892 [error] [Thread-7  ]: 7 of 45 ERROR creating sql view model tpcdi.DailyMarketIncremental ............. [[31mERROR[0m in 1.00s]
[0m15:30:05.935218 [debug] [Thread-7  ]: Finished running node model.dbsql_dbt_tpch.DailyMarketIncremental
[0m15:30:05.935593 [debug] [Thread-6  ]: Began running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
[0m15:30:05.935839 [info ] [Thread-6  ]: 21 of 45 SKIP relation tpcdi.tempDailyMarketHistorical ......................... [[33mSKIP[0m]
[0m15:30:05.936115 [debug] [Thread-6  ]: Finished running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
[0m15:30:05.953777 [debug] [Thread-13 ]: Runtime Error in model WatchIncremental (models/base/WatchIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`WatchIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:30:05.954130 [debug] [Thread-13 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45e836c6-6b38-41c0-a54e-547104f046cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x147a6c1c0>]}
[0m15:30:05.954462 [error] [Thread-13 ]: 13 of 45 ERROR creating sql view model tpcdi.WatchIncremental .................. [[31mERROR[0m in 1.01s]
[0m15:30:05.954752 [debug] [Thread-13 ]: Finished running node model.dbsql_dbt_tpch.WatchIncremental
[0m15:30:05.990561 [debug] [Thread-11 ]: Runtime Error in model ProspectRaw (models/base/ProspectRaw.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`ProspectRawuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:30:05.990911 [debug] [Thread-11 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45e836c6-6b38-41c0-a54e-547104f046cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x147ada160>]}
[0m15:30:05.991230 [error] [Thread-11 ]: 11 of 45 ERROR creating sql view model tpcdi.ProspectRaw ....................... [[31mERROR[0m in 1.05s]
[0m15:30:05.991520 [debug] [Thread-11 ]: Finished running node model.dbsql_dbt_tpch.ProspectRaw
[0m15:30:05.991988 [debug] [Thread-3  ]: Began running node model.dbsql_dbt_tpch.Prospect
[0m15:30:05.992188 [info ] [Thread-3  ]: 22 of 45 SKIP relation tpcdi.Prospect .......................................... [[33mSKIP[0m]
[0m15:30:05.992423 [debug] [Thread-3  ]: Finished running node model.dbsql_dbt_tpch.Prospect
[0m15:30:05.992670 [debug] [Thread-8  ]: Began running node model.dbsql_dbt_tpch.DimCustomer
[0m15:30:05.992845 [info ] [Thread-8  ]: 23 of 45 SKIP relation tpcdi.DimCustomer ....................................... [[33mSKIP[0m]
[0m15:30:05.993074 [debug] [Thread-8  ]: Finished running node model.dbsql_dbt_tpch.DimCustomer
[0m15:30:05.993341 [debug] [Thread-16 ]: Began running node test.dbsql_dbt_tpch.accepted_values_DimCustomer_tier__1__2__3.7be85b3b7a
[0m15:30:05.993541 [info ] [Thread-16 ]: 24 of 45 SKIP test accepted_values_DimCustomer_tier__1__2__3 ................... [[33mSKIP[0m]
[0m15:30:05.993780 [debug] [Thread-16 ]: Finished running node test.dbsql_dbt_tpch.accepted_values_DimCustomer_tier__1__2__3.7be85b3b7a
[0m15:30:05.993965 [debug] [Thread-16 ]: Began running node test.dbsql_dbt_tpch.not_null_DimCustomer_tier.e0964b5cac
[0m15:30:05.994135 [info ] [Thread-16 ]: 25 of 45 SKIP test not_null_DimCustomer_tier ................................... [[33mSKIP[0m]
[0m15:30:05.994362 [debug] [Thread-16 ]: Finished running node test.dbsql_dbt_tpch.not_null_DimCustomer_tier.e0964b5cac
[0m15:30:06.000801 [debug] [Thread-12 ]: Runtime Error in model TradeIncremental (models/base/TradeIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`TradeIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:30:06.001122 [debug] [Thread-12 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45e836c6-6b38-41c0-a54e-547104f046cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x147a2ee80>]}
[0m15:30:06.001417 [error] [Thread-12 ]: 12 of 45 ERROR creating sql view model tpcdi.TradeIncremental .................. [[31mERROR[0m in 1.06s]
[0m15:30:06.001691 [debug] [Thread-12 ]: Finished running node model.dbsql_dbt_tpch.TradeIncremental
[0m15:30:06.011524 [debug] [Thread-9  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.FinWire"} */
create or replace view `main`.`tpcdi`.`FinWire`
  
  
  as
    

select *, substring(value, 16, 3) rectype from 
text.`dbfs:/tmp/tpcdi/sf=10/Batch1/FINWIRE*`

[0m15:30:06.011800 [debug] [Thread-9  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: text; line 9 pos 0
[0m15:30:06.012377 [debug] [Thread-9  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: text; line 9 pos 0
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: text; line 9 pos 0
	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:76)
	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:171)
	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:93)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1306)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1305)
	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:81)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1335)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1332)
	at org.apache.spark.sql.catalyst.plans.logical.CreateView.mapChildren(v2Commands.scala:1350)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:79)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:78)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:93)
	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:51)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:229)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:229)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:226)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:218)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:296)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:296)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:296)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:383)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:376)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:283)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:376)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:304)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:379)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:355)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:156)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:348)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:380)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:817)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:380)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1040)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:377)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:150)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:140)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getRuntimeCategory$1(QueryRuntimePrediction.scala:300)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1040)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:299)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:349)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:345)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more
Caused by: org.apache.spark.sql.AnalysisException: [PATH_NOT_FOUND] Path does not exist: dbfs:/tmp/tpcdi/sf=10/Batch1/FINWIRE*.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:1592)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:844)
	at scala.collection.immutable.List.flatMap(List.scala:366)
	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:831)
	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:616)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:451)
	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:160)
	... 95 more

[0m15:30:06.012953 [debug] [Thread-9  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe0\x13t\x16\x1f\x84\xfb\x12zz\xf1\xee['
[0m15:30:06.013327 [debug] [Thread-9  ]: Timing info for model.dbsql_dbt_tpch.FinWire (execute): 15:30:05.054569 => 15:30:06.013195
[0m15:30:06.013538 [debug] [Thread-9  ]: On model.dbsql_dbt_tpch.FinWire: ROLLBACK
[0m15:30:06.013754 [debug] [Thread-9  ]: Databricks adapter: NotImplemented: rollback
[0m15:30:06.013965 [debug] [Thread-9  ]: On model.dbsql_dbt_tpch.FinWire: Close
[0m15:30:06.071301 [debug] [Thread-9  ]: Runtime Error in model FinWire (models/base/FinWire.sql)
  [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: text; line 9 pos 0
[0m15:30:06.071730 [debug] [Thread-9  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45e836c6-6b38-41c0-a54e-547104f046cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x147ca0910>]}
[0m15:30:06.072127 [error] [Thread-9  ]: 9 of 45 ERROR creating sql view model tpcdi.FinWire ............................ [[31mERROR[0m in 1.13s]
[0m15:30:06.072469 [debug] [Thread-9  ]: Finished running node model.dbsql_dbt_tpch.FinWire
[0m15:30:06.073008 [debug] [Thread-18 ]: Began running node model.dbsql_dbt_tpch.DimCompany
[0m15:30:06.073334 [info ] [Thread-18 ]: 26 of 45 SKIP relation tpcdi.DimCompany ........................................ [[33mSKIP[0m]
[0m15:30:06.073634 [debug] [Thread-18 ]: Finished running node model.dbsql_dbt_tpch.DimCompany
[0m15:30:06.074005 [debug] [Thread-20 ]: Began running node test.dbsql_dbt_tpch.dateval_DimCompany_effectivedate.4df1621a42
[0m15:30:06.074419 [info ] [Thread-20 ]: 27 of 45 SKIP test dateval_DimCompany_effectivedate ............................ [[33mSKIP[0m]
[0m15:30:06.074782 [debug] [Thread-20 ]: Finished running node test.dbsql_dbt_tpch.dateval_DimCompany_effectivedate.4df1621a42
[0m15:30:06.075159 [debug] [Thread-23 ]: Began running node model.dbsql_dbt_tpch.DimSecurity
[0m15:30:06.075417 [info ] [Thread-23 ]: 28 of 45 SKIP relation tpcdi.DimSecurity ....................................... [[33mSKIP[0m]
[0m15:30:06.075734 [debug] [Thread-23 ]: Finished running node model.dbsql_dbt_tpch.DimSecurity
[0m15:30:06.075970 [debug] [Thread-23 ]: Began running node model.dbsql_dbt_tpch.Financial
[0m15:30:06.076191 [info ] [Thread-23 ]: 29 of 45 SKIP relation tpcdi.Financial ......................................... [[33mSKIP[0m]
[0m15:30:06.076566 [debug] [Thread-23 ]: Finished running node model.dbsql_dbt_tpch.Financial
[0m15:30:06.077060 [debug] [Thread-22 ]: Began running node test.dbsql_dbt_tpch.not_null_DimSecurity_sk_companyid.e728df82fe
[0m15:30:06.077384 [debug] [Thread-25 ]: Began running node test.dbsql_dbt_tpch.not_null_Financial_sk_companyid.529c7c2c12
[0m15:30:06.077712 [info ] [Thread-22 ]: 30 of 45 SKIP test not_null_DimSecurity_sk_companyid ........................... [[33mSKIP[0m]
[0m15:30:06.078447 [debug] [Thread-22 ]: Finished running node test.dbsql_dbt_tpch.not_null_DimSecurity_sk_companyid.e728df82fe
[0m15:30:06.078062 [info ] [Thread-25 ]: 31 of 45 SKIP test not_null_Financial_sk_companyid ............................. [[33mSKIP[0m]
[0m15:30:06.078943 [debug] [Thread-1  ]: Began running node model.dbsql_dbt_tpch.DimTrade
[0m15:30:06.079158 [debug] [Thread-25 ]: Finished running node test.dbsql_dbt_tpch.not_null_Financial_sk_companyid.529c7c2c12
[0m15:30:06.079361 [debug] [Thread-14 ]: Began running node model.dbsql_dbt_tpch.FactWatches
[0m15:30:06.079588 [info ] [Thread-1  ]: 32 of 45 SKIP relation tpcdi.DimTrade .......................................... [[33mSKIP[0m]
[0m15:30:06.080404 [debug] [Thread-7  ]: Began running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
[0m15:30:06.080082 [info ] [Thread-14 ]: 33 of 45 SKIP relation tpcdi.FactWatches ....................................... [[33mSKIP[0m]
[0m15:30:06.080817 [debug] [Thread-1  ]: Finished running node model.dbsql_dbt_tpch.DimTrade
[0m15:30:06.081084 [info ] [Thread-7  ]: 34 of 45 SKIP relation tpcdi.tempSumpFiBasicEps ................................ [[33mSKIP[0m]
[0m15:30:06.081382 [debug] [Thread-14 ]: Finished running node model.dbsql_dbt_tpch.FactWatches
[0m15:30:06.081833 [debug] [Thread-7  ]: Finished running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
[0m15:30:06.082112 [debug] [Thread-6  ]: Began running node test.dbsql_dbt_tpch.not_null_DimTrade_sk_accountid.4db26dca2c
[0m15:30:06.082367 [debug] [Thread-13 ]: Began running node test.dbsql_dbt_tpch.not_null_DimTrade_sk_securityid.13bd8f9da6
[0m15:30:06.082590 [debug] [Thread-14 ]: Began running node test.dbsql_dbt_tpch.tradecom_DimTrade_commission.ad38ac2d42
[0m15:30:06.082782 [debug] [Thread-11 ]: Began running node test.dbsql_dbt_tpch.tradefee_DimTrade_fee.5ee05431ee
[0m15:30:06.083396 [debug] [Thread-2  ]: Began running node model.dbsql_dbt_tpch.FactMarketHistory
[0m15:30:06.083839 [debug] [Thread-3  ]: Began running node test.dbsql_dbt_tpch.not_null_FactWatches_sk_customerid.1c7356b8c9
[0m15:30:06.083174 [info ] [Thread-6  ]: 35 of 45 SKIP test not_null_DimTrade_sk_accountid .............................. [[33mSKIP[0m]
[0m15:30:06.084119 [debug] [Thread-5  ]: Began running node test.dbsql_dbt_tpch.not_null_FactWatches_sk_securityid.8a6b5e97b3
[0m15:30:06.083637 [info ] [Thread-13 ]: 36 of 45 SKIP test not_null_DimTrade_sk_securityid ............................. [[33mSKIP[0m]
[0m15:30:06.084397 [info ] [Thread-14 ]: 37 of 45 SKIP test tradecom_DimTrade_commission ................................ [[33mSKIP[0m]
[0m15:30:06.084687 [info ] [Thread-11 ]: 38 of 45 SKIP test tradefee_DimTrade_fee ....................................... [[33mSKIP[0m]
[0m15:30:06.084992 [info ] [Thread-2  ]: 39 of 45 SKIP relation tpcdi.FactMarketHistory ................................. [[33mSKIP[0m]
[0m15:30:06.085291 [info ] [Thread-3  ]: 40 of 45 SKIP test not_null_FactWatches_sk_customerid .......................... [[33mSKIP[0m]
[0m15:30:06.085576 [debug] [Thread-6  ]: Finished running node test.dbsql_dbt_tpch.not_null_DimTrade_sk_accountid.4db26dca2c
[0m15:30:06.085799 [info ] [Thread-5  ]: 41 of 45 SKIP test not_null_FactWatches_sk_securityid .......................... [[33mSKIP[0m]
[0m15:30:06.086069 [debug] [Thread-13 ]: Finished running node test.dbsql_dbt_tpch.not_null_DimTrade_sk_securityid.13bd8f9da6
[0m15:30:06.086274 [debug] [Thread-14 ]: Finished running node test.dbsql_dbt_tpch.tradecom_DimTrade_commission.ad38ac2d42
[0m15:30:06.086476 [debug] [Thread-11 ]: Finished running node test.dbsql_dbt_tpch.tradefee_DimTrade_fee.5ee05431ee
[0m15:30:06.086668 [debug] [Thread-2  ]: Finished running node model.dbsql_dbt_tpch.FactMarketHistory
[0m15:30:06.086873 [debug] [Thread-3  ]: Finished running node test.dbsql_dbt_tpch.not_null_FactWatches_sk_customerid.1c7356b8c9
[0m15:30:06.087127 [debug] [Thread-5  ]: Finished running node test.dbsql_dbt_tpch.not_null_FactWatches_sk_securityid.8a6b5e97b3
[0m15:30:06.087796 [debug] [Thread-15 ]: Began running node model.dbsql_dbt_tpch.FactHoldings
[0m15:30:06.088009 [debug] [Thread-16 ]: Began running node test.dbsql_dbt_tpch.not_null_FactMarketHistory_PERatio.83761a27d0
[0m15:30:06.088197 [info ] [Thread-15 ]: 42 of 45 SKIP relation tpcdi.FactHoldings ...................................... [[33mSKIP[0m]
[0m15:30:06.088429 [debug] [Thread-12 ]: Began running node test.dbsql_dbt_tpch.not_null_FactMarketHistory_sk_securityid.16a0e9b8b2
[0m15:30:06.088680 [info ] [Thread-16 ]: 43 of 45 SKIP test not_null_FactMarketHistory_PERatio .......................... [[33mSKIP[0m]
[0m15:30:06.088945 [debug] [Thread-15 ]: Finished running node model.dbsql_dbt_tpch.FactHoldings
[0m15:30:06.089152 [info ] [Thread-12 ]: 44 of 45 SKIP test not_null_FactMarketHistory_sk_securityid .................... [[33mSKIP[0m]
[0m15:30:06.089403 [debug] [Thread-16 ]: Finished running node test.dbsql_dbt_tpch.not_null_FactMarketHistory_PERatio.83761a27d0
[0m15:30:06.089665 [debug] [Thread-12 ]: Finished running node test.dbsql_dbt_tpch.not_null_FactMarketHistory_sk_securityid.16a0e9b8b2
[0m15:30:06.089942 [debug] [Thread-17 ]: Began running node test.dbsql_dbt_tpch.not_null_FactHoldings_currentprice.98a44eac00
[0m15:30:06.090175 [info ] [Thread-17 ]: 45 of 45 SKIP test not_null_FactHoldings_currentprice .......................... [[33mSKIP[0m]
[0m15:30:06.090422 [debug] [Thread-17 ]: Finished running node test.dbsql_dbt_tpch.not_null_FactHoldings_currentprice.98a44eac00
[0m15:30:06.091688 [debug] [MainThread]: On master: ROLLBACK
[0m15:30:06.091882 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:30:06.287707 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:30:06.288336 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:30:06.288634 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:30:06.288922 [debug] [MainThread]: On master: ROLLBACK
[0m15:30:06.289191 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:30:06.289404 [debug] [MainThread]: On master: Close
[0m15:30:06.356033 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:30:06.356687 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.AccountIncremental' was properly closed.
[0m15:30:06.356962 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.BatchDate' was properly closed.
[0m15:30:06.357184 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.CashTransactionIncremental' was properly closed.
[0m15:30:06.357394 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.CustomerIncremental' was properly closed.
[0m15:30:06.357592 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.CustomerMgmtView' was properly closed.
[0m15:30:06.357789 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.DailyMarketHistorical' was properly closed.
[0m15:30:06.357989 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.DailyMarketIncremental' was properly closed.
[0m15:30:06.358180 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.DimBroker' was properly closed.
[0m15:30:06.358372 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.FinWire' was properly closed.
[0m15:30:06.358564 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.HoldingIncremental' was properly closed.
[0m15:30:06.358755 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.ProspectRaw' was properly closed.
[0m15:30:06.358945 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.TradeIncremental' was properly closed.
[0m15:30:06.359130 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.WatchIncremental' was properly closed.
[0m15:30:06.362518 [info ] [MainThread]: 
[0m15:30:06.362934 [info ] [MainThread]: Finished running 12 view models, 15 table models, 18 tests in 0 hours 0 minutes and 3.26 seconds (3.26s).
[0m15:30:06.364866 [debug] [MainThread]: Command end result
[0m15:30:06.394671 [info ] [MainThread]: 
[0m15:30:06.395096 [info ] [MainThread]: [31mCompleted with 13 errors and 0 warnings:[0m
[0m15:30:06.395363 [info ] [MainThread]: 
[0m15:30:06.395617 [error] [MainThread]: [33mRuntime Error in model AccountIncremental (models/base/AccountIncremental.sql)[0m
[0m15:30:06.395862 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`AccountIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:30:06.396097 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:30:06.396325 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:30:06.396553 [info ] [MainThread]: 
[0m15:30:06.396782 [error] [MainThread]: [33mRuntime Error in model DailyMarketHistorical (models/base/DailyMarketHistorical.sql)[0m
[0m15:30:06.397012 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketHistorical` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:30:06.397241 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:30:06.397465 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:30:06.397688 [info ] [MainThread]: 
[0m15:30:06.397896 [error] [MainThread]: [33mRuntime Error in model CustomerIncremental (models/base/CustomerIncremental.sql)[0m
[0m15:30:06.398101 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`customerincrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:30:06.398306 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:30:06.398508 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:30:06.398706 [info ] [MainThread]: 
[0m15:30:06.398905 [error] [MainThread]: [33mRuntime Error in model CashTransactionIncremental (models/base/CashTransactionIncremental.sql)[0m
[0m15:30:06.399110 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`CashTransactionIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:30:06.399312 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:30:06.399512 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:30:06.399708 [info ] [MainThread]: 
[0m15:30:06.399907 [error] [MainThread]: [33mRuntime Error in model BatchDate (models/base/BatchDate.sql)[0m
[0m15:30:06.400108 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`BatchDateuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:30:06.400309 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:30:06.400505 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:30:06.400699 [info ] [MainThread]: 
[0m15:30:06.400898 [error] [MainThread]: [33mRuntime Error in model DimBroker (models/silver/DimBroker.sql)[0m
[0m15:30:06.401096 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HR` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:30:06.401296 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:30:06.401495 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 33 pos 6
[0m15:30:06.401695 [info ] [MainThread]: 
[0m15:30:06.401896 [error] [MainThread]: [33mRuntime Error in model CustomerMgmtView (models/base/CustomerMgmtView.sql)[0m
[0m15:30:06.402099 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`roberto_salcido_tpcdi_stage`.`customermgmt10` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:30:06.402302 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:30:06.402499 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 10 pos 4
[0m15:30:06.402691 [info ] [MainThread]: 
[0m15:30:06.402889 [error] [MainThread]: [33mRuntime Error in model HoldingIncremental (models/base/HoldingIncremental.sql)[0m
[0m15:30:06.403092 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HoldingIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:30:06.403296 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:30:06.403494 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:30:06.403688 [info ] [MainThread]: 
[0m15:30:06.403888 [error] [MainThread]: [33mRuntime Error in model DailyMarketIncremental (models/base/DailyMarketIncremental.sql)[0m
[0m15:30:06.404089 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:30:06.404292 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:30:06.404506 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:30:06.404703 [info ] [MainThread]: 
[0m15:30:06.404900 [error] [MainThread]: [33mRuntime Error in model WatchIncremental (models/base/WatchIncremental.sql)[0m
[0m15:30:06.405098 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`WatchIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:30:06.405298 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:30:06.405494 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:30:06.405690 [info ] [MainThread]: 
[0m15:30:06.405888 [error] [MainThread]: [33mRuntime Error in model ProspectRaw (models/base/ProspectRaw.sql)[0m
[0m15:30:06.406087 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`ProspectRawuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:30:06.406287 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:30:06.406482 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:30:06.406678 [info ] [MainThread]: 
[0m15:30:06.406875 [error] [MainThread]: [33mRuntime Error in model TradeIncremental (models/base/TradeIncremental.sql)[0m
[0m15:30:06.407073 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`TradeIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:30:06.407275 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:30:06.407473 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:30:06.407670 [info ] [MainThread]: 
[0m15:30:06.407851 [error] [MainThread]: [33mRuntime Error in model FinWire (models/base/FinWire.sql)[0m
[0m15:30:06.408033 [error] [MainThread]:   [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: text; line 9 pos 0
[0m15:30:06.408245 [info ] [MainThread]: 
[0m15:30:06.408453 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=13 SKIP=32 TOTAL=45
[0m15:30:06.408900 [debug] [MainThread]: Command `dbt build` failed at 15:30:06.408834 after 4.28 seconds
[0m15:30:06.409116 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110c395b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x147165d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x147ada160>]}
[0m15:30:06.409332 [debug] [MainThread]: Flushing usage events
[0m15:33:15.272803 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104be55e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e5be80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ea6a30>]}


============================== 15:33:15.276129 | 19724bb1-5a2d-4e80-8ab9-bb94f1ce71cb ==============================
[0m15:33:15.276129 [info ] [MainThread]: Running with dbt=1.5.1
[0m15:33:15.276474 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi', 'log_path': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:33:15.918565 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '19724bb1-5a2d-4e80-8ab9-bb94f1ce71cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102aac040>]}
[0m15:33:15.926808 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '19724bb1-5a2d-4e80-8ab9-bb94f1ce71cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ff66b50>]}
[0m15:33:15.951282 [debug] [MainThread]: checksum: 51f6b581eba8f8101bc020bf9faf8f96af641e2da86d581f66e2bdf0ff384b1c, vars: {}, profile: , target: , version: 1.5.1
[0m15:33:16.027359 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:33:16.029913 [debug] [MainThread]: Partial parsing: updated file: dbsql_dbt_tpch://models/sources.yml
[0m15:33:16.046000 [debug] [MainThread]: 1699: static parser successfully parsed incremental/FactHoldings.sql
[0m15:33:16.053734 [debug] [MainThread]: 1699: static parser successfully parsed incremental/DimTrade.sql
[0m15:33:16.055696 [debug] [MainThread]: 1699: static parser successfully parsed incremental/FactCashBalances.sql
[0m15:33:16.057501 [debug] [MainThread]: 1699: static parser successfully parsed incremental/DimAccount.sql
[0m15:33:16.059760 [debug] [MainThread]: 1699: static parser successfully parsed incremental/FactMarketHistory.sql
[0m15:33:16.061887 [debug] [MainThread]: 1699: static parser successfully parsed incremental/FactWatches.sql
[0m15:33:16.063814 [debug] [MainThread]: 1699: static parser successfully parsed silver/DimSecurity.sql
[0m15:33:16.065463 [debug] [MainThread]: 1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
[0m15:33:16.067183 [debug] [MainThread]: 1699: static parser successfully parsed silver/Financial.sql
[0m15:33:16.069516 [debug] [MainThread]: 1699: static parser successfully parsed silver/DimCompany.sql
[0m15:33:16.071202 [debug] [MainThread]: 1699: static parser successfully parsed incremental/DimCustomer.sql
[0m15:33:16.073153 [debug] [MainThread]: 1699: static parser successfully parsed incremental/Prospect.sql
[0m15:33:16.075039 [debug] [MainThread]: 1699: static parser successfully parsed incremental/DimCustomerStg.sql
[0m15:33:16.076653 [debug] [MainThread]: 1699: static parser successfully parsed base/AccountIncremental.sql
[0m15:33:16.078252 [debug] [MainThread]: 1699: static parser successfully parsed base/CashTransactionIncremental.sql
[0m15:33:16.079816 [debug] [MainThread]: 1699: static parser successfully parsed base/CustomerIncremental.sql
[0m15:33:16.081537 [debug] [MainThread]: 1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
[0m15:33:16.083163 [debug] [MainThread]: 1699: static parser successfully parsed base/DailyMarketHistorical.sql
[0m15:33:16.084768 [debug] [MainThread]: 1699: static parser successfully parsed base/DailyMarketIncremental.sql
[0m15:33:16.086455 [debug] [MainThread]: 1699: static parser successfully parsed silver/DimBroker.sql
[0m15:33:16.088007 [debug] [MainThread]: 1699: static parser successfully parsed base/HoldingIncremental.sql
[0m15:33:16.089674 [debug] [MainThread]: 1699: static parser successfully parsed base/BatchDate.sql
[0m15:33:16.091712 [debug] [MainThread]: 1699: static parser successfully parsed base/TradeIncremental.sql
[0m15:33:16.093735 [debug] [MainThread]: 1699: static parser successfully parsed base/WatchIncremental.sql
[0m15:33:16.095439 [debug] [MainThread]: 1699: static parser successfully parsed base/ProspectRaw.sql
[0m15:33:16.330119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '19724bb1-5a2d-4e80-8ab9-bb94f1ce71cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13a419100>]}
[0m15:33:16.344007 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '19724bb1-5a2d-4e80-8ab9-bb94f1ce71cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13a1313d0>]}
[0m15:33:16.344361 [info ] [MainThread]: Found 27 models, 18 tests, 0 snapshots, 0 analyses, 669 macros, 0 operations, 0 seed files, 33 sources, 0 exposures, 0 metrics, 0 groups
[0m15:33:16.346618 [info ] [MainThread]: 
[0m15:33:16.347185 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m15:33:16.349030 [debug] [ThreadPool]: Acquiring new databricks connection 'list_main'
[0m15:33:16.349607 [debug] [ThreadPool]: Acquiring new databricks connection 'list_main'
[0m15:33:16.349855 [debug] [ThreadPool]: Using databricks connection "list_main"
[0m15:33:16.350143 [debug] [ThreadPool]: Using databricks connection "list_main"
[0m15:33:16.350362 [debug] [ThreadPool]: On list_main: GetSchemas(database=`main`, schema=None)
[0m15:33:16.350567 [debug] [ThreadPool]: On list_main: GetSchemas(database=`main`, schema=None)
[0m15:33:16.350781 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:33:16.350990 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:33:16.849131 [debug] [ThreadPool]: SQL status: OK in 0.5 seconds
[0m15:33:16.854523 [debug] [ThreadPool]: On list_main: Close
[0m15:33:16.924984 [debug] [ThreadPool]: SQL status: OK in 0.5699999928474426 seconds
[0m15:33:16.927944 [debug] [ThreadPool]: On list_main: Close
[0m15:33:16.990575 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_main, now create_main_tpcdi)
[0m15:33:16.991858 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_main, now create_main_tpcdi_dbt_test__audit)
[0m15:33:16.992913 [debug] [ThreadPool]: Creating schema "database: "main"
schema: "tpcdi"
"
[0m15:33:16.993859 [debug] [ThreadPool]: Creating schema "database: "main"
schema: "tpcdi_dbt_test__audit"
"
[0m15:33:17.009406 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:33:17.006802 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:33:17.009748 [debug] [ThreadPool]: Using databricks connection "create_main_tpcdi_dbt_test__audit"
[0m15:33:17.010003 [debug] [ThreadPool]: Using databricks connection "create_main_tpcdi"
[0m15:33:17.010265 [debug] [ThreadPool]: On create_main_tpcdi_dbt_test__audit: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "create_main_tpcdi_dbt_test__audit"} */
create schema if not exists `main`.`tpcdi_dbt_test__audit`
  
[0m15:33:17.010535 [debug] [ThreadPool]: On create_main_tpcdi: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "create_main_tpcdi"} */
create schema if not exists `main`.`tpcdi`
  
[0m15:33:17.010785 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:33:17.011013 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:33:17.428745 [debug] [ThreadPool]: SQL status: OK in 0.41999998688697815 seconds
[0m15:33:17.433398 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m15:33:17.433871 [debug] [ThreadPool]: SQL status: OK in 0.41999998688697815 seconds
[0m15:33:17.434299 [debug] [ThreadPool]: On create_main_tpcdi_dbt_test__audit: ROLLBACK
[0m15:33:17.435420 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m15:33:17.435825 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m15:33:17.436168 [debug] [ThreadPool]: On create_main_tpcdi: ROLLBACK
[0m15:33:17.436487 [debug] [ThreadPool]: On create_main_tpcdi_dbt_test__audit: Close
[0m15:33:17.436817 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m15:33:17.437577 [debug] [ThreadPool]: On create_main_tpcdi: Close
[0m15:33:17.509593 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_main_tpcdi, now list_main_tpcdi)
[0m15:33:17.510606 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_main_tpcdi_dbt_test__audit, now list_main_tpcdi_dbt_test__audit)
[0m15:33:17.518669 [debug] [ThreadPool]: Using databricks connection "list_main_tpcdi_dbt_test__audit"
[0m15:33:17.520647 [debug] [ThreadPool]: On list_main_tpcdi_dbt_test__audit: GetTables(database=main, schema=tpcdi_dbt_test__audit, identifier=None)
[0m15:33:17.522408 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:33:17.555611 [debug] [ThreadPool]: Using databricks connection "list_main_tpcdi"
[0m15:33:17.556034 [debug] [ThreadPool]: On list_main_tpcdi: GetTables(database=main, schema=tpcdi, identifier=None)
[0m15:33:17.556522 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:33:17.887823 [debug] [ThreadPool]: SQL status: OK in 0.3700000047683716 seconds
[0m15:33:17.893116 [debug] [ThreadPool]: On list_main_tpcdi_dbt_test__audit: Close
[0m15:33:17.914858 [debug] [ThreadPool]: SQL status: OK in 0.36000001430511475 seconds
[0m15:33:17.918239 [debug] [ThreadPool]: On list_main_tpcdi: Close
[0m15:33:17.977371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '19724bb1-5a2d-4e80-8ab9-bb94f1ce71cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ff819d0>]}
[0m15:33:17.978221 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:33:17.978663 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:33:17.979575 [info ] [MainThread]: Concurrency: 25 threads (target='dev')
[0m15:33:17.980164 [info ] [MainThread]: 
[0m15:33:17.988897 [debug] [Thread-1  ]: Began running node model.dbsql_dbt_tpch.AccountIncremental
[0m15:33:17.989518 [debug] [Thread-2  ]: Began running node model.dbsql_dbt_tpch.BatchDate
[0m15:33:17.989932 [debug] [Thread-3  ]: Began running node model.dbsql_dbt_tpch.CashTransactionIncremental
[0m15:33:17.990967 [debug] [Thread-4  ]: Began running node model.dbsql_dbt_tpch.CustomerIncremental
[0m15:33:17.990528 [info ] [Thread-1  ]: 1 of 45 START sql view model tpcdi.AccountIncremental .......................... [RUN]
[0m15:33:17.991591 [debug] [Thread-5  ]: Began running node model.dbsql_dbt_tpch.CustomerMgmtView
[0m15:33:17.992036 [info ] [Thread-2  ]: 2 of 45 START sql view model tpcdi.BatchDate ................................... [RUN]
[0m15:33:17.992599 [info ] [Thread-3  ]: 3 of 45 START sql view model tpcdi.CashTransactionIncremental .................. [RUN]
[0m15:33:17.993138 [info ] [Thread-4  ]: 4 of 45 START sql view model tpcdi.CustomerIncremental ......................... [RUN]
[0m15:33:17.994019 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_main_tpcdi, now model.dbsql_dbt_tpch.AccountIncremental)
[0m15:33:17.994459 [debug] [Thread-6  ]: Began running node model.dbsql_dbt_tpch.DailyMarketHistorical
[0m15:33:17.995032 [debug] [Thread-7  ]: Began running node model.dbsql_dbt_tpch.DailyMarketIncremental
[0m15:33:17.995624 [info ] [Thread-5  ]: 5 of 45 START sql view model tpcdi.CustomerMgmtView ............................ [RUN]
[0m15:33:17.996250 [debug] [Thread-8  ]: Began running node model.dbsql_dbt_tpch.DimBroker
[0m15:33:17.996975 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_main_tpcdi_dbt_test__audit, now model.dbsql_dbt_tpch.BatchDate)
[0m15:33:17.997373 [debug] [Thread-9  ]: Began running node model.dbsql_dbt_tpch.FinWire
[0m15:33:17.997736 [debug] [Thread-10 ]: Began running node model.dbsql_dbt_tpch.HoldingIncremental
[0m15:33:17.998367 [debug] [Thread-3  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.CashTransactionIncremental'
[0m15:33:17.998818 [debug] [Thread-11 ]: Began running node model.dbsql_dbt_tpch.ProspectRaw
[0m15:33:17.999170 [debug] [Thread-12 ]: Began running node model.dbsql_dbt_tpch.TradeIncremental
[0m15:33:17.999744 [debug] [Thread-4  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.CustomerIncremental'
[0m15:33:18.000043 [debug] [Thread-13 ]: Began running node model.dbsql_dbt_tpch.WatchIncremental
[0m15:33:18.000366 [debug] [Thread-1  ]: Began compiling node model.dbsql_dbt_tpch.AccountIncremental
[0m15:33:18.000779 [info ] [Thread-6  ]: 6 of 45 START sql view model tpcdi.DailyMarketHistorical ....................... [RUN]
[0m15:33:18.001269 [info ] [Thread-7  ]: 7 of 45 START sql view model tpcdi.DailyMarketIncremental ...................... [RUN]
[0m15:33:18.002010 [debug] [Thread-5  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.CustomerMgmtView'
[0m15:33:18.002423 [info ] [Thread-8  ]: 8 of 45 START sql table model tpcdi.DimBroker .................................. [RUN]
[0m15:33:18.002841 [debug] [Thread-2  ]: Began compiling node model.dbsql_dbt_tpch.BatchDate
[0m15:33:18.003221 [info ] [Thread-9  ]: 9 of 45 START sql view model tpcdi.FinWire ..................................... [RUN]
[0m15:33:18.003649 [info ] [Thread-10 ]: 10 of 45 START sql view model tpcdi.HoldingIncremental ......................... [RUN]
[0m15:33:18.003994 [debug] [Thread-3  ]: Began compiling node model.dbsql_dbt_tpch.CashTransactionIncremental
[0m15:33:18.004305 [info ] [Thread-11 ]: 11 of 45 START sql view model tpcdi.ProspectRaw ................................ [RUN]
[0m15:33:18.004701 [info ] [Thread-12 ]: 12 of 45 START sql view model tpcdi.TradeIncremental ........................... [RUN]
[0m15:33:18.005037 [debug] [Thread-4  ]: Began compiling node model.dbsql_dbt_tpch.CustomerIncremental
[0m15:33:18.005339 [info ] [Thread-13 ]: 13 of 45 START sql view model tpcdi.WatchIncremental ........................... [RUN]
[0m15:33:18.009058 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.AccountIncremental"
[0m15:33:18.009731 [debug] [Thread-6  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.DailyMarketHistorical'
[0m15:33:18.010253 [debug] [Thread-7  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.DailyMarketIncremental'
[0m15:33:18.010562 [debug] [Thread-5  ]: Began compiling node model.dbsql_dbt_tpch.CustomerMgmtView
[0m15:33:18.011094 [debug] [Thread-8  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimBroker'
[0m15:33:18.014624 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.BatchDate"
[0m15:33:18.015159 [debug] [Thread-9  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.FinWire'
[0m15:33:18.015616 [debug] [Thread-10 ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.HoldingIncremental'
[0m15:33:18.017777 [debug] [Thread-11 ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.ProspectRaw'
[0m15:33:18.019378 [debug] [Thread-12 ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.TradeIncremental'
[0m15:33:18.024405 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.CashTransactionIncremental"
[0m15:33:18.027669 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.CustomerIncremental"
[0m15:33:18.028280 [debug] [Thread-13 ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.WatchIncremental'
[0m15:33:18.028693 [debug] [Thread-6  ]: Began compiling node model.dbsql_dbt_tpch.DailyMarketHistorical
[0m15:33:18.028976 [debug] [Thread-7  ]: Began compiling node model.dbsql_dbt_tpch.DailyMarketIncremental
[0m15:33:18.031762 [debug] [Thread-5  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.CustomerMgmtView"
[0m15:33:18.032089 [debug] [Thread-1  ]: Timing info for model.dbsql_dbt_tpch.AccountIncremental (compile): 15:33:18.005600 => 15:33:18.031955
[0m15:33:18.032281 [debug] [Thread-8  ]: Began compiling node model.dbsql_dbt_tpch.DimBroker
[0m15:33:18.032548 [debug] [Thread-9  ]: Began compiling node model.dbsql_dbt_tpch.FinWire
[0m15:33:18.032783 [debug] [Thread-10 ]: Began compiling node model.dbsql_dbt_tpch.HoldingIncremental
[0m15:33:18.033011 [debug] [Thread-11 ]: Began compiling node model.dbsql_dbt_tpch.ProspectRaw
[0m15:33:18.033268 [debug] [Thread-2  ]: Timing info for model.dbsql_dbt_tpch.BatchDate (compile): 15:33:18.011275 => 15:33:18.033155
[0m15:33:18.033461 [debug] [Thread-12 ]: Began compiling node model.dbsql_dbt_tpch.TradeIncremental
[0m15:33:18.033736 [debug] [Thread-13 ]: Began compiling node model.dbsql_dbt_tpch.WatchIncremental
[0m15:33:18.036110 [debug] [Thread-6  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.DailyMarketHistorical"
[0m15:33:18.038538 [debug] [Thread-7  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.DailyMarketIncremental"
[0m15:33:18.038876 [debug] [Thread-1  ]: Began executing node model.dbsql_dbt_tpch.AccountIncremental
[0m15:33:18.039163 [debug] [Thread-3  ]: Timing info for model.dbsql_dbt_tpch.CashTransactionIncremental (compile): 15:33:18.015784 => 15:33:18.039043
[0m15:33:18.041758 [debug] [Thread-8  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.DimBroker"
[0m15:33:18.042091 [debug] [Thread-4  ]: Timing info for model.dbsql_dbt_tpch.CustomerIncremental (compile): 15:33:18.024706 => 15:33:18.041958
[0m15:33:18.045068 [debug] [Thread-9  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.FinWire"
[0m15:33:18.047331 [debug] [Thread-10 ]: Writing injected SQL for node "model.dbsql_dbt_tpch.HoldingIncremental"
[0m15:33:18.047587 [debug] [Thread-5  ]: Timing info for model.dbsql_dbt_tpch.CustomerMgmtView (compile): 15:33:18.029196 => 15:33:18.047479
[0m15:33:18.049850 [debug] [Thread-11 ]: Writing injected SQL for node "model.dbsql_dbt_tpch.ProspectRaw"
[0m15:33:18.050065 [debug] [Thread-2  ]: Began executing node model.dbsql_dbt_tpch.BatchDate
[0m15:33:18.052202 [debug] [Thread-12 ]: Writing injected SQL for node "model.dbsql_dbt_tpch.TradeIncremental"
[0m15:33:18.054294 [debug] [Thread-13 ]: Writing injected SQL for node "model.dbsql_dbt_tpch.WatchIncremental"
[0m15:33:18.070634 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.AccountIncremental"
[0m15:33:18.070992 [debug] [Thread-6  ]: Timing info for model.dbsql_dbt_tpch.DailyMarketHistorical (compile): 15:33:18.033873 => 15:33:18.070864
[0m15:33:18.071200 [debug] [Thread-3  ]: Began executing node model.dbsql_dbt_tpch.CashTransactionIncremental
[0m15:33:18.071429 [debug] [Thread-7  ]: Timing info for model.dbsql_dbt_tpch.DailyMarketIncremental (compile): 15:33:18.036322 => 15:33:18.071327
[0m15:33:18.071676 [debug] [Thread-4  ]: Began executing node model.dbsql_dbt_tpch.CustomerIncremental
[0m15:33:18.071943 [debug] [Thread-5  ]: Began executing node model.dbsql_dbt_tpch.CustomerMgmtView
[0m15:33:18.072306 [debug] [Thread-10 ]: Timing info for model.dbsql_dbt_tpch.HoldingIncremental (compile): 15:33:18.045248 => 15:33:18.072176
[0m15:33:18.074552 [debug] [Thread-2  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.BatchDate"
[0m15:33:18.074826 [debug] [Thread-8  ]: Timing info for model.dbsql_dbt_tpch.DimBroker (compile): 15:33:18.039288 => 15:33:18.074700
[0m15:33:18.075156 [debug] [Thread-9  ]: Timing info for model.dbsql_dbt_tpch.FinWire (compile): 15:33:18.042296 => 15:33:18.075055
[0m15:33:18.075486 [debug] [Thread-6  ]: Began executing node model.dbsql_dbt_tpch.DailyMarketHistorical
[0m15:33:18.075715 [debug] [Thread-11 ]: Timing info for model.dbsql_dbt_tpch.ProspectRaw (compile): 15:33:18.047717 => 15:33:18.075616
[0m15:33:18.077793 [debug] [Thread-3  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.CashTransactionIncremental"
[0m15:33:18.078046 [debug] [Thread-7  ]: Began executing node model.dbsql_dbt_tpch.DailyMarketIncremental
[0m15:33:18.078284 [debug] [Thread-13 ]: Timing info for model.dbsql_dbt_tpch.WatchIncremental (compile): 15:33:18.052375 => 15:33:18.078176
[0m15:33:18.078497 [debug] [Thread-12 ]: Timing info for model.dbsql_dbt_tpch.TradeIncremental (compile): 15:33:18.050202 => 15:33:18.078401
[0m15:33:18.078711 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:33:18.080557 [debug] [Thread-4  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.CustomerIncremental"
[0m15:33:18.082515 [debug] [Thread-5  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.CustomerMgmtView"
[0m15:33:18.082715 [debug] [Thread-10 ]: Began executing node model.dbsql_dbt_tpch.HoldingIncremental
[0m15:33:18.082927 [debug] [Thread-8  ]: Began executing node model.dbsql_dbt_tpch.DimBroker
[0m15:33:18.083107 [debug] [Thread-9  ]: Began executing node model.dbsql_dbt_tpch.FinWire
[0m15:33:18.085453 [debug] [Thread-6  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.DailyMarketHistorical"
[0m15:33:18.085672 [debug] [Thread-11 ]: Began executing node model.dbsql_dbt_tpch.ProspectRaw
[0m15:33:18.085873 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:33:18.087809 [debug] [Thread-7  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.DailyMarketIncremental"
[0m15:33:18.087998 [debug] [Thread-13 ]: Began executing node model.dbsql_dbt_tpch.WatchIncremental
[0m15:33:18.088202 [debug] [Thread-12 ]: Began executing node model.dbsql_dbt_tpch.TradeIncremental
[0m15:33:18.088348 [debug] [Thread-1  ]: Using databricks connection "model.dbsql_dbt_tpch.AccountIncremental"
[0m15:33:18.088547 [debug] [Thread-3  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:33:18.090552 [debug] [Thread-10 ]: Writing runtime sql for node "model.dbsql_dbt_tpch.HoldingIncremental"
[0m15:33:18.111667 [debug] [Thread-9  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.FinWire"
[0m15:33:18.114579 [debug] [Thread-8  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.DimBroker"
[0m15:33:18.114911 [debug] [Thread-5  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:33:18.116875 [debug] [Thread-11 ]: Writing runtime sql for node "model.dbsql_dbt_tpch.ProspectRaw"
[0m15:33:18.117069 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:33:18.117221 [debug] [Thread-2  ]: Using databricks connection "model.dbsql_dbt_tpch.BatchDate"
[0m15:33:18.119187 [debug] [Thread-13 ]: Writing runtime sql for node "model.dbsql_dbt_tpch.WatchIncremental"
[0m15:33:18.119389 [debug] [Thread-6  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:33:18.121271 [debug] [Thread-12 ]: Writing runtime sql for node "model.dbsql_dbt_tpch.TradeIncremental"
[0m15:33:18.121484 [debug] [Thread-1  ]: On model.dbsql_dbt_tpch.AccountIncremental: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.AccountIncremental"} */
create or replace view `main`.`tpcdi`.`AccountIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`AccountIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`AccountIncrementaltres`

[0m15:33:18.121703 [debug] [Thread-7  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:33:18.121839 [debug] [Thread-3  ]: Using databricks connection "model.dbsql_dbt_tpch.CashTransactionIncremental"
[0m15:33:18.122093 [debug] [Thread-5  ]: Using databricks connection "model.dbsql_dbt_tpch.CustomerMgmtView"
[0m15:33:18.122319 [debug] [Thread-4  ]: Using databricks connection "model.dbsql_dbt_tpch.CustomerIncremental"
[0m15:33:18.122577 [debug] [Thread-10 ]: Spark adapter: NotImplemented: add_begin_query
[0m15:33:18.122767 [debug] [Thread-9  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:33:18.122952 [debug] [Thread-11 ]: Spark adapter: NotImplemented: add_begin_query
[0m15:33:18.123124 [debug] [Thread-8  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:33:18.123271 [debug] [Thread-2  ]: On model.dbsql_dbt_tpch.BatchDate: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.BatchDate"} */
create or replace view `main`.`tpcdi`.`BatchDate`
  
  
  as
    

select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`BatchDateuno`

 UNION ALL

select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`BatchDatedos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`BatchDatetres`

[0m15:33:18.123459 [debug] [Thread-6  ]: Using databricks connection "model.dbsql_dbt_tpch.DailyMarketHistorical"
[0m15:33:18.123691 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:33:18.123886 [debug] [Thread-13 ]: Spark adapter: NotImplemented: add_begin_query
[0m15:33:18.124032 [debug] [Thread-7  ]: Using databricks connection "model.dbsql_dbt_tpch.DailyMarketIncremental"
[0m15:33:18.124188 [debug] [Thread-3  ]: On model.dbsql_dbt_tpch.CashTransactionIncremental: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CashTransactionIncremental"} */
create or replace view `main`.`tpcdi`.`CashTransactionIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`CashTransactionIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`CashTransactionIncrementaltres`

[0m15:33:18.124397 [debug] [Thread-12 ]: Spark adapter: NotImplemented: add_begin_query
[0m15:33:18.124537 [debug] [Thread-5  ]: On model.dbsql_dbt_tpch.CustomerMgmtView: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CustomerMgmtView"} */
create or replace view `main`.`tpcdi`.`CustomerMgmtView`
  
  
  as
    
select
    *
from
    hive_metastore.roberto_salcido_tpcdi_stage.customermgmt10

[0m15:33:18.124700 [debug] [Thread-4  ]: On model.dbsql_dbt_tpch.CustomerIncremental: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CustomerIncremental"} */
create or replace view `main`.`tpcdi`.`CustomerIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`customerincrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`customerincrementaltres`

[0m15:33:18.124840 [debug] [Thread-10 ]: Using databricks connection "model.dbsql_dbt_tpch.HoldingIncremental"
[0m15:33:18.124965 [debug] [Thread-9  ]: Using databricks connection "model.dbsql_dbt_tpch.FinWire"
[0m15:33:18.125087 [debug] [Thread-11 ]: Using databricks connection "model.dbsql_dbt_tpch.ProspectRaw"
[0m15:33:18.125207 [debug] [Thread-8  ]: Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
[0m15:33:18.125359 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m15:33:18.125503 [debug] [Thread-6  ]: On model.dbsql_dbt_tpch.DailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DailyMarketHistorical"} */
create or replace view `main`.`tpcdi`.`DailyMarketHistorical`
  
  
  as
    
select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`DailyMarketHistorical`

[0m15:33:18.125722 [debug] [Thread-13 ]: Using databricks connection "model.dbsql_dbt_tpch.WatchIncremental"
[0m15:33:18.125883 [debug] [Thread-7  ]: On model.dbsql_dbt_tpch.DailyMarketIncremental: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DailyMarketIncremental"} */
create or replace view `main`.`tpcdi`.`DailyMarketIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`DailyMarketIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`DailyMarketIncrementaltres`

[0m15:33:18.141751 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m15:33:18.142023 [debug] [Thread-12 ]: Using databricks connection "model.dbsql_dbt_tpch.TradeIncremental"
[0m15:33:18.142200 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m15:33:18.142631 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m15:33:18.142798 [debug] [Thread-10 ]: On model.dbsql_dbt_tpch.HoldingIncremental: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.HoldingIncremental"} */
create or replace view `main`.`tpcdi`.`HoldingIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`HoldingIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`HoldingIncrementaltres`

[0m15:33:18.142957 [debug] [Thread-9  ]: On model.dbsql_dbt_tpch.FinWire: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.FinWire"} */
create or replace view `main`.`tpcdi`.`FinWire`
  
  
  as
    

select *, substring(value, 16, 3) rectype from 
text.`dbfs:/tmp/tpcdi/sf=10/Batch1/FINWIRE*`

[0m15:33:18.143124 [debug] [Thread-11 ]: On model.dbsql_dbt_tpch.ProspectRaw: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.ProspectRaw"} */
create or replace view `main`.`tpcdi`.`ProspectRaw`
  
  
  as
    

select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`ProspectRawuno`

 UNION ALL

select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`ProspectRawdos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`ProspectRawtres`

[0m15:33:18.143301 [debug] [Thread-8  ]: On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

  
    
        create or replace table `main`.`tpcdi`.`DimBroker`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT

  cast(employeeid as BIGINT) brokerid,
  cast(managerid as BIGINT) managerid,
  employeefirstname firstname,
  employeelastname lastname,
  employeemi middleinitial,
  employeebranch branch,
  employeeoffice office,
  employeephone phone,
  true iscurrent,
  1 batchid,
  (SELECT min(to_date(datevalue)) as effectivedate FROM `main`.`tpcdi`.`DimDate`) effectivedate,
  date('9999-12-31') enddate,
  bigint(concat(date_format(enddate, 'yyyyMMdd'), cast(brokerid as string))) as sk_brokerid
FROM  `main`.`tpcdi`.`HR`
WHERE employeejobcode = 314
  
[0m15:33:18.143582 [debug] [Thread-6  ]: Opening a new connection, currently in state init
[0m15:33:18.143744 [debug] [Thread-13 ]: On model.dbsql_dbt_tpch.WatchIncremental: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.WatchIncremental"} */
create or replace view `main`.`tpcdi`.`WatchIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`WatchIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`WatchIncrementaltres`

[0m15:33:18.159239 [debug] [Thread-7  ]: Opening a new connection, currently in state init
[0m15:33:18.175367 [debug] [Thread-12 ]: On model.dbsql_dbt_tpch.TradeIncremental: /* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.TradeIncremental"} */
create or replace view `main`.`tpcdi`.`TradeIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`TradeIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`TradeIncrementaltres`

[0m15:33:18.206765 [debug] [Thread-10 ]: Opening a new connection, currently in state init
[0m15:33:18.207545 [debug] [Thread-9  ]: Opening a new connection, currently in state init
[0m15:33:18.207755 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m15:33:18.207920 [debug] [Thread-8  ]: Opening a new connection, currently in state init
[0m15:33:18.208192 [debug] [Thread-13 ]: Opening a new connection, currently in state init
[0m15:33:18.224087 [debug] [Thread-12 ]: Opening a new connection, currently in state init
[0m15:33:18.650889 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.AccountIncremental"} */
create or replace view `main`.`tpcdi`.`AccountIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`AccountIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`AccountIncrementaltres`

[0m15:33:18.651401 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`AccountIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:33:18.652520 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`AccountIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`AccountIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:33:18.652956 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe0\x86d\x13\xb4\xba\xbd*\xd7C\x9d\xb6\xdc'
[0m15:33:18.653449 [debug] [Thread-1  ]: Timing info for model.dbsql_dbt_tpch.AccountIncremental (execute): 15:33:18.054538 => 15:33:18.653293
[0m15:33:18.653745 [debug] [Thread-1  ]: On model.dbsql_dbt_tpch.AccountIncremental: ROLLBACK
[0m15:33:18.653987 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m15:33:18.654255 [debug] [Thread-1  ]: On model.dbsql_dbt_tpch.AccountIncremental: Close
[0m15:33:18.665758 [debug] [Thread-2  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.BatchDate"} */
create or replace view `main`.`tpcdi`.`BatchDate`
  
  
  as
    

select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`BatchDateuno`

 UNION ALL

select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`BatchDatedos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`BatchDatetres`

[0m15:33:18.666058 [debug] [Thread-2  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`BatchDateuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:33:18.667197 [debug] [Thread-2  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`BatchDateuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`BatchDateuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:33:18.667608 [debug] [Thread-2  ]: Databricks adapter: operation-id: b"\x01\xee\x0e\xe0\x86f\x1c\xac\xbc\x11\x05'>o\xb1\xc8"
[0m15:33:18.667969 [debug] [Thread-2  ]: Timing info for model.dbsql_dbt_tpch.BatchDate (execute): 15:33:18.072445 => 15:33:18.667837
[0m15:33:18.668199 [debug] [Thread-2  ]: On model.dbsql_dbt_tpch.BatchDate: ROLLBACK
[0m15:33:18.668417 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m15:33:18.668626 [debug] [Thread-2  ]: On model.dbsql_dbt_tpch.BatchDate: Close
[0m15:33:18.720664 [debug] [Thread-4  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CustomerIncremental"} */
create or replace view `main`.`tpcdi`.`CustomerIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`customerincrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`customerincrementaltres`

[0m15:33:18.721940 [debug] [Thread-4  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`customerincrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:33:18.722477 [debug] [Thread-4  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`customerincrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`customerincrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:33:18.722970 [debug] [Thread-4  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe0\x86o\x15|\x9f\x9a\xd1P\x96\xea!\xf8'
[0m15:33:18.723429 [debug] [Thread-4  ]: Timing info for model.dbsql_dbt_tpch.CustomerIncremental (execute): 15:33:18.078829 => 15:33:18.723257
[0m15:33:18.723763 [debug] [Thread-4  ]: On model.dbsql_dbt_tpch.CustomerIncremental: ROLLBACK
[0m15:33:18.724202 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m15:33:18.724526 [debug] [Thread-4  ]: On model.dbsql_dbt_tpch.CustomerIncremental: Close
[0m15:33:18.728584 [debug] [Thread-1  ]: Runtime Error in model AccountIncremental (models/base/AccountIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`AccountIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:33:18.729014 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '19724bb1-5a2d-4e80-8ab9-bb94f1ce71cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13aa66d90>]}
[0m15:33:18.729546 [error] [Thread-1  ]: 1 of 45 ERROR creating sql view model tpcdi.AccountIncremental ................. [[31mERROR[0m in 0.74s]
[0m15:33:18.730044 [debug] [Thread-1  ]: Finished running node model.dbsql_dbt_tpch.AccountIncremental
[0m15:33:18.736835 [debug] [Thread-3  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CashTransactionIncremental"} */
create or replace view `main`.`tpcdi`.`CashTransactionIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`CashTransactionIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`CashTransactionIncrementaltres`

[0m15:33:18.737381 [debug] [Thread-3  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`CashTransactionIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:33:18.737866 [debug] [Thread-3  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`CashTransactionIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`CashTransactionIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:33:18.738313 [debug] [Thread-3  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe0\x86p\x15P\xb0j\xc8 &rVU'
[0m15:33:18.738736 [debug] [Thread-3  ]: Timing info for model.dbsql_dbt_tpch.CashTransactionIncremental (execute): 15:33:18.075879 => 15:33:18.738577
[0m15:33:18.739023 [debug] [Thread-3  ]: On model.dbsql_dbt_tpch.CashTransactionIncremental: ROLLBACK
[0m15:33:18.739275 [debug] [Thread-3  ]: Databricks adapter: NotImplemented: rollback
[0m15:33:18.739585 [debug] [Thread-3  ]: On model.dbsql_dbt_tpch.CashTransactionIncremental: Close
[0m15:33:18.740932 [debug] [Thread-2  ]: Runtime Error in model BatchDate (models/base/BatchDate.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`BatchDateuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:33:18.741342 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '19724bb1-5a2d-4e80-8ab9-bb94f1ce71cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13aa5a6a0>]}
[0m15:33:18.741838 [error] [Thread-2  ]: 2 of 45 ERROR creating sql view model tpcdi.BatchDate .......................... [[31mERROR[0m in 0.74s]
[0m15:33:18.742299 [debug] [Thread-2  ]: Finished running node model.dbsql_dbt_tpch.BatchDate
[0m15:33:18.780048 [debug] [Thread-5  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CustomerMgmtView"} */
create or replace view `main`.`tpcdi`.`CustomerMgmtView`
  
  
  as
    
select
    *
from
    hive_metastore.roberto_salcido_tpcdi_stage.customermgmt10

[0m15:33:18.780503 [debug] [Thread-5  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`roberto_salcido_tpcdi_stage`.`customermgmt10` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 10 pos 4
[0m15:33:18.781765 [debug] [Thread-7  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DailyMarketIncremental"} */
create or replace view `main`.`tpcdi`.`DailyMarketIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`DailyMarketIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`DailyMarketIncrementaltres`

[0m15:33:18.782256 [debug] [Thread-5  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`roberto_salcido_tpcdi_stage`.`customermgmt10` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 10 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`roberto_salcido_tpcdi_stage`.`customermgmt10` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 10 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:33:18.782714 [debug] [Thread-7  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:33:18.782987 [debug] [Thread-5  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe0\x86p\x12\x80\x82\xdc\x02[R\x18o#'
[0m15:33:18.783369 [debug] [Thread-7  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:33:18.783910 [debug] [Thread-5  ]: Timing info for model.dbsql_dbt_tpch.CustomerMgmtView (execute): 15:33:18.080725 => 15:33:18.783765
[0m15:33:18.784152 [debug] [Thread-7  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe0\x86x\x172\xab\x8e\x8eI\x94\xb9\xb3\x9e'
[0m15:33:18.784404 [debug] [Thread-5  ]: On model.dbsql_dbt_tpch.CustomerMgmtView: ROLLBACK
[0m15:33:18.784749 [debug] [Thread-7  ]: Timing info for model.dbsql_dbt_tpch.DailyMarketIncremental (execute): 15:33:18.086014 => 15:33:18.784626
[0m15:33:18.784976 [debug] [Thread-5  ]: Databricks adapter: NotImplemented: rollback
[0m15:33:18.785200 [debug] [Thread-7  ]: On model.dbsql_dbt_tpch.DailyMarketIncremental: ROLLBACK
[0m15:33:18.785423 [debug] [Thread-5  ]: On model.dbsql_dbt_tpch.CustomerMgmtView: Close
[0m15:33:18.785645 [debug] [Thread-7  ]: Databricks adapter: NotImplemented: rollback
[0m15:33:18.786786 [debug] [Thread-7  ]: On model.dbsql_dbt_tpch.DailyMarketIncremental: Close
[0m15:33:18.787967 [debug] [Thread-4  ]: Runtime Error in model CustomerIncremental (models/base/CustomerIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`customerincrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:33:18.788301 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '19724bb1-5a2d-4e80-8ab9-bb94f1ce71cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13aa81670>]}
[0m15:33:18.788714 [error] [Thread-4  ]: 4 of 45 ERROR creating sql view model tpcdi.CustomerIncremental ................ [[31mERROR[0m in 0.79s]
[0m15:33:18.789098 [debug] [Thread-4  ]: Finished running node model.dbsql_dbt_tpch.CustomerIncremental
[0m15:33:18.796680 [debug] [Thread-10 ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.HoldingIncremental"} */
create or replace view `main`.`tpcdi`.`HoldingIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`HoldingIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`HoldingIncrementaltres`

[0m15:33:18.797037 [debug] [Thread-10 ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HoldingIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:33:18.797397 [debug] [Thread-10 ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HoldingIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HoldingIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:33:18.797738 [debug] [Thread-10 ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe0\x86z\x11\x01\xb8\xa9>\xdf*M\xe3\xa9'
[0m15:33:18.798067 [debug] [Thread-10 ]: Timing info for model.dbsql_dbt_tpch.HoldingIncremental (execute): 15:33:18.088744 => 15:33:18.797942
[0m15:33:18.798289 [debug] [Thread-10 ]: On model.dbsql_dbt_tpch.HoldingIncremental: ROLLBACK
[0m15:33:18.798489 [debug] [Thread-10 ]: Databricks adapter: NotImplemented: rollback
[0m15:33:18.798679 [debug] [Thread-10 ]: On model.dbsql_dbt_tpch.HoldingIncremental: Close
[0m15:33:18.805061 [debug] [Thread-3  ]: Runtime Error in model CashTransactionIncremental (models/base/CashTransactionIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`CashTransactionIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:33:18.805423 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '19724bb1-5a2d-4e80-8ab9-bb94f1ce71cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13acdac70>]}
[0m15:33:18.805806 [error] [Thread-3  ]: 3 of 45 ERROR creating sql view model tpcdi.CashTransactionIncremental ......... [[31mERROR[0m in 0.81s]
[0m15:33:18.806157 [debug] [Thread-3  ]: Finished running node model.dbsql_dbt_tpch.CashTransactionIncremental
[0m15:33:18.831223 [debug] [Thread-13 ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.WatchIncremental"} */
create or replace view `main`.`tpcdi`.`WatchIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`WatchIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`WatchIncrementaltres`

[0m15:33:18.831622 [debug] [Thread-13 ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`WatchIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:33:18.831998 [debug] [Thread-13 ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`WatchIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`WatchIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:33:18.832343 [debug] [Thread-13 ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe0\x86\x7f\x1d\x0e\x87#\x8dh\xea;\xd5M'
[0m15:33:18.832691 [debug] [Thread-13 ]: Timing info for model.dbsql_dbt_tpch.WatchIncremental (execute): 15:33:18.117397 => 15:33:18.832560
[0m15:33:18.832918 [debug] [Thread-13 ]: On model.dbsql_dbt_tpch.WatchIncremental: ROLLBACK
[0m15:33:18.833121 [debug] [Thread-13 ]: Databricks adapter: NotImplemented: rollback
[0m15:33:18.833318 [debug] [Thread-13 ]: On model.dbsql_dbt_tpch.WatchIncremental: Close
[0m15:33:18.842467 [debug] [Thread-6  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DailyMarketHistorical"} */
create or replace view `main`.`tpcdi`.`DailyMarketHistorical`
  
  
  as
    
select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`DailyMarketHistorical`

[0m15:33:18.842862 [debug] [Thread-6  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketHistorical` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:33:18.843317 [debug] [Thread-6  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketHistorical` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketHistorical` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:33:18.843719 [debug] [Thread-6  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe0\x86~\x1c\xde\xbfU\xd0\xda\xfa\xf9\xe9\x93'
[0m15:33:18.844102 [debug] [Thread-6  ]: Timing info for model.dbsql_dbt_tpch.DailyMarketHistorical (execute): 15:33:18.083247 => 15:33:18.843965
[0m15:33:18.844461 [debug] [Thread-6  ]: On model.dbsql_dbt_tpch.DailyMarketHistorical: ROLLBACK
[0m15:33:18.844792 [debug] [Thread-6  ]: Databricks adapter: NotImplemented: rollback
[0m15:33:18.845016 [debug] [Thread-6  ]: On model.dbsql_dbt_tpch.DailyMarketHistorical: Close
[0m15:33:18.846216 [debug] [Thread-11 ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.ProspectRaw"} */
create or replace view `main`.`tpcdi`.`ProspectRaw`
  
  
  as
    

select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`ProspectRawuno`

 UNION ALL

select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`ProspectRawdos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`ProspectRawtres`

[0m15:33:18.846683 [debug] [Thread-11 ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`ProspectRawuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:33:18.847165 [debug] [Thread-11 ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`ProspectRawuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`ProspectRawuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:33:18.847582 [debug] [Thread-11 ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe0\x86~\x1b\xb7\x85\x02/.\x9b\x9f\xde\xdd'
[0m15:33:18.847982 [debug] [Thread-11 ]: Timing info for model.dbsql_dbt_tpch.ProspectRaw (execute): 15:33:18.115055 => 15:33:18.847857
[0m15:33:18.848204 [debug] [Thread-11 ]: On model.dbsql_dbt_tpch.ProspectRaw: ROLLBACK
[0m15:33:18.848394 [debug] [Thread-11 ]: Databricks adapter: NotImplemented: rollback
[0m15:33:18.848573 [debug] [Thread-11 ]: On model.dbsql_dbt_tpch.ProspectRaw: Close
[0m15:33:18.857763 [debug] [Thread-5  ]: Runtime Error in model CustomerMgmtView (models/base/CustomerMgmtView.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`roberto_salcido_tpcdi_stage`.`customermgmt10` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 10 pos 4
[0m15:33:18.858582 [debug] [Thread-7  ]: Runtime Error in model DailyMarketIncremental (models/base/DailyMarketIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:33:18.859089 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '19724bb1-5a2d-4e80-8ab9-bb94f1ce71cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13aaa32e0>]}
[0m15:33:18.859414 [debug] [Thread-7  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '19724bb1-5a2d-4e80-8ab9-bb94f1ce71cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13f900730>]}
[0m15:33:18.860670 [error] [Thread-5  ]: 5 of 45 ERROR creating sql view model tpcdi.CustomerMgmtView ................... [[31mERROR[0m in 0.86s]
[0m15:33:18.861327 [error] [Thread-7  ]: 7 of 45 ERROR creating sql view model tpcdi.DailyMarketIncremental ............. [[31mERROR[0m in 0.85s]
[0m15:33:18.861803 [debug] [Thread-5  ]: Finished running node model.dbsql_dbt_tpch.CustomerMgmtView
[0m15:33:18.862122 [debug] [Thread-7  ]: Finished running node model.dbsql_dbt_tpch.DailyMarketIncremental
[0m15:33:18.863168 [debug] [Thread-10 ]: Runtime Error in model HoldingIncremental (models/base/HoldingIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HoldingIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:33:18.863621 [debug] [Thread-10 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '19724bb1-5a2d-4e80-8ab9-bb94f1ce71cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13acda970>]}
[0m15:33:18.863925 [debug] [Thread-15 ]: Began running node model.dbsql_dbt_tpch.DimCustomerStg
[0m15:33:18.864272 [error] [Thread-10 ]: 10 of 45 ERROR creating sql view model tpcdi.HoldingIncremental ................ [[31mERROR[0m in 0.85s]
[0m15:33:18.864594 [info ] [Thread-15 ]: 14 of 45 SKIP relation tpcdi.DimCustomerStg .................................... [[33mSKIP[0m]
[0m15:33:18.864945 [debug] [Thread-10 ]: Finished running node model.dbsql_dbt_tpch.HoldingIncremental
[0m15:33:18.865183 [debug] [Thread-15 ]: Finished running node model.dbsql_dbt_tpch.DimCustomerStg
[0m15:33:18.865543 [debug] [Thread-17 ]: Began running node test.dbsql_dbt_tpch.dateval_DimCustomerStg_effectivedate.8a3a23ae4e
[0m15:33:18.865762 [info ] [Thread-17 ]: 15 of 45 SKIP test dateval_DimCustomerStg_effectivedate ........................ [[33mSKIP[0m]
[0m15:33:18.866013 [debug] [Thread-17 ]: Finished running node test.dbsql_dbt_tpch.dateval_DimCustomerStg_effectivedate.8a3a23ae4e
[0m15:33:18.880864 [debug] [Thread-12 ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.TradeIncremental"} */
create or replace view `main`.`tpcdi`.`TradeIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`TradeIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`TradeIncrementaltres`

[0m15:33:18.881207 [debug] [Thread-12 ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`TradeIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:33:18.881533 [debug] [Thread-12 ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`TradeIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`TradeIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:33:18.881835 [debug] [Thread-12 ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe0\x86\x86\x1c\xe4\xa8\x9fN\x1cK \xf5\xaf'
[0m15:33:18.882123 [debug] [Thread-12 ]: Timing info for model.dbsql_dbt_tpch.TradeIncremental (execute): 15:33:18.119524 => 15:33:18.882016
[0m15:33:18.882311 [debug] [Thread-12 ]: On model.dbsql_dbt_tpch.TradeIncremental: ROLLBACK
[0m15:33:18.882478 [debug] [Thread-12 ]: Databricks adapter: NotImplemented: rollback
[0m15:33:18.882634 [debug] [Thread-12 ]: On model.dbsql_dbt_tpch.TradeIncremental: Close
[0m15:33:18.889622 [debug] [Thread-8  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

  
    
        create or replace table `main`.`tpcdi`.`DimBroker`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT

  cast(employeeid as BIGINT) brokerid,
  cast(managerid as BIGINT) managerid,
  employeefirstname firstname,
  employeelastname lastname,
  employeemi middleinitial,
  employeebranch branch,
  employeeoffice office,
  employeephone phone,
  true iscurrent,
  1 batchid,
  (SELECT min(to_date(datevalue)) as effectivedate FROM `main`.`tpcdi`.`DimDate`) effectivedate,
  date('9999-12-31') enddate,
  bigint(concat(date_format(enddate, 'yyyyMMdd'), cast(brokerid as string))) as sk_brokerid
FROM  `main`.`tpcdi`.`HR`
WHERE employeejobcode = 314
  
[0m15:33:18.890478 [debug] [Thread-8  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HR` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 33 pos 6
[0m15:33:18.890826 [debug] [Thread-8  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HR` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 33 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HR` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 33 pos 6
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:33:18.891149 [debug] [Thread-8  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe0\x86\x82\x1cf\x82\xe6\x7fa\xcb$O\x98'
[0m15:33:18.891446 [debug] [Thread-8  ]: Timing info for model.dbsql_dbt_tpch.DimBroker (execute): 15:33:18.090693 => 15:33:18.891338
[0m15:33:18.891649 [debug] [Thread-8  ]: On model.dbsql_dbt_tpch.DimBroker: ROLLBACK
[0m15:33:18.891824 [debug] [Thread-8  ]: Databricks adapter: NotImplemented: rollback
[0m15:33:18.892037 [debug] [Thread-8  ]: On model.dbsql_dbt_tpch.DimBroker: Close
[0m15:33:18.893101 [debug] [Thread-13 ]: Runtime Error in model WatchIncremental (models/base/WatchIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`WatchIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:33:18.893452 [debug] [Thread-13 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '19724bb1-5a2d-4e80-8ab9-bb94f1ce71cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13aa811c0>]}
[0m15:33:18.893866 [error] [Thread-13 ]: 13 of 45 ERROR creating sql view model tpcdi.WatchIncremental .................. [[31mERROR[0m in 0.87s]
[0m15:33:18.894247 [debug] [Thread-13 ]: Finished running node model.dbsql_dbt_tpch.WatchIncremental
[0m15:33:18.909301 [debug] [Thread-11 ]: Runtime Error in model ProspectRaw (models/base/ProspectRaw.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`ProspectRawuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:33:18.909807 [debug] [Thread-11 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '19724bb1-5a2d-4e80-8ab9-bb94f1ce71cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13acdaac0>]}
[0m15:33:18.910217 [error] [Thread-11 ]: 11 of 45 ERROR creating sql view model tpcdi.ProspectRaw ....................... [[31mERROR[0m in 0.89s]
[0m15:33:18.910550 [debug] [Thread-11 ]: Finished running node model.dbsql_dbt_tpch.ProspectRaw
[0m15:33:18.911011 [debug] [Thread-19 ]: Began running node model.dbsql_dbt_tpch.Prospect
[0m15:33:18.911319 [info ] [Thread-19 ]: 16 of 45 SKIP relation tpcdi.Prospect .......................................... [[33mSKIP[0m]
[0m15:33:18.911626 [debug] [Thread-19 ]: Finished running node model.dbsql_dbt_tpch.Prospect
[0m15:33:18.911953 [debug] [Thread-21 ]: Began running node model.dbsql_dbt_tpch.DimCustomer
[0m15:33:18.912172 [info ] [Thread-21 ]: 17 of 45 SKIP relation tpcdi.DimCustomer ....................................... [[33mSKIP[0m]
[0m15:33:18.912411 [debug] [Thread-21 ]: Finished running node model.dbsql_dbt_tpch.DimCustomer
[0m15:33:18.912711 [debug] [Thread-23 ]: Began running node test.dbsql_dbt_tpch.accepted_values_DimCustomer_tier__1__2__3.7be85b3b7a
[0m15:33:18.912938 [debug] [Thread-24 ]: Began running node test.dbsql_dbt_tpch.not_null_DimCustomer_tier.e0964b5cac
[0m15:33:18.913168 [info ] [Thread-23 ]: 18 of 45 SKIP test accepted_values_DimCustomer_tier__1__2__3 ................... [[33mSKIP[0m]
[0m15:33:18.913535 [info ] [Thread-24 ]: 19 of 45 SKIP test not_null_DimCustomer_tier ................................... [[33mSKIP[0m]
[0m15:33:18.913900 [debug] [Thread-23 ]: Finished running node test.dbsql_dbt_tpch.accepted_values_DimCustomer_tier__1__2__3.7be85b3b7a
[0m15:33:18.914147 [debug] [Thread-24 ]: Finished running node test.dbsql_dbt_tpch.not_null_DimCustomer_tier.e0964b5cac
[0m15:33:18.921554 [debug] [Thread-6  ]: Runtime Error in model DailyMarketHistorical (models/base/DailyMarketHistorical.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketHistorical` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:33:18.921870 [debug] [Thread-6  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '19724bb1-5a2d-4e80-8ab9-bb94f1ce71cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13acfbfd0>]}
[0m15:33:18.922186 [error] [Thread-6  ]: 6 of 45 ERROR creating sql view model tpcdi.DailyMarketHistorical .............. [[31mERROR[0m in 0.91s]
[0m15:33:18.922461 [debug] [Thread-6  ]: Finished running node model.dbsql_dbt_tpch.DailyMarketHistorical
[0m15:33:18.922875 [debug] [Thread-14 ]: Began running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
[0m15:33:18.923194 [info ] [Thread-14 ]: 20 of 45 SKIP relation tpcdi.tempDailyMarketHistorical ......................... [[33mSKIP[0m]
[0m15:33:18.923476 [debug] [Thread-14 ]: Finished running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
[0m15:33:18.941262 [debug] [Thread-12 ]: Runtime Error in model TradeIncremental (models/base/TradeIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`TradeIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:33:18.941625 [debug] [Thread-12 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '19724bb1-5a2d-4e80-8ab9-bb94f1ce71cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13acfbb50>]}
[0m15:33:18.941959 [error] [Thread-12 ]: 12 of 45 ERROR creating sql view model tpcdi.TradeIncremental .................. [[31mERROR[0m in 0.92s]
[0m15:33:18.942335 [debug] [Thread-12 ]: Finished running node model.dbsql_dbt_tpch.TradeIncremental
[0m15:33:18.952781 [debug] [Thread-8  ]: Runtime Error in model DimBroker (models/silver/DimBroker.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HR` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 33 pos 6
[0m15:33:18.953148 [debug] [Thread-8  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '19724bb1-5a2d-4e80-8ab9-bb94f1ce71cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13fa26e80>]}
[0m15:33:18.953489 [error] [Thread-8  ]: 8 of 45 ERROR creating sql table model tpcdi.DimBroker ......................... [[31mERROR[0m in 0.94s]
[0m15:33:18.953782 [debug] [Thread-8  ]: Finished running node model.dbsql_dbt_tpch.DimBroker
[0m15:33:18.954145 [debug] [Thread-2  ]: Began running node model.dbsql_dbt_tpch.DimAccount
[0m15:33:18.954377 [info ] [Thread-2  ]: 21 of 45 SKIP relation tpcdi.DimAccount ........................................ [[33mSKIP[0m]
[0m15:33:18.954695 [debug] [Thread-2  ]: Finished running node model.dbsql_dbt_tpch.DimAccount
[0m15:33:18.955009 [debug] [Thread-3  ]: Began running node test.dbsql_dbt_tpch.not_null_DimAccount_sk_brokerid.dd6e992726
[0m15:33:18.955222 [debug] [Thread-5  ]: Began running node test.dbsql_dbt_tpch.not_null_DimAccount_sk_customerid.b8baa6ce87
[0m15:33:18.955419 [info ] [Thread-3  ]: 22 of 45 SKIP test not_null_DimAccount_sk_brokerid ............................. [[33mSKIP[0m]
[0m15:33:18.955738 [info ] [Thread-5  ]: 23 of 45 SKIP test not_null_DimAccount_sk_customerid ........................... [[33mSKIP[0m]
[0m15:33:18.956041 [debug] [Thread-3  ]: Finished running node test.dbsql_dbt_tpch.not_null_DimAccount_sk_brokerid.dd6e992726
[0m15:33:18.956270 [debug] [Thread-5  ]: Finished running node test.dbsql_dbt_tpch.not_null_DimAccount_sk_customerid.b8baa6ce87
[0m15:33:18.956610 [debug] [Thread-16 ]: Began running node model.dbsql_dbt_tpch.FactCashBalances
[0m15:33:18.956864 [info ] [Thread-16 ]: 24 of 45 SKIP relation tpcdi.FactCashBalances .................................. [[33mSKIP[0m]
[0m15:33:18.957227 [debug] [Thread-16 ]: Finished running node model.dbsql_dbt_tpch.FactCashBalances
[0m15:33:18.957602 [debug] [Thread-15 ]: Began running node test.dbsql_dbt_tpch.not_null_FactCashBalances_sk_accountid.878cf23033
[0m15:33:18.957816 [info ] [Thread-15 ]: 25 of 45 SKIP test not_null_FactCashBalances_sk_accountid ...................... [[33mSKIP[0m]
[0m15:33:18.958076 [debug] [Thread-15 ]: Finished running node test.dbsql_dbt_tpch.not_null_FactCashBalances_sk_accountid.878cf23033
[0m15:33:19.058980 [debug] [Thread-9  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.1", "dbt_databricks_version": "1.5.4", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.FinWire"} */
create or replace view `main`.`tpcdi`.`FinWire`
  
  
  as
    

select *, substring(value, 16, 3) rectype from 
text.`dbfs:/tmp/tpcdi/sf=10/Batch1/FINWIRE*`

[0m15:33:19.059440 [debug] [Thread-9  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: text; line 9 pos 0
[0m15:33:19.060332 [debug] [Thread-9  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: text; line 9 pos 0
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: text; line 9 pos 0
	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:76)
	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:171)
	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:93)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1306)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1305)
	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:81)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1335)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1332)
	at org.apache.spark.sql.catalyst.plans.logical.CreateView.mapChildren(v2Commands.scala:1350)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:79)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:78)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:93)
	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:51)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:229)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:229)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:226)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:218)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:296)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:296)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:296)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:383)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:376)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:283)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:376)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:304)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:379)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:355)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:156)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:348)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:380)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:817)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:380)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1040)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:377)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:150)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:140)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getRuntimeCategory$1(QueryRuntimePrediction.scala:300)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1040)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:299)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:349)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:345)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more
Caused by: org.apache.spark.sql.AnalysisException: [PATH_NOT_FOUND] Path does not exist: dbfs:/tmp/tpcdi/sf=10/Batch1/FINWIRE*.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:1592)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:844)
	at scala.collection.immutable.List.flatMap(List.scala:366)
	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:831)
	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:616)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:451)
	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:160)
	... 95 more

[0m15:33:19.061222 [debug] [Thread-9  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe0\x86}\x19\xca\x8f^\xd9\xfc9\xee|\xa5'
[0m15:33:19.061630 [debug] [Thread-9  ]: Timing info for model.dbsql_dbt_tpch.FinWire (execute): 15:33:18.109498 => 15:33:19.061484
[0m15:33:19.061886 [debug] [Thread-9  ]: On model.dbsql_dbt_tpch.FinWire: ROLLBACK
[0m15:33:19.062104 [debug] [Thread-9  ]: Databricks adapter: NotImplemented: rollback
[0m15:33:19.062308 [debug] [Thread-9  ]: On model.dbsql_dbt_tpch.FinWire: Close
[0m15:33:19.123859 [debug] [Thread-9  ]: Runtime Error in model FinWire (models/base/FinWire.sql)
  [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: text; line 9 pos 0
[0m15:33:19.124364 [debug] [Thread-9  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '19724bb1-5a2d-4e80-8ab9-bb94f1ce71cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13acdae50>]}
[0m15:33:19.124989 [error] [Thread-9  ]: 9 of 45 ERROR creating sql view model tpcdi.FinWire ............................ [[31mERROR[0m in 1.11s]
[0m15:33:19.125511 [debug] [Thread-9  ]: Finished running node model.dbsql_dbt_tpch.FinWire
[0m15:33:19.126382 [debug] [Thread-17 ]: Began running node model.dbsql_dbt_tpch.DimCompany
[0m15:33:19.126905 [info ] [Thread-17 ]: 26 of 45 SKIP relation tpcdi.DimCompany ........................................ [[33mSKIP[0m]
[0m15:33:19.127494 [debug] [Thread-17 ]: Finished running node model.dbsql_dbt_tpch.DimCompany
[0m15:33:19.128181 [debug] [Thread-11 ]: Began running node test.dbsql_dbt_tpch.dateval_DimCompany_effectivedate.4df1621a42
[0m15:33:19.128548 [info ] [Thread-11 ]: 27 of 45 SKIP test dateval_DimCompany_effectivedate ............................ [[33mSKIP[0m]
[0m15:33:19.128983 [debug] [Thread-11 ]: Finished running node test.dbsql_dbt_tpch.dateval_DimCompany_effectivedate.4df1621a42
[0m15:33:19.129517 [debug] [Thread-19 ]: Began running node model.dbsql_dbt_tpch.DimSecurity
[0m15:33:19.130144 [debug] [Thread-22 ]: Began running node model.dbsql_dbt_tpch.Financial
[0m15:33:19.130604 [info ] [Thread-19 ]: 28 of 45 SKIP relation tpcdi.DimSecurity ....................................... [[33mSKIP[0m]
[0m15:33:19.131530 [debug] [Thread-19 ]: Finished running node model.dbsql_dbt_tpch.DimSecurity
[0m15:33:19.131040 [info ] [Thread-22 ]: 29 of 45 SKIP relation tpcdi.Financial ......................................... [[33mSKIP[0m]
[0m15:33:19.132241 [debug] [Thread-22 ]: Finished running node model.dbsql_dbt_tpch.Financial
[0m15:33:19.132638 [debug] [Thread-22 ]: Began running node test.dbsql_dbt_tpch.not_null_DimSecurity_sk_companyid.e728df82fe
[0m15:33:19.133004 [info ] [Thread-22 ]: 30 of 45 SKIP test not_null_DimSecurity_sk_companyid ........................... [[33mSKIP[0m]
[0m15:33:19.133432 [debug] [Thread-22 ]: Finished running node test.dbsql_dbt_tpch.not_null_DimSecurity_sk_companyid.e728df82fe
[0m15:33:19.134325 [debug] [Thread-23 ]: Began running node test.dbsql_dbt_tpch.not_null_Financial_sk_companyid.529c7c2c12
[0m15:33:19.135025 [debug] [Thread-24 ]: Began running node model.dbsql_dbt_tpch.DimTrade
[0m15:33:19.135380 [debug] [Thread-6  ]: Began running node model.dbsql_dbt_tpch.FactWatches
[0m15:33:19.134696 [info ] [Thread-23 ]: 31 of 45 SKIP test not_null_Financial_sk_companyid ............................. [[33mSKIP[0m]
[0m15:33:19.135682 [info ] [Thread-24 ]: 32 of 45 SKIP relation tpcdi.DimTrade .......................................... [[33mSKIP[0m]
[0m15:33:19.136201 [info ] [Thread-6  ]: 33 of 45 SKIP relation tpcdi.FactWatches ....................................... [[33mSKIP[0m]
[0m15:33:19.136629 [debug] [Thread-23 ]: Finished running node test.dbsql_dbt_tpch.not_null_Financial_sk_companyid.529c7c2c12
[0m15:33:19.136935 [debug] [Thread-24 ]: Finished running node model.dbsql_dbt_tpch.DimTrade
[0m15:33:19.137216 [debug] [Thread-6  ]: Finished running node model.dbsql_dbt_tpch.FactWatches
[0m15:33:19.137904 [debug] [Thread-14 ]: Began running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
[0m15:33:19.138204 [debug] [Thread-12 ]: Began running node test.dbsql_dbt_tpch.not_null_DimTrade_sk_accountid.4db26dca2c
[0m15:33:19.138500 [info ] [Thread-14 ]: 34 of 45 SKIP relation tpcdi.tempSumpFiBasicEps ................................ [[33mSKIP[0m]
[0m15:33:19.138879 [debug] [Thread-8  ]: Began running node test.dbsql_dbt_tpch.not_null_DimTrade_sk_securityid.13bd8f9da6
[0m15:33:19.139179 [info ] [Thread-12 ]: 35 of 45 SKIP test not_null_DimTrade_sk_accountid .............................. [[33mSKIP[0m]
[0m15:33:19.139552 [debug] [Thread-14 ]: Finished running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
[0m15:33:19.139811 [debug] [Thread-4  ]: Began running node test.dbsql_dbt_tpch.tradecom_DimTrade_commission.ad38ac2d42
[0m15:33:19.140136 [info ] [Thread-8  ]: 36 of 45 SKIP test not_null_DimTrade_sk_securityid ............................. [[33mSKIP[0m]
[0m15:33:19.140504 [debug] [Thread-2  ]: Began running node test.dbsql_dbt_tpch.tradefee_DimTrade_fee.5ee05431ee
[0m15:33:19.140772 [debug] [Thread-12 ]: Finished running node test.dbsql_dbt_tpch.not_null_DimTrade_sk_accountid.4db26dca2c
[0m15:33:19.141024 [debug] [Thread-7  ]: Began running node test.dbsql_dbt_tpch.not_null_FactWatches_sk_customerid.1c7356b8c9
[0m15:33:19.141327 [debug] [Thread-14 ]: Began running node test.dbsql_dbt_tpch.not_null_FactWatches_sk_securityid.8a6b5e97b3
[0m15:33:19.141700 [info ] [Thread-4  ]: 37 of 45 SKIP test tradecom_DimTrade_commission ................................ [[33mSKIP[0m]
[0m15:33:19.142042 [debug] [Thread-5  ]: Began running node model.dbsql_dbt_tpch.FactMarketHistory
[0m15:33:19.142286 [debug] [Thread-8  ]: Finished running node test.dbsql_dbt_tpch.not_null_DimTrade_sk_securityid.13bd8f9da6
[0m15:33:19.142548 [info ] [Thread-2  ]: 38 of 45 SKIP test tradefee_DimTrade_fee ....................................... [[33mSKIP[0m]
[0m15:33:19.142972 [info ] [Thread-7  ]: 39 of 45 SKIP test not_null_FactWatches_sk_customerid .......................... [[33mSKIP[0m]
[0m15:33:19.143329 [info ] [Thread-14 ]: 40 of 45 SKIP test not_null_FactWatches_sk_securityid .......................... [[33mSKIP[0m]
[0m15:33:19.143673 [debug] [Thread-4  ]: Finished running node test.dbsql_dbt_tpch.tradecom_DimTrade_commission.ad38ac2d42
[0m15:33:19.143950 [info ] [Thread-5  ]: 41 of 45 SKIP relation tpcdi.FactMarketHistory ................................. [[33mSKIP[0m]
[0m15:33:19.144408 [debug] [Thread-2  ]: Finished running node test.dbsql_dbt_tpch.tradefee_DimTrade_fee.5ee05431ee
[0m15:33:19.144671 [debug] [Thread-7  ]: Finished running node test.dbsql_dbt_tpch.not_null_FactWatches_sk_customerid.1c7356b8c9
[0m15:33:19.144935 [debug] [Thread-14 ]: Finished running node test.dbsql_dbt_tpch.not_null_FactWatches_sk_securityid.8a6b5e97b3
[0m15:33:19.145267 [debug] [Thread-5  ]: Finished running node model.dbsql_dbt_tpch.FactMarketHistory
[0m15:33:19.145803 [debug] [Thread-5  ]: Began running node model.dbsql_dbt_tpch.FactHoldings
[0m15:33:19.146112 [info ] [Thread-5  ]: 42 of 45 SKIP relation tpcdi.FactHoldings ...................................... [[33mSKIP[0m]
[0m15:33:19.146450 [debug] [Thread-5  ]: Finished running node model.dbsql_dbt_tpch.FactHoldings
[0m15:33:19.146971 [debug] [Thread-18 ]: Began running node test.dbsql_dbt_tpch.not_null_FactMarketHistory_PERatio.83761a27d0
[0m15:33:19.147420 [info ] [Thread-18 ]: 43 of 45 SKIP test not_null_FactMarketHistory_PERatio .......................... [[33mSKIP[0m]
[0m15:33:19.147971 [debug] [Thread-15 ]: Began running node test.dbsql_dbt_tpch.not_null_FactMarketHistory_sk_securityid.16a0e9b8b2
[0m15:33:19.148265 [debug] [Thread-18 ]: Finished running node test.dbsql_dbt_tpch.not_null_FactMarketHistory_PERatio.83761a27d0
[0m15:33:19.148510 [debug] [Thread-9  ]: Began running node test.dbsql_dbt_tpch.not_null_FactHoldings_currentprice.98a44eac00
[0m15:33:19.148807 [info ] [Thread-15 ]: 44 of 45 SKIP test not_null_FactMarketHistory_sk_securityid .................... [[33mSKIP[0m]
[0m15:33:19.149236 [info ] [Thread-9  ]: 45 of 45 SKIP test not_null_FactHoldings_currentprice .......................... [[33mSKIP[0m]
[0m15:33:19.149579 [debug] [Thread-15 ]: Finished running node test.dbsql_dbt_tpch.not_null_FactMarketHistory_sk_securityid.16a0e9b8b2
[0m15:33:19.149842 [debug] [Thread-9  ]: Finished running node test.dbsql_dbt_tpch.not_null_FactHoldings_currentprice.98a44eac00
[0m15:33:19.151675 [debug] [MainThread]: On master: ROLLBACK
[0m15:33:19.151942 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:33:19.372034 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:33:19.372766 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:33:19.373128 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:33:19.373467 [debug] [MainThread]: On master: ROLLBACK
[0m15:33:19.373782 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:33:19.374087 [debug] [MainThread]: On master: Close
[0m15:33:19.435070 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:33:19.435992 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.AccountIncremental' was properly closed.
[0m15:33:19.436577 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.BatchDate' was properly closed.
[0m15:33:19.436951 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.CashTransactionIncremental' was properly closed.
[0m15:33:19.437285 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.CustomerIncremental' was properly closed.
[0m15:33:19.437609 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.CustomerMgmtView' was properly closed.
[0m15:33:19.437935 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.DailyMarketHistorical' was properly closed.
[0m15:33:19.438259 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.DailyMarketIncremental' was properly closed.
[0m15:33:19.438571 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.DimBroker' was properly closed.
[0m15:33:19.438880 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.FinWire' was properly closed.
[0m15:33:19.439205 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.HoldingIncremental' was properly closed.
[0m15:33:19.439523 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.ProspectRaw' was properly closed.
[0m15:33:19.439834 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.TradeIncremental' was properly closed.
[0m15:33:19.440142 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.WatchIncremental' was properly closed.
[0m15:33:19.444618 [info ] [MainThread]: 
[0m15:33:19.445148 [info ] [MainThread]: Finished running 12 view models, 15 table models, 18 tests in 0 hours 0 minutes and 3.10 seconds (3.10s).
[0m15:33:19.447762 [debug] [MainThread]: Command end result
[0m15:33:19.465516 [info ] [MainThread]: 
[0m15:33:19.465932 [info ] [MainThread]: [31mCompleted with 13 errors and 0 warnings:[0m
[0m15:33:19.466212 [info ] [MainThread]: 
[0m15:33:19.466489 [error] [MainThread]: [33mRuntime Error in model AccountIncremental (models/base/AccountIncremental.sql)[0m
[0m15:33:19.466757 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`AccountIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:33:19.467026 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:33:19.467283 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:33:19.467535 [info ] [MainThread]: 
[0m15:33:19.467786 [error] [MainThread]: [33mRuntime Error in model BatchDate (models/base/BatchDate.sql)[0m
[0m15:33:19.468029 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`BatchDateuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:33:19.468258 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:33:19.468483 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:33:19.468710 [info ] [MainThread]: 
[0m15:33:19.468934 [error] [MainThread]: [33mRuntime Error in model CustomerIncremental (models/base/CustomerIncremental.sql)[0m
[0m15:33:19.469163 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`customerincrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:33:19.469386 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:33:19.469610 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:33:19.469832 [info ] [MainThread]: 
[0m15:33:19.470056 [error] [MainThread]: [33mRuntime Error in model CashTransactionIncremental (models/base/CashTransactionIncremental.sql)[0m
[0m15:33:19.470282 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`CashTransactionIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:33:19.470508 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:33:19.470733 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:33:19.470957 [info ] [MainThread]: 
[0m15:33:19.471180 [error] [MainThread]: [33mRuntime Error in model CustomerMgmtView (models/base/CustomerMgmtView.sql)[0m
[0m15:33:19.471407 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`roberto_salcido_tpcdi_stage`.`customermgmt10` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:33:19.471637 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:33:19.471859 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 10 pos 4
[0m15:33:19.472075 [info ] [MainThread]: 
[0m15:33:19.472294 [error] [MainThread]: [33mRuntime Error in model DailyMarketIncremental (models/base/DailyMarketIncremental.sql)[0m
[0m15:33:19.472516 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:33:19.472741 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:33:19.472961 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:33:19.473180 [info ] [MainThread]: 
[0m15:33:19.473400 [error] [MainThread]: [33mRuntime Error in model HoldingIncremental (models/base/HoldingIncremental.sql)[0m
[0m15:33:19.473627 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HoldingIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:33:19.473854 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:33:19.474076 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:33:19.474297 [info ] [MainThread]: 
[0m15:33:19.474521 [error] [MainThread]: [33mRuntime Error in model WatchIncremental (models/base/WatchIncremental.sql)[0m
[0m15:33:19.474748 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`WatchIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:33:19.474972 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:33:19.475191 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:33:19.475413 [info ] [MainThread]: 
[0m15:33:19.475632 [error] [MainThread]: [33mRuntime Error in model ProspectRaw (models/base/ProspectRaw.sql)[0m
[0m15:33:19.475856 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`ProspectRawuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:33:19.476112 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:33:19.476333 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:33:19.476554 [info ] [MainThread]: 
[0m15:33:19.476776 [error] [MainThread]: [33mRuntime Error in model DailyMarketHistorical (models/base/DailyMarketHistorical.sql)[0m
[0m15:33:19.477000 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketHistorical` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:33:19.477227 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:33:19.477448 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:33:19.477667 [info ] [MainThread]: 
[0m15:33:19.477865 [error] [MainThread]: [33mRuntime Error in model TradeIncremental (models/base/TradeIncremental.sql)[0m
[0m15:33:19.478065 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`TradeIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:33:19.478267 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:33:19.478467 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:33:19.478667 [info ] [MainThread]: 
[0m15:33:19.478865 [error] [MainThread]: [33mRuntime Error in model DimBroker (models/silver/DimBroker.sql)[0m
[0m15:33:19.479073 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HR` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:33:19.479278 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:33:19.479481 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 33 pos 6
[0m15:33:19.479681 [info ] [MainThread]: 
[0m15:33:19.479880 [error] [MainThread]: [33mRuntime Error in model FinWire (models/base/FinWire.sql)[0m
[0m15:33:19.480081 [error] [MainThread]:   [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: text; line 9 pos 0
[0m15:33:19.480317 [info ] [MainThread]: 
[0m15:33:19.480553 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=13 SKIP=32 TOTAL=45
[0m15:33:19.481036 [debug] [MainThread]: Command `dbt build` failed at 15:33:19.480965 after 4.23 seconds
[0m15:33:19.481281 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104be55e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12ff66b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13a141af0>]}
[0m15:33:19.481524 [debug] [MainThread]: Flushing usage events
[0m15:54:09.841701 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1036a4b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105790ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105790e80>]}


============================== 15:54:09.863902 | e01d10bd-9d53-4e1f-8722-8befaaac6841 ==============================
[0m15:54:09.863902 [info ] [MainThread]: Running with dbt=1.5.0
[0m15:54:09.864265 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:54:10.742847 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e01d10bd-9d53-4e1f-8722-8befaaac6841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100991040>]}
[0m15:54:10.752495 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e01d10bd-9d53-4e1f-8722-8befaaac6841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15cc834c0>]}
[0m15:54:10.784379 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m15:54:10.800981 [info ] [MainThread]: Unable to do partial parsing because of a version mismatch
[0m15:54:10.801375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'e01d10bd-9d53-4e1f-8722-8befaaac6841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15cc1b070>]}
[0m15:54:11.801969 [debug] [MainThread]: 1699: static parser successfully parsed incremental/FactWatches.sql
[0m15:54:11.810684 [debug] [MainThread]: 1699: static parser successfully parsed incremental/Prospect.sql
[0m15:54:11.812994 [debug] [MainThread]: 1699: static parser successfully parsed incremental/FactCashBalances.sql
[0m15:54:11.815469 [debug] [MainThread]: 1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
[0m15:54:11.818090 [debug] [MainThread]: 1699: static parser successfully parsed incremental/FactHoldings.sql
[0m15:54:11.820637 [debug] [MainThread]: 1699: static parser successfully parsed incremental/FactMarketHistory.sql
[0m15:54:11.823086 [debug] [MainThread]: 1699: static parser successfully parsed incremental/DimCustomer.sql
[0m15:54:11.825372 [debug] [MainThread]: 1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
[0m15:54:11.827867 [debug] [MainThread]: 1699: static parser successfully parsed incremental/DimTrade.sql
[0m15:54:11.830316 [debug] [MainThread]: 1699: static parser successfully parsed incremental/DimAccount.sql
[0m15:54:11.832692 [debug] [MainThread]: 1699: static parser successfully parsed incremental/DimCustomerStg.sql
[0m15:54:11.835258 [debug] [MainThread]: 1699: static parser successfully parsed silver/DimCompany.sql
[0m15:54:11.837968 [debug] [MainThread]: 1699: static parser successfully parsed silver/DimBroker.sql
[0m15:54:11.840316 [debug] [MainThread]: 1699: static parser successfully parsed silver/Financial.sql
[0m15:54:11.842734 [debug] [MainThread]: 1699: static parser successfully parsed silver/DimSecurity.sql
[0m15:54:11.845067 [debug] [MainThread]: 1699: static parser successfully parsed base/CustomerIncremental.sql
[0m15:54:11.847193 [debug] [MainThread]: 1699: static parser successfully parsed base/CashTransactionIncremental.sql
[0m15:54:11.849287 [debug] [MainThread]: 1603: static parser failed on base/CustomerMgmtView.sql
[0m15:54:11.853073 [debug] [MainThread]: 1602: parser fallback to jinja rendering on base/CustomerMgmtView.sql
[0m15:54:11.854643 [debug] [MainThread]: 1699: static parser successfully parsed base/TradeIncremental.sql
[0m15:54:11.857011 [debug] [MainThread]: 1699: static parser successfully parsed base/HoldingIncremental.sql
[0m15:54:11.860059 [debug] [MainThread]: 1699: static parser successfully parsed base/DailyMarketIncremental.sql
[0m15:54:11.862462 [debug] [MainThread]: 1699: static parser successfully parsed base/BatchDate.sql
[0m15:54:11.864685 [debug] [MainThread]: 1699: static parser successfully parsed base/AccountIncremental.sql
[0m15:54:11.866949 [debug] [MainThread]: 1603: static parser failed on base/FinWire.sql
[0m15:54:11.870607 [debug] [MainThread]: 1602: parser fallback to jinja rendering on base/FinWire.sql
[0m15:54:11.871981 [debug] [MainThread]: 1699: static parser successfully parsed base/WatchIncremental.sql
[0m15:54:11.874477 [debug] [MainThread]: 1699: static parser successfully parsed base/ProspectRaw.sql
[0m15:54:11.877065 [debug] [MainThread]: 1699: static parser successfully parsed base/DailyMarketHistorical.sql
[0m15:54:12.201151 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e01d10bd-9d53-4e1f-8722-8befaaac6841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15d17e850>]}
[0m15:54:12.215950 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e01d10bd-9d53-4e1f-8722-8befaaac6841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15cc99f10>]}
[0m15:54:12.216272 [info ] [MainThread]: Found 27 models, 18 tests, 0 snapshots, 0 analyses, 681 macros, 0 operations, 0 seed files, 33 sources, 0 exposures, 0 metrics, 0 groups
[0m15:54:12.218464 [info ] [MainThread]: 
[0m15:54:12.219025 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m15:54:12.220987 [debug] [ThreadPool]: Acquiring new databricks connection 'list_main'
[0m15:54:12.221502 [debug] [ThreadPool]: Acquiring new databricks connection 'list_main'
[0m15:54:12.221742 [debug] [ThreadPool]: Using databricks connection "list_main"
[0m15:54:12.221972 [debug] [ThreadPool]: Using databricks connection "list_main"
[0m15:54:12.222184 [debug] [ThreadPool]: On list_main: GetSchemas(database=`main`, schema=None)
[0m15:54:12.222361 [debug] [ThreadPool]: On list_main: GetSchemas(database=`main`, schema=None)
[0m15:54:12.222541 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:54:12.222706 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:54:12.841528 [debug] [ThreadPool]: SQL status: OK in 0.6200000047683716 seconds
[0m15:54:12.853172 [debug] [ThreadPool]: On list_main: Close
[0m15:54:12.894296 [debug] [ThreadPool]: SQL status: OK in 0.6700000166893005 seconds
[0m15:54:12.896584 [debug] [ThreadPool]: On list_main: Close
[0m15:54:12.956721 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_main, now create_main_tpcdi_dbt_test__audit)
[0m15:54:12.957804 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_main, now create_main_tpcdi)
[0m15:54:12.960506 [debug] [ThreadPool]: Creating schema "database: "main"
schema: "tpcdi_dbt_test__audit"
"
[0m15:54:12.961358 [debug] [ThreadPool]: Creating schema "database: "main"
schema: "tpcdi"
"
[0m15:54:12.973587 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:54:12.976193 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:54:12.976498 [debug] [ThreadPool]: Using databricks connection "create_main_tpcdi_dbt_test__audit"
[0m15:54:12.976756 [debug] [ThreadPool]: Using databricks connection "create_main_tpcdi"
[0m15:54:12.977028 [debug] [ThreadPool]: On create_main_tpcdi_dbt_test__audit: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "create_main_tpcdi_dbt_test__audit"} */
create schema if not exists `main`.`tpcdi_dbt_test__audit`
  
[0m15:54:12.977308 [debug] [ThreadPool]: On create_main_tpcdi: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "create_main_tpcdi"} */
create schema if not exists `main`.`tpcdi`
  
[0m15:54:12.977576 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:54:12.977805 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:54:13.348992 [debug] [ThreadPool]: SQL status: OK in 0.3700000047683716 seconds
[0m15:54:13.350817 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m15:54:13.351244 [debug] [ThreadPool]: On create_main_tpcdi: ROLLBACK
[0m15:54:13.351613 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m15:54:13.351938 [debug] [ThreadPool]: On create_main_tpcdi: Close
[0m15:54:13.355982 [debug] [ThreadPool]: SQL status: OK in 0.3799999952316284 seconds
[0m15:54:13.357084 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m15:54:13.357445 [debug] [ThreadPool]: On create_main_tpcdi_dbt_test__audit: ROLLBACK
[0m15:54:13.357729 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m15:54:13.357985 [debug] [ThreadPool]: On create_main_tpcdi_dbt_test__audit: Close
[0m15:54:13.438184 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_main_tpcdi_dbt_test__audit, now list_main_tpcdi_dbt_test__audit)
[0m15:54:13.439360 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_main_tpcdi, now list_main_tpcdi)
[0m15:54:13.448851 [debug] [ThreadPool]: Using databricks connection "list_main_tpcdi"
[0m15:54:13.453245 [debug] [ThreadPool]: On list_main_tpcdi: GetTables(database=main, schema=tpcdi, identifier=None)
[0m15:54:13.465716 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:54:13.466957 [debug] [ThreadPool]: Using databricks connection "list_main_tpcdi_dbt_test__audit"
[0m15:54:13.467401 [debug] [ThreadPool]: On list_main_tpcdi_dbt_test__audit: GetTables(database=main, schema=tpcdi_dbt_test__audit, identifier=None)
[0m15:54:13.475834 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:54:13.811537 [debug] [ThreadPool]: SQL status: OK in 0.3499999940395355 seconds
[0m15:54:13.817256 [debug] [ThreadPool]: On list_main_tpcdi: Close
[0m15:54:13.829632 [debug] [ThreadPool]: SQL status: OK in 0.3499999940395355 seconds
[0m15:54:13.832803 [debug] [ThreadPool]: On list_main_tpcdi_dbt_test__audit: Close
[0m15:54:13.898739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e01d10bd-9d53-4e1f-8722-8befaaac6841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15d17eb50>]}
[0m15:54:13.900727 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:54:13.901198 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:54:13.902458 [info ] [MainThread]: Concurrency: 25 threads (target='dev')
[0m15:54:13.903237 [info ] [MainThread]: 
[0m15:54:13.913388 [debug] [Thread-1  ]: Began running node model.dbsql_dbt_tpch.AccountIncremental
[0m15:54:13.914019 [debug] [Thread-2  ]: Began running node model.dbsql_dbt_tpch.BatchDate
[0m15:54:13.914449 [debug] [Thread-3  ]: Began running node model.dbsql_dbt_tpch.CashTransactionIncremental
[0m15:54:13.914890 [debug] [Thread-4  ]: Began running node model.dbsql_dbt_tpch.CustomerIncremental
[0m15:54:13.915357 [debug] [Thread-5  ]: Began running node model.dbsql_dbt_tpch.CustomerMgmtView
[0m15:54:13.915793 [debug] [Thread-6  ]: Began running node model.dbsql_dbt_tpch.DailyMarketHistorical
[0m15:54:13.916806 [debug] [Thread-7  ]: Began running node model.dbsql_dbt_tpch.DailyMarketIncremental
[0m15:54:13.916398 [info ] [Thread-1  ]: 1 of 45 START sql view model tpcdi.AccountIncremental .......................... [RUN]
[0m15:54:13.917531 [debug] [Thread-8  ]: Began running node model.dbsql_dbt_tpch.DimBroker
[0m15:54:13.918022 [debug] [Thread-9  ]: Began running node model.dbsql_dbt_tpch.FinWire
[0m15:54:13.918512 [debug] [Thread-10 ]: Began running node model.dbsql_dbt_tpch.HoldingIncremental
[0m15:54:13.918987 [info ] [Thread-2  ]: 2 of 45 START sql view model tpcdi.BatchDate ................................... [RUN]
[0m15:54:13.919499 [debug] [Thread-11 ]: Began running node model.dbsql_dbt_tpch.ProspectRaw
[0m15:54:13.919953 [info ] [Thread-3  ]: 3 of 45 START sql view model tpcdi.CashTransactionIncremental .................. [RUN]
[0m15:54:13.920393 [debug] [Thread-12 ]: Began running node model.dbsql_dbt_tpch.TradeIncremental
[0m15:54:13.920768 [debug] [Thread-13 ]: Began running node model.dbsql_dbt_tpch.WatchIncremental
[0m15:54:13.921200 [info ] [Thread-4  ]: 4 of 45 START sql view model tpcdi.CustomerIncremental ......................... [RUN]
[0m15:54:13.921715 [info ] [Thread-5  ]: 5 of 45 START sql view model tpcdi.CustomerMgmtView ............................ [RUN]
[0m15:54:13.922212 [info ] [Thread-6  ]: 6 of 45 START sql view model tpcdi.DailyMarketHistorical ....................... [RUN]
[0m15:54:13.922666 [info ] [Thread-7  ]: 7 of 45 START sql view model tpcdi.DailyMarketIncremental ...................... [RUN]
[0m15:54:13.923426 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_main_tpcdi_dbt_test__audit, now model.dbsql_dbt_tpch.AccountIncremental)
[0m15:54:13.923811 [info ] [Thread-8  ]: 8 of 45 START sql table model tpcdi.DimBroker .................................. [RUN]
[0m15:54:13.924250 [info ] [Thread-9  ]: 9 of 45 START sql view model tpcdi.FinWire ..................................... [RUN]
[0m15:54:13.924684 [info ] [Thread-10 ]: 10 of 45 START sql view model tpcdi.HoldingIncremental ......................... [RUN]
[0m15:54:13.925329 [debug] [Thread-2  ]: Re-using an available connection from the pool (formerly list_main_tpcdi, now model.dbsql_dbt_tpch.BatchDate)
[0m15:54:13.925687 [info ] [Thread-11 ]: 11 of 45 START sql view model tpcdi.ProspectRaw ................................ [RUN]
[0m15:54:13.926682 [debug] [Thread-3  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.CashTransactionIncremental'
[0m15:54:13.927195 [info ] [Thread-12 ]: 12 of 45 START sql view model tpcdi.TradeIncremental ........................... [RUN]
[0m15:54:13.927664 [info ] [Thread-13 ]: 13 of 45 START sql view model tpcdi.WatchIncremental ........................... [RUN]
[0m15:54:13.928467 [debug] [Thread-4  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.CustomerIncremental'
[0m15:54:13.929020 [debug] [Thread-5  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.CustomerMgmtView'
[0m15:54:13.929566 [debug] [Thread-6  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.DailyMarketHistorical'
[0m15:54:13.930099 [debug] [Thread-7  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.DailyMarketIncremental'
[0m15:54:13.930389 [debug] [Thread-1  ]: Began compiling node model.dbsql_dbt_tpch.AccountIncremental
[0m15:54:13.930923 [debug] [Thread-8  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.DimBroker'
[0m15:54:13.931452 [debug] [Thread-9  ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.FinWire'
[0m15:54:13.932259 [debug] [Thread-10 ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.HoldingIncremental'
[0m15:54:13.932689 [debug] [Thread-2  ]: Began compiling node model.dbsql_dbt_tpch.BatchDate
[0m15:54:13.933453 [debug] [Thread-11 ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.ProspectRaw'
[0m15:54:13.933709 [debug] [Thread-3  ]: Began compiling node model.dbsql_dbt_tpch.CashTransactionIncremental
[0m15:54:13.934217 [debug] [Thread-12 ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.TradeIncremental'
[0m15:54:13.934716 [debug] [Thread-13 ]: Acquiring new databricks connection 'model.dbsql_dbt_tpch.WatchIncremental'
[0m15:54:13.934961 [debug] [Thread-4  ]: Began compiling node model.dbsql_dbt_tpch.CustomerIncremental
[0m15:54:13.935192 [debug] [Thread-5  ]: Began compiling node model.dbsql_dbt_tpch.CustomerMgmtView
[0m15:54:13.935410 [debug] [Thread-6  ]: Began compiling node model.dbsql_dbt_tpch.DailyMarketHistorical
[0m15:54:13.935634 [debug] [Thread-7  ]: Began compiling node model.dbsql_dbt_tpch.DailyMarketIncremental
[0m15:54:13.939319 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.AccountIncremental"
[0m15:54:13.939730 [debug] [Thread-8  ]: Began compiling node model.dbsql_dbt_tpch.DimBroker
[0m15:54:13.939980 [debug] [Thread-9  ]: Began compiling node model.dbsql_dbt_tpch.FinWire
[0m15:54:13.940216 [debug] [Thread-10 ]: Began compiling node model.dbsql_dbt_tpch.HoldingIncremental
[0m15:54:13.943445 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.BatchDate"
[0m15:54:13.943801 [debug] [Thread-11 ]: Began compiling node model.dbsql_dbt_tpch.ProspectRaw
[0m15:54:13.946888 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.CashTransactionIncremental"
[0m15:54:13.947196 [debug] [Thread-12 ]: Began compiling node model.dbsql_dbt_tpch.TradeIncremental
[0m15:54:13.947466 [debug] [Thread-13 ]: Began compiling node model.dbsql_dbt_tpch.WatchIncremental
[0m15:54:13.951293 [debug] [Thread-5  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.CustomerMgmtView"
[0m15:54:13.958579 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.CustomerIncremental"
[0m15:54:13.960978 [debug] [Thread-6  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.DailyMarketHistorical"
[0m15:54:13.963337 [debug] [Thread-7  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.DailyMarketIncremental"
[0m15:54:13.966447 [debug] [Thread-8  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.DimBroker"
[0m15:54:13.968853 [debug] [Thread-9  ]: Writing injected SQL for node "model.dbsql_dbt_tpch.FinWire"
[0m15:54:13.972274 [debug] [Thread-10 ]: Writing injected SQL for node "model.dbsql_dbt_tpch.HoldingIncremental"
[0m15:54:13.972571 [debug] [Thread-1  ]: Timing info for model.dbsql_dbt_tpch.AccountIncremental (compile): 15:54:13.935797 => 15:54:13.972445
[0m15:54:13.975297 [debug] [Thread-11 ]: Writing injected SQL for node "model.dbsql_dbt_tpch.ProspectRaw"
[0m15:54:13.977876 [debug] [Thread-12 ]: Writing injected SQL for node "model.dbsql_dbt_tpch.TradeIncremental"
[0m15:54:13.978155 [debug] [Thread-2  ]: Timing info for model.dbsql_dbt_tpch.BatchDate (compile): 15:54:13.940376 => 15:54:13.978042
[0m15:54:13.980569 [debug] [Thread-13 ]: Writing injected SQL for node "model.dbsql_dbt_tpch.WatchIncremental"
[0m15:54:13.980964 [debug] [Thread-3  ]: Timing info for model.dbsql_dbt_tpch.CashTransactionIncremental (compile): 15:54:13.944019 => 15:54:13.980843
[0m15:54:13.981396 [debug] [Thread-5  ]: Timing info for model.dbsql_dbt_tpch.CustomerMgmtView (compile): 15:54:13.948753 => 15:54:13.981264
[0m15:54:13.981752 [debug] [Thread-4  ]: Timing info for model.dbsql_dbt_tpch.CustomerIncremental (compile): 15:54:13.947635 => 15:54:13.981646
[0m15:54:13.982035 [debug] [Thread-6  ]: Timing info for model.dbsql_dbt_tpch.DailyMarketHistorical (compile): 15:54:13.958887 => 15:54:13.981894
[0m15:54:13.982468 [debug] [Thread-7  ]: Timing info for model.dbsql_dbt_tpch.DailyMarketIncremental (compile): 15:54:13.961162 => 15:54:13.982343
[0m15:54:13.982706 [debug] [Thread-8  ]: Timing info for model.dbsql_dbt_tpch.DimBroker (compile): 15:54:13.963652 => 15:54:13.982605
[0m15:54:13.982921 [debug] [Thread-1  ]: Began executing node model.dbsql_dbt_tpch.AccountIncremental
[0m15:54:13.983156 [debug] [Thread-9  ]: Timing info for model.dbsql_dbt_tpch.FinWire (compile): 15:54:13.966725 => 15:54:13.983042
[0m15:54:13.983445 [debug] [Thread-10 ]: Timing info for model.dbsql_dbt_tpch.HoldingIncremental (compile): 15:54:13.969071 => 15:54:13.983344
[0m15:54:13.983729 [debug] [Thread-11 ]: Timing info for model.dbsql_dbt_tpch.ProspectRaw (compile): 15:54:13.972795 => 15:54:13.983633
[0m15:54:13.983904 [debug] [Thread-2  ]: Began executing node model.dbsql_dbt_tpch.BatchDate
[0m15:54:13.984193 [debug] [Thread-12 ]: Timing info for model.dbsql_dbt_tpch.TradeIncremental (compile): 15:54:13.975618 => 15:54:13.984085
[0m15:54:13.984415 [debug] [Thread-3  ]: Began executing node model.dbsql_dbt_tpch.CashTransactionIncremental
[0m15:54:13.984627 [debug] [Thread-13 ]: Timing info for model.dbsql_dbt_tpch.WatchIncremental (compile): 15:54:13.978318 => 15:54:13.984534
[0m15:54:13.984829 [debug] [Thread-5  ]: Began executing node model.dbsql_dbt_tpch.CustomerMgmtView
[0m15:54:13.985251 [debug] [Thread-4  ]: Began executing node model.dbsql_dbt_tpch.CustomerIncremental
[0m15:54:13.985596 [debug] [Thread-6  ]: Began executing node model.dbsql_dbt_tpch.DailyMarketHistorical
[0m15:54:13.985852 [debug] [Thread-7  ]: Began executing node model.dbsql_dbt_tpch.DailyMarketIncremental
[0m15:54:13.986074 [debug] [Thread-8  ]: Began executing node model.dbsql_dbt_tpch.DimBroker
[0m15:54:14.002934 [debug] [Thread-1  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.AccountIncremental"
[0m15:54:14.003293 [debug] [Thread-9  ]: Began executing node model.dbsql_dbt_tpch.FinWire
[0m15:54:14.003539 [debug] [Thread-10 ]: Began executing node model.dbsql_dbt_tpch.HoldingIncremental
[0m15:54:14.003755 [debug] [Thread-11 ]: Began executing node model.dbsql_dbt_tpch.ProspectRaw
[0m15:54:14.007303 [debug] [Thread-2  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.BatchDate"
[0m15:54:14.007619 [debug] [Thread-12 ]: Began executing node model.dbsql_dbt_tpch.TradeIncremental
[0m15:54:14.010076 [debug] [Thread-3  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.CashTransactionIncremental"
[0m15:54:14.010322 [debug] [Thread-13 ]: Began executing node model.dbsql_dbt_tpch.WatchIncremental
[0m15:54:14.012568 [debug] [Thread-5  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.CustomerMgmtView"
[0m15:54:14.014923 [debug] [Thread-4  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.CustomerIncremental"
[0m15:54:14.017187 [debug] [Thread-6  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.DailyMarketHistorical"
[0m15:54:14.019982 [debug] [Thread-7  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.DailyMarketIncremental"
[0m15:54:14.046305 [debug] [Thread-8  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.DimBroker"
[0m15:54:14.049239 [debug] [Thread-9  ]: Writing runtime sql for node "model.dbsql_dbt_tpch.FinWire"
[0m15:54:14.051668 [debug] [Thread-10 ]: Writing runtime sql for node "model.dbsql_dbt_tpch.HoldingIncremental"
[0m15:54:14.054594 [debug] [Thread-11 ]: Writing runtime sql for node "model.dbsql_dbt_tpch.ProspectRaw"
[0m15:54:14.054856 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:54:14.057294 [debug] [Thread-12 ]: Writing runtime sql for node "model.dbsql_dbt_tpch.TradeIncremental"
[0m15:54:14.060086 [debug] [Thread-13 ]: Writing runtime sql for node "model.dbsql_dbt_tpch.WatchIncremental"
[0m15:54:14.060461 [debug] [Thread-2  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:54:14.060869 [debug] [Thread-3  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:54:14.061332 [debug] [Thread-4  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:54:14.061629 [debug] [Thread-5  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:54:14.061979 [debug] [Thread-7  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:54:14.062216 [debug] [Thread-6  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:54:14.062489 [debug] [Thread-8  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:54:14.062790 [debug] [Thread-9  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:54:14.062972 [debug] [Thread-1  ]: Using databricks connection "model.dbsql_dbt_tpch.AccountIncremental"
[0m15:54:14.063265 [debug] [Thread-10 ]: Spark adapter: NotImplemented: add_begin_query
[0m15:54:14.063526 [debug] [Thread-12 ]: Spark adapter: NotImplemented: add_begin_query
[0m15:54:14.063728 [debug] [Thread-11 ]: Spark adapter: NotImplemented: add_begin_query
[0m15:54:14.063896 [debug] [Thread-2  ]: Using databricks connection "model.dbsql_dbt_tpch.BatchDate"
[0m15:54:14.064081 [debug] [Thread-13 ]: Spark adapter: NotImplemented: add_begin_query
[0m15:54:14.064230 [debug] [Thread-3  ]: Using databricks connection "model.dbsql_dbt_tpch.CashTransactionIncremental"
[0m15:54:14.064388 [debug] [Thread-4  ]: Using databricks connection "model.dbsql_dbt_tpch.CustomerIncremental"
[0m15:54:14.064539 [debug] [Thread-5  ]: Using databricks connection "model.dbsql_dbt_tpch.CustomerMgmtView"
[0m15:54:14.064684 [debug] [Thread-7  ]: Using databricks connection "model.dbsql_dbt_tpch.DailyMarketIncremental"
[0m15:54:14.064826 [debug] [Thread-6  ]: Using databricks connection "model.dbsql_dbt_tpch.DailyMarketHistorical"
[0m15:54:14.064966 [debug] [Thread-8  ]: Using databricks connection "model.dbsql_dbt_tpch.DimBroker"
[0m15:54:14.065106 [debug] [Thread-9  ]: Using databricks connection "model.dbsql_dbt_tpch.FinWire"
[0m15:54:14.065291 [debug] [Thread-1  ]: On model.dbsql_dbt_tpch.AccountIncremental: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.AccountIncremental"} */
create or replace view `main`.`tpcdi`.`AccountIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`AccountIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`AccountIncrementaltres`

[0m15:54:14.065456 [debug] [Thread-10 ]: Using databricks connection "model.dbsql_dbt_tpch.HoldingIncremental"
[0m15:54:14.065594 [debug] [Thread-12 ]: Using databricks connection "model.dbsql_dbt_tpch.TradeIncremental"
[0m15:54:14.065748 [debug] [Thread-11 ]: Using databricks connection "model.dbsql_dbt_tpch.ProspectRaw"
[0m15:54:14.065938 [debug] [Thread-2  ]: On model.dbsql_dbt_tpch.BatchDate: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.BatchDate"} */
create or replace view `main`.`tpcdi`.`BatchDate`
  
  
  as
    

select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`BatchDateuno`

 UNION ALL

select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`BatchDatedos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`BatchDatetres`

[0m15:54:14.066112 [debug] [Thread-13 ]: Using databricks connection "model.dbsql_dbt_tpch.WatchIncremental"
[0m15:54:14.066299 [debug] [Thread-3  ]: On model.dbsql_dbt_tpch.CashTransactionIncremental: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CashTransactionIncremental"} */
create or replace view `main`.`tpcdi`.`CashTransactionIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`CashTransactionIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`CashTransactionIncrementaltres`

[0m15:54:14.066520 [debug] [Thread-4  ]: On model.dbsql_dbt_tpch.CustomerIncremental: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CustomerIncremental"} */
create or replace view `main`.`tpcdi`.`CustomerIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`customerincrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`customerincrementaltres`

[0m15:54:14.066736 [debug] [Thread-5  ]: On model.dbsql_dbt_tpch.CustomerMgmtView: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CustomerMgmtView"} */
create or replace view `main`.`tpcdi`.`CustomerMgmtView`
  
  
  as
    
select
    *
from
    hive_metastore.roberto_salcido_tpcdi_stage.customermgmt10

[0m15:54:14.066937 [debug] [Thread-7  ]: On model.dbsql_dbt_tpch.DailyMarketIncremental: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DailyMarketIncremental"} */
create or replace view `main`.`tpcdi`.`DailyMarketIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`DailyMarketIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`DailyMarketIncrementaltres`

[0m15:54:14.067145 [debug] [Thread-6  ]: On model.dbsql_dbt_tpch.DailyMarketHistorical: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DailyMarketHistorical"} */
create or replace view `main`.`tpcdi`.`DailyMarketHistorical`
  
  
  as
    
select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`DailyMarketHistorical`

[0m15:54:14.067379 [debug] [Thread-8  ]: On model.dbsql_dbt_tpch.DimBroker: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

  
    
        create or replace table `main`.`tpcdi`.`DimBroker`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT

  cast(employeeid as BIGINT) brokerid,
  cast(managerid as BIGINT) managerid,
  employeefirstname firstname,
  employeelastname lastname,
  employeemi middleinitial,
  employeebranch branch,
  employeeoffice office,
  employeephone phone,
  true iscurrent,
  1 batchid,
  (SELECT min(to_date(datevalue)) as effectivedate FROM `main`.`tpcdi`.`DimDate`) effectivedate,
  date('9999-12-31') enddate,
  bigint(concat(date_format(enddate, 'yyyyMMdd'), cast(brokerid as string))) as sk_brokerid
FROM  `main`.`tpcdi`.`HR`
WHERE employeejobcode = 314
  
[0m15:54:14.067590 [debug] [Thread-9  ]: On model.dbsql_dbt_tpch.FinWire: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.FinWire"} */
create or replace view `main`.`tpcdi`.`FinWire`
  
  
  as
    

select *, substring(value, 16, 3) rectype from 
text.`dbfs:/tmp/tpcdi/sf=10/Batch1/FINWIRE*`

[0m15:54:14.067794 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:54:14.067977 [debug] [Thread-10 ]: On model.dbsql_dbt_tpch.HoldingIncremental: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.HoldingIncremental"} */
create or replace view `main`.`tpcdi`.`HoldingIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`HoldingIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`HoldingIncrementaltres`

[0m15:54:14.068182 [debug] [Thread-12 ]: On model.dbsql_dbt_tpch.TradeIncremental: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.TradeIncremental"} */
create or replace view `main`.`tpcdi`.`TradeIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`TradeIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`TradeIncrementaltres`

[0m15:54:14.068384 [debug] [Thread-11 ]: On model.dbsql_dbt_tpch.ProspectRaw: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.ProspectRaw"} */
create or replace view `main`.`tpcdi`.`ProspectRaw`
  
  
  as
    

select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`ProspectRawuno`

 UNION ALL

select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`ProspectRawdos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`ProspectRawtres`

[0m15:54:14.068587 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m15:54:14.068762 [debug] [Thread-13 ]: On model.dbsql_dbt_tpch.WatchIncremental: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.WatchIncremental"} */
create or replace view `main`.`tpcdi`.`WatchIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`WatchIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`WatchIncrementaltres`

[0m15:54:14.068955 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m15:54:14.069137 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m15:54:14.069316 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m15:54:14.069496 [debug] [Thread-7  ]: Opening a new connection, currently in state init
[0m15:54:14.069674 [debug] [Thread-6  ]: Opening a new connection, currently in state init
[0m15:54:14.069853 [debug] [Thread-8  ]: Opening a new connection, currently in state init
[0m15:54:14.070028 [debug] [Thread-9  ]: Opening a new connection, currently in state init
[0m15:54:14.070293 [debug] [Thread-10 ]: Opening a new connection, currently in state init
[0m15:54:14.077343 [debug] [Thread-12 ]: Opening a new connection, currently in state init
[0m15:54:14.080393 [debug] [Thread-11 ]: Opening a new connection, currently in state init
[0m15:54:14.080700 [debug] [Thread-13 ]: Opening a new connection, currently in state init
[0m15:54:14.507784 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.AccountIncremental"} */
create or replace view `main`.`tpcdi`.`AccountIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`AccountIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`AccountIncrementaltres`

[0m15:54:14.508889 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`AccountIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:54:14.509633 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`AccountIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`AccountIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:54:14.510381 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe3r\xf4\x1dY\xa6@\x94\xcc\xa4\x0cv\xb7'
[0m15:54:14.510814 [debug] [Thread-1  ]: Timing info for model.dbsql_dbt_tpch.AccountIncremental (execute): 15:54:13.986225 => 15:54:14.510677
[0m15:54:14.511066 [debug] [Thread-1  ]: On model.dbsql_dbt_tpch.AccountIncremental: ROLLBACK
[0m15:54:14.511272 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m15:54:14.511463 [debug] [Thread-1  ]: On model.dbsql_dbt_tpch.AccountIncremental: Close
[0m15:54:14.583249 [debug] [Thread-2  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.BatchDate"} */
create or replace view `main`.`tpcdi`.`BatchDate`
  
  
  as
    

select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`BatchDateuno`

 UNION ALL

select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`BatchDatedos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`BatchDatetres`

[0m15:54:14.585774 [debug] [Thread-3  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CashTransactionIncremental"} */
create or replace view `main`.`tpcdi`.`CashTransactionIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`CashTransactionIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`CashTransactionIncrementaltres`

[0m15:54:14.586584 [debug] [Thread-2  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`BatchDateuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:54:14.587336 [debug] [Thread-3  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`CashTransactionIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:54:14.588191 [debug] [Thread-2  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`BatchDateuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`BatchDateuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:54:14.589281 [debug] [Thread-3  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`CashTransactionIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`CashTransactionIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:54:14.590096 [debug] [Thread-2  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe3r\xfd\x1f \xab\xae>g&\xad\x9f\xe8'
[0m15:54:14.590597 [debug] [Thread-3  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe3r\xfe\x18\xeb\xbd\xb1\xf2|x\x91K%'
[0m15:54:14.591255 [debug] [Thread-2  ]: Timing info for model.dbsql_dbt_tpch.BatchDate (execute): 15:54:14.003912 => 15:54:14.591024
[0m15:54:14.591879 [debug] [Thread-3  ]: Timing info for model.dbsql_dbt_tpch.CashTransactionIncremental (execute): 15:54:14.007788 => 15:54:14.591666
[0m15:54:14.592405 [debug] [Thread-2  ]: On model.dbsql_dbt_tpch.BatchDate: ROLLBACK
[0m15:54:14.593010 [debug] [Thread-3  ]: On model.dbsql_dbt_tpch.CashTransactionIncremental: ROLLBACK
[0m15:54:14.593498 [debug] [Thread-2  ]: Databricks adapter: NotImplemented: rollback
[0m15:54:14.593861 [debug] [Thread-3  ]: Databricks adapter: NotImplemented: rollback
[0m15:54:14.594271 [debug] [Thread-2  ]: On model.dbsql_dbt_tpch.BatchDate: Close
[0m15:54:14.594680 [debug] [Thread-3  ]: On model.dbsql_dbt_tpch.CashTransactionIncremental: Close
[0m15:54:14.600416 [debug] [Thread-1  ]: Runtime Error in model AccountIncremental (models/base/AccountIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`AccountIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:54:14.600862 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e01d10bd-9d53-4e1f-8722-8befaaac6841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15d233c70>]}
[0m15:54:14.601402 [error] [Thread-1  ]: 1 of 45 ERROR creating sql view model tpcdi.AccountIncremental ................. [[31mERROR[0m in 0.68s]
[0m15:54:14.601905 [debug] [Thread-1  ]: Finished running node model.dbsql_dbt_tpch.AccountIncremental
[0m15:54:14.627381 [debug] [Thread-7  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DailyMarketIncremental"} */
create or replace view `main`.`tpcdi`.`DailyMarketIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`DailyMarketIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`DailyMarketIncrementaltres`

[0m15:54:14.627967 [debug] [Thread-7  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:54:14.628534 [debug] [Thread-7  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:54:14.629051 [debug] [Thread-7  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe3s\x03\x11\x85\x95\x9e\xec\x8dvc\xa0\xed'
[0m15:54:14.629543 [debug] [Thread-7  ]: Timing info for model.dbsql_dbt_tpch.DailyMarketIncremental (execute): 15:54:14.017347 => 15:54:14.629359
[0m15:54:14.629891 [debug] [Thread-7  ]: On model.dbsql_dbt_tpch.DailyMarketIncremental: ROLLBACK
[0m15:54:14.630186 [debug] [Thread-7  ]: Databricks adapter: NotImplemented: rollback
[0m15:54:14.630468 [debug] [Thread-7  ]: On model.dbsql_dbt_tpch.DailyMarketIncremental: Close
[0m15:54:14.634588 [debug] [Thread-6  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DailyMarketHistorical"} */
create or replace view `main`.`tpcdi`.`DailyMarketHistorical`
  
  
  as
    
select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`DailyMarketHistorical`

[0m15:54:14.635050 [debug] [Thread-6  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketHistorical` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:54:14.636404 [debug] [Thread-10 ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.HoldingIncremental"} */
create or replace view `main`.`tpcdi`.`HoldingIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`HoldingIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`HoldingIncrementaltres`

[0m15:54:14.637634 [debug] [Thread-12 ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.TradeIncremental"} */
create or replace view `main`.`tpcdi`.`TradeIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`TradeIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`TradeIncrementaltres`

[0m15:54:14.638865 [debug] [Thread-4  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CustomerIncremental"} */
create or replace view `main`.`tpcdi`.`CustomerIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`customerincrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`customerincrementaltres`

[0m15:54:14.639382 [debug] [Thread-6  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketHistorical` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketHistorical` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:54:14.639883 [debug] [Thread-10 ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HoldingIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:54:14.640197 [debug] [Thread-12 ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`TradeIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:54:14.640513 [debug] [Thread-4  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`customerincrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:54:14.640807 [debug] [Thread-6  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe3s\x00\x1dg\xb1\x88x\xb3\xda\xca\xf38'
[0m15:54:14.641267 [debug] [Thread-10 ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HoldingIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HoldingIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:54:14.641888 [debug] [Thread-12 ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`TradeIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`TradeIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:54:14.642525 [debug] [Thread-4  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`customerincrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`customerincrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:54:14.643080 [debug] [Thread-6  ]: Timing info for model.dbsql_dbt_tpch.DailyMarketHistorical (execute): 15:54:14.015089 => 15:54:14.642955
[0m15:54:14.643322 [debug] [Thread-10 ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe3s\x05\x18\x14\xa6\x90%a~\xe3\x99\xf9'
[0m15:54:14.643559 [debug] [Thread-12 ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe3s\x05\x18\xd2\xa0\xc2t\x9d\x8b\xd78\x16'
[0m15:54:14.643828 [debug] [Thread-4  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe3s\x02\x1f\xa3\xaf\xae\xc6p\xd2r\xa0\xf5'
[0m15:54:14.644870 [debug] [Thread-11 ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.ProspectRaw"} */
create or replace view `main`.`tpcdi`.`ProspectRaw`
  
  
  as
    

select
    *,
    1 as batchid
from
    `main`.`tpcdi`.`ProspectRawuno`

 UNION ALL

select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`ProspectRawdos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`ProspectRawtres`

[0m15:54:14.645961 [debug] [Thread-8  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.DimBroker"} */

  
    
        create or replace table `main`.`tpcdi`.`DimBroker`
      
      
    using delta
      
      
      
      
      
      
      as
      

SELECT

  cast(employeeid as BIGINT) brokerid,
  cast(managerid as BIGINT) managerid,
  employeefirstname firstname,
  employeelastname lastname,
  employeemi middleinitial,
  employeebranch branch,
  employeeoffice office,
  employeephone phone,
  true iscurrent,
  1 batchid,
  (SELECT min(to_date(datevalue)) as effectivedate FROM `main`.`tpcdi`.`DimDate`) effectivedate,
  date('9999-12-31') enddate,
  bigint(concat(date_format(enddate, 'yyyyMMdd'), cast(brokerid as string))) as sk_brokerid
FROM  `main`.`tpcdi`.`HR`
WHERE employeejobcode = 314
  
[0m15:54:14.646267 [debug] [Thread-6  ]: On model.dbsql_dbt_tpch.DailyMarketHistorical: ROLLBACK
[0m15:54:14.646615 [debug] [Thread-10 ]: Timing info for model.dbsql_dbt_tpch.HoldingIncremental (execute): 15:54:14.049462 => 15:54:14.646494
[0m15:54:14.646962 [debug] [Thread-12 ]: Timing info for model.dbsql_dbt_tpch.TradeIncremental (execute): 15:54:14.055092 => 15:54:14.646831
[0m15:54:14.647297 [debug] [Thread-4  ]: Timing info for model.dbsql_dbt_tpch.CustomerIncremental (execute): 15:54:14.012731 => 15:54:14.647185
[0m15:54:14.647558 [debug] [Thread-11 ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`ProspectRawuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:54:14.647829 [debug] [Thread-8  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HR` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 33 pos 6
[0m15:54:14.648077 [debug] [Thread-6  ]: Databricks adapter: NotImplemented: rollback
[0m15:54:14.648304 [debug] [Thread-10 ]: On model.dbsql_dbt_tpch.HoldingIncremental: ROLLBACK
[0m15:54:14.648528 [debug] [Thread-12 ]: On model.dbsql_dbt_tpch.TradeIncremental: ROLLBACK
[0m15:54:14.648745 [debug] [Thread-4  ]: On model.dbsql_dbt_tpch.CustomerIncremental: ROLLBACK
[0m15:54:14.649139 [debug] [Thread-11 ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`ProspectRawuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`ProspectRawuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:54:14.649696 [debug] [Thread-8  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HR` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 33 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HR` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 33 pos 6
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:54:14.650081 [debug] [Thread-6  ]: On model.dbsql_dbt_tpch.DailyMarketHistorical: Close
[0m15:54:14.650302 [debug] [Thread-10 ]: Databricks adapter: NotImplemented: rollback
[0m15:54:14.650533 [debug] [Thread-12 ]: Databricks adapter: NotImplemented: rollback
[0m15:54:14.650754 [debug] [Thread-4  ]: Databricks adapter: NotImplemented: rollback
[0m15:54:14.650980 [debug] [Thread-11 ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe3s\x05\x17\r\xb3\x18\x89\x18\xa3\x10\xb5R'
[0m15:54:14.651209 [debug] [Thread-8  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe3r\xfd\x1f\x94\xb4\xf5\xf2\x01\xc1\r\xfd\xa0'
[0m15:54:14.652230 [debug] [Thread-13 ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.WatchIncremental"} */
create or replace view `main`.`tpcdi`.`WatchIncremental`
  
  
  as
    
select
    *,
    2 as batchid
from
    `main`.`tpcdi`.`WatchIncrementaldos`

 UNION ALL

 select
    *,
    3 as batchid
from
    `main`.`tpcdi`.`WatchIncrementaltres`

[0m15:54:14.652715 [debug] [Thread-10 ]: On model.dbsql_dbt_tpch.HoldingIncremental: Close
[0m15:54:14.652979 [debug] [Thread-12 ]: On model.dbsql_dbt_tpch.TradeIncremental: Close
[0m15:54:14.653180 [debug] [Thread-4  ]: On model.dbsql_dbt_tpch.CustomerIncremental: Close
[0m15:54:14.653483 [debug] [Thread-11 ]: Timing info for model.dbsql_dbt_tpch.ProspectRaw (execute): 15:54:14.051858 => 15:54:14.653375
[0m15:54:14.653784 [debug] [Thread-8  ]: Timing info for model.dbsql_dbt_tpch.DimBroker (execute): 15:54:14.020143 => 15:54:14.653678
[0m15:54:14.654008 [debug] [Thread-13 ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`WatchIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:54:14.654857 [debug] [Thread-11 ]: On model.dbsql_dbt_tpch.ProspectRaw: ROLLBACK
[0m15:54:14.655118 [debug] [Thread-8  ]: On model.dbsql_dbt_tpch.DimBroker: ROLLBACK
[0m15:54:14.655488 [debug] [Thread-13 ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`WatchIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`WatchIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:54:14.655846 [debug] [Thread-11 ]: Databricks adapter: NotImplemented: rollback
[0m15:54:14.656041 [debug] [Thread-8  ]: Databricks adapter: NotImplemented: rollback
[0m15:54:14.656244 [debug] [Thread-13 ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe3s\t\x195\x88\xc1M\x1a\x9b\x85i\x83'
[0m15:54:14.656433 [debug] [Thread-11 ]: On model.dbsql_dbt_tpch.ProspectRaw: Close
[0m15:54:14.656620 [debug] [Thread-8  ]: On model.dbsql_dbt_tpch.DimBroker: Close
[0m15:54:14.656923 [debug] [Thread-13 ]: Timing info for model.dbsql_dbt_tpch.WatchIncremental (execute): 15:54:14.057709 => 15:54:14.656814
[0m15:54:14.658880 [debug] [Thread-13 ]: On model.dbsql_dbt_tpch.WatchIncremental: ROLLBACK
[0m15:54:14.659184 [debug] [Thread-13 ]: Databricks adapter: NotImplemented: rollback
[0m15:54:14.659847 [debug] [Thread-3  ]: Runtime Error in model CashTransactionIncremental (models/base/CashTransactionIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`CashTransactionIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:54:14.660089 [debug] [Thread-13 ]: On model.dbsql_dbt_tpch.WatchIncremental: Close
[0m15:54:14.660714 [debug] [Thread-2  ]: Runtime Error in model BatchDate (models/base/BatchDate.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`BatchDateuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:54:14.661004 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e01d10bd-9d53-4e1f-8722-8befaaac6841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15d265520>]}
[0m15:54:14.661371 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e01d10bd-9d53-4e1f-8722-8befaaac6841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15d233eb0>]}
[0m15:54:14.661844 [error] [Thread-3  ]: 3 of 45 ERROR creating sql view model tpcdi.CashTransactionIncremental ......... [[31mERROR[0m in 0.73s]
[0m15:54:14.662228 [error] [Thread-2  ]: 2 of 45 ERROR creating sql view model tpcdi.BatchDate .......................... [[31mERROR[0m in 0.74s]
[0m15:54:14.662552 [debug] [Thread-3  ]: Finished running node model.dbsql_dbt_tpch.CashTransactionIncremental
[0m15:54:14.662782 [debug] [Thread-2  ]: Finished running node model.dbsql_dbt_tpch.BatchDate
[0m15:54:14.664171 [debug] [Thread-5  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.CustomerMgmtView"} */
create or replace view `main`.`tpcdi`.`CustomerMgmtView`
  
  
  as
    
select
    *
from
    hive_metastore.roberto_salcido_tpcdi_stage.customermgmt10

[0m15:54:14.664425 [debug] [Thread-5  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`roberto_salcido_tpcdi_stage`.`customermgmt10` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 10 pos 4
[0m15:54:14.664770 [debug] [Thread-5  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`roberto_salcido_tpcdi_stage`.`customermgmt10` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 10 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`roberto_salcido_tpcdi_stage`.`customermgmt10` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 10 pos 4
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:111)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:633)
	... 21 more

[0m15:54:14.665099 [debug] [Thread-5  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe3s\x00\x1d\xb0\xaf\xc1O\x08k|\xfe\xc7'
[0m15:54:14.665391 [debug] [Thread-5  ]: Timing info for model.dbsql_dbt_tpch.CustomerMgmtView (execute): 15:54:14.010469 => 15:54:14.665284
[0m15:54:14.665592 [debug] [Thread-5  ]: On model.dbsql_dbt_tpch.CustomerMgmtView: ROLLBACK
[0m15:54:14.665777 [debug] [Thread-5  ]: Databricks adapter: NotImplemented: rollback
[0m15:54:14.665946 [debug] [Thread-5  ]: On model.dbsql_dbt_tpch.CustomerMgmtView: Close
[0m15:54:14.694809 [debug] [Thread-7  ]: Runtime Error in model DailyMarketIncremental (models/base/DailyMarketIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:54:14.695251 [debug] [Thread-7  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e01d10bd-9d53-4e1f-8722-8befaaac6841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15d2b2610>]}
[0m15:54:14.695649 [error] [Thread-7  ]: 7 of 45 ERROR creating sql view model tpcdi.DailyMarketIncremental ............. [[31mERROR[0m in 0.77s]
[0m15:54:14.696024 [debug] [Thread-7  ]: Finished running node model.dbsql_dbt_tpch.DailyMarketIncremental
[0m15:54:14.713804 [debug] [Thread-10 ]: Runtime Error in model HoldingIncremental (models/base/HoldingIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HoldingIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:54:14.714187 [debug] [Thread-10 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e01d10bd-9d53-4e1f-8722-8befaaac6841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15d5816a0>]}
[0m15:54:14.714549 [error] [Thread-10 ]: 10 of 45 ERROR creating sql view model tpcdi.HoldingIncremental ................ [[31mERROR[0m in 0.78s]
[0m15:54:14.714874 [debug] [Thread-10 ]: Finished running node model.dbsql_dbt_tpch.HoldingIncremental
[0m15:54:14.721488 [debug] [Thread-4  ]: Runtime Error in model CustomerIncremental (models/base/CustomerIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`customerincrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:54:14.722025 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e01d10bd-9d53-4e1f-8722-8befaaac6841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15d2b20d0>]}
[0m15:54:14.722683 [debug] [Thread-8  ]: Runtime Error in model DimBroker (models/silver/DimBroker.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HR` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 33 pos 6
[0m15:54:14.723125 [error] [Thread-4  ]: 4 of 45 ERROR creating sql view model tpcdi.CustomerIncremental ................ [[31mERROR[0m in 0.79s]
[0m15:54:14.723920 [debug] [Thread-6  ]: Runtime Error in model DailyMarketHistorical (models/base/DailyMarketHistorical.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketHistorical` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:54:14.725222 [debug] [Thread-12 ]: Runtime Error in model TradeIncremental (models/base/TradeIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`TradeIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:54:14.725515 [debug] [Thread-8  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e01d10bd-9d53-4e1f-8722-8befaaac6841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15d2b2190>]}
[0m15:54:14.726107 [debug] [Thread-11 ]: Runtime Error in model ProspectRaw (models/base/ProspectRaw.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`ProspectRawuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:54:14.726432 [debug] [Thread-4  ]: Finished running node model.dbsql_dbt_tpch.CustomerIncremental
[0m15:54:14.727108 [debug] [Thread-13 ]: Runtime Error in model WatchIncremental (models/base/WatchIncremental.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`WatchIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:54:14.727494 [debug] [Thread-6  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e01d10bd-9d53-4e1f-8722-8befaaac6841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15d075a30>]}
[0m15:54:14.727749 [debug] [Thread-12 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e01d10bd-9d53-4e1f-8722-8befaaac6841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15d044760>]}
[0m15:54:14.728176 [error] [Thread-8  ]: 8 of 45 ERROR creating sql table model tpcdi.DimBroker ......................... [[31mERROR[0m in 0.79s]
[0m15:54:14.728584 [debug] [Thread-11 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e01d10bd-9d53-4e1f-8722-8befaaac6841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15d169790>]}
[0m15:54:14.729159 [debug] [Thread-13 ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e01d10bd-9d53-4e1f-8722-8befaaac6841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15d34f790>]}
[0m15:54:14.729653 [error] [Thread-6  ]: 6 of 45 ERROR creating sql view model tpcdi.DailyMarketHistorical .............. [[31mERROR[0m in 0.80s]
[0m15:54:14.730056 [error] [Thread-12 ]: 12 of 45 ERROR creating sql view model tpcdi.TradeIncremental .................. [[31mERROR[0m in 0.79s]
[0m15:54:14.730405 [debug] [Thread-8  ]: Finished running node model.dbsql_dbt_tpch.DimBroker
[0m15:54:14.730714 [error] [Thread-11 ]: 11 of 45 ERROR creating sql view model tpcdi.ProspectRaw ....................... [[31mERROR[0m in 0.80s]
[0m15:54:14.731111 [error] [Thread-13 ]: 13 of 45 ERROR creating sql view model tpcdi.WatchIncremental .................. [[31mERROR[0m in 0.79s]
[0m15:54:14.732153 [debug] [Thread-6  ]: Finished running node model.dbsql_dbt_tpch.DailyMarketHistorical
[0m15:54:14.732500 [debug] [Thread-12 ]: Finished running node model.dbsql_dbt_tpch.TradeIncremental
[0m15:54:14.733287 [debug] [Thread-5  ]: Runtime Error in model CustomerMgmtView (models/base/CustomerMgmtView.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`roberto_salcido_tpcdi_stage`.`customermgmt10` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 10 pos 4
[0m15:54:14.733561 [debug] [Thread-11 ]: Finished running node model.dbsql_dbt_tpch.ProspectRaw
[0m15:54:14.733800 [debug] [Thread-13 ]: Finished running node model.dbsql_dbt_tpch.WatchIncremental
[0m15:54:14.734179 [debug] [Thread-15 ]: Began running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
[0m15:54:14.734519 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e01d10bd-9d53-4e1f-8722-8befaaac6841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15d375910>]}
[0m15:54:14.734920 [info ] [Thread-15 ]: 14 of 45 SKIP relation tpcdi.tempDailyMarketHistorical ......................... [[33mSKIP[0m]
[0m15:54:14.735342 [error] [Thread-5  ]: 5 of 45 ERROR creating sql view model tpcdi.CustomerMgmtView ................... [[31mERROR[0m in 0.81s]
[0m15:54:14.735655 [debug] [Thread-15 ]: Finished running node model.dbsql_dbt_tpch.tempDailyMarketHistorical
[0m15:54:14.735902 [debug] [Thread-5  ]: Finished running node model.dbsql_dbt_tpch.CustomerMgmtView
[0m15:54:14.736289 [debug] [Thread-17 ]: Began running node model.dbsql_dbt_tpch.DimCustomerStg
[0m15:54:14.736517 [info ] [Thread-17 ]: 15 of 45 SKIP relation tpcdi.DimCustomerStg .................................... [[33mSKIP[0m]
[0m15:54:14.736783 [debug] [Thread-17 ]: Finished running node model.dbsql_dbt_tpch.DimCustomerStg
[0m15:54:14.737125 [debug] [Thread-19 ]: Began running node test.dbsql_dbt_tpch.dateval_DimCustomerStg_effectivedate.8a3a23ae4e
[0m15:54:14.737413 [info ] [Thread-19 ]: 16 of 45 SKIP test dateval_DimCustomerStg_effectivedate ........................ [[33mSKIP[0m]
[0m15:54:14.737744 [debug] [Thread-19 ]: Finished running node test.dbsql_dbt_tpch.dateval_DimCustomerStg_effectivedate.8a3a23ae4e
[0m15:54:14.738081 [debug] [Thread-21 ]: Began running node model.dbsql_dbt_tpch.DimAccount
[0m15:54:14.738344 [debug] [Thread-22 ]: Began running node model.dbsql_dbt_tpch.Prospect
[0m15:54:14.738562 [info ] [Thread-21 ]: 17 of 45 SKIP relation tpcdi.DimAccount ........................................ [[33mSKIP[0m]
[0m15:54:14.738848 [info ] [Thread-22 ]: 18 of 45 SKIP relation tpcdi.Prospect .......................................... [[33mSKIP[0m]
[0m15:54:14.739140 [debug] [Thread-21 ]: Finished running node model.dbsql_dbt_tpch.DimAccount
[0m15:54:14.739360 [debug] [Thread-22 ]: Finished running node model.dbsql_dbt_tpch.Prospect
[0m15:54:14.739689 [debug] [Thread-24 ]: Began running node test.dbsql_dbt_tpch.not_null_DimAccount_sk_brokerid.dd6e992726
[0m15:54:14.739913 [debug] [Thread-22 ]: Began running node test.dbsql_dbt_tpch.not_null_DimAccount_sk_customerid.b8baa6ce87
[0m15:54:14.740207 [debug] [Thread-14 ]: Began running node model.dbsql_dbt_tpch.DimCustomer
[0m15:54:14.740461 [info ] [Thread-24 ]: 19 of 45 SKIP test not_null_DimAccount_sk_brokerid ............................. [[33mSKIP[0m]
[0m15:54:14.740759 [info ] [Thread-22 ]: 20 of 45 SKIP test not_null_DimAccount_sk_customerid ........................... [[33mSKIP[0m]
[0m15:54:14.741038 [info ] [Thread-14 ]: 21 of 45 SKIP relation tpcdi.DimCustomer ....................................... [[33mSKIP[0m]
[0m15:54:14.741310 [debug] [Thread-24 ]: Finished running node test.dbsql_dbt_tpch.not_null_DimAccount_sk_brokerid.dd6e992726
[0m15:54:14.741526 [debug] [Thread-22 ]: Finished running node test.dbsql_dbt_tpch.not_null_DimAccount_sk_customerid.b8baa6ce87
[0m15:54:14.741722 [debug] [Thread-14 ]: Finished running node model.dbsql_dbt_tpch.DimCustomer
[0m15:54:14.742123 [debug] [Thread-14 ]: Began running node model.dbsql_dbt_tpch.FactCashBalances
[0m15:54:14.742424 [info ] [Thread-14 ]: 22 of 45 SKIP relation tpcdi.FactCashBalances .................................. [[33mSKIP[0m]
[0m15:54:14.742726 [debug] [Thread-2  ]: Began running node test.dbsql_dbt_tpch.accepted_values_DimCustomer_tier__1__2__3.7be85b3b7a
[0m15:54:14.742943 [debug] [Thread-7  ]: Began running node test.dbsql_dbt_tpch.not_null_DimCustomer_tier.e0964b5cac
[0m15:54:14.743137 [debug] [Thread-14 ]: Finished running node model.dbsql_dbt_tpch.FactCashBalances
[0m15:54:14.743360 [info ] [Thread-2  ]: 23 of 45 SKIP test accepted_values_DimCustomer_tier__1__2__3 ................... [[33mSKIP[0m]
[0m15:54:14.743659 [info ] [Thread-7  ]: 24 of 45 SKIP test not_null_DimCustomer_tier ................................... [[33mSKIP[0m]
[0m15:54:14.744080 [debug] [Thread-4  ]: Began running node test.dbsql_dbt_tpch.not_null_FactCashBalances_sk_accountid.878cf23033
[0m15:54:14.744289 [debug] [Thread-2  ]: Finished running node test.dbsql_dbt_tpch.accepted_values_DimCustomer_tier__1__2__3.7be85b3b7a
[0m15:54:14.744506 [debug] [Thread-7  ]: Finished running node test.dbsql_dbt_tpch.not_null_DimCustomer_tier.e0964b5cac
[0m15:54:14.744723 [info ] [Thread-4  ]: 25 of 45 SKIP test not_null_FactCashBalances_sk_accountid ...................... [[33mSKIP[0m]
[0m15:54:14.745273 [debug] [Thread-4  ]: Finished running node test.dbsql_dbt_tpch.not_null_FactCashBalances_sk_accountid.878cf23033
[0m15:54:14.834727 [debug] [Thread-9  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "node_id": "model.dbsql_dbt_tpch.FinWire"} */
create or replace view `main`.`tpcdi`.`FinWire`
  
  
  as
    

select *, substring(value, 16, 3) rectype from 
text.`dbfs:/tmp/tpcdi/sf=10/Batch1/FINWIRE*`

[0m15:54:14.835244 [debug] [Thread-9  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: text; line 9 pos 0
[0m15:54:14.836141 [debug] [Thread-9  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: text; line 9 pos 0
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: text; line 9 pos 0
	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:76)
	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:171)
	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:93)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1306)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1305)
	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:81)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1335)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1332)
	at org.apache.spark.sql.catalyst.plans.logical.CreateView.mapChildren(v2Commands.scala:1350)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:102)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:79)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:78)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:93)
	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:51)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:229)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:229)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:226)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:218)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:296)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:296)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:296)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:383)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:376)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:283)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:376)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:304)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:379)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:355)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:156)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:348)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:380)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:817)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:380)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1040)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:377)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:150)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:140)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getRuntimeCategory$1(QueryRuntimePrediction.scala:300)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1040)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:299)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:349)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:345)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more
Caused by: org.apache.spark.sql.AnalysisException: [PATH_NOT_FOUND] Path does not exist: dbfs:/tmp/tpcdi/sf=10/Batch1/FINWIRE*.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:1592)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:844)
	at scala.collection.immutable.List.flatMap(List.scala:366)
	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:831)
	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:616)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:451)
	at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:160)
	... 95 more

[0m15:54:14.837057 [debug] [Thread-9  ]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe3s\x05\x1b\x96\x9f\xf8K\x05\xf8\xdb=\x1d'
[0m15:54:14.837470 [debug] [Thread-9  ]: Timing info for model.dbsql_dbt_tpch.FinWire (execute): 15:54:14.046616 => 15:54:14.837335
[0m15:54:14.837745 [debug] [Thread-9  ]: On model.dbsql_dbt_tpch.FinWire: ROLLBACK
[0m15:54:14.837978 [debug] [Thread-9  ]: Databricks adapter: NotImplemented: rollback
[0m15:54:14.838189 [debug] [Thread-9  ]: On model.dbsql_dbt_tpch.FinWire: Close
[0m15:54:14.901655 [debug] [Thread-9  ]: Runtime Error in model FinWire (models/base/FinWire.sql)
  [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: text; line 9 pos 0
[0m15:54:14.902104 [debug] [Thread-9  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e01d10bd-9d53-4e1f-8722-8befaaac6841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15d0abfd0>]}
[0m15:54:14.902675 [error] [Thread-9  ]: 9 of 45 ERROR creating sql view model tpcdi.FinWire ............................ [[31mERROR[0m in 0.97s]
[0m15:54:14.903135 [debug] [Thread-9  ]: Finished running node model.dbsql_dbt_tpch.FinWire
[0m15:54:14.903908 [debug] [Thread-6  ]: Began running node model.dbsql_dbt_tpch.DimCompany
[0m15:54:14.904281 [info ] [Thread-6  ]: 26 of 45 SKIP relation tpcdi.DimCompany ........................................ [[33mSKIP[0m]
[0m15:54:14.904814 [debug] [Thread-6  ]: Finished running node model.dbsql_dbt_tpch.DimCompany
[0m15:54:14.905521 [debug] [Thread-16 ]: Began running node test.dbsql_dbt_tpch.dateval_DimCompany_effectivedate.4df1621a42
[0m15:54:14.905933 [info ] [Thread-16 ]: 27 of 45 SKIP test dateval_DimCompany_effectivedate ............................ [[33mSKIP[0m]
[0m15:54:14.906373 [debug] [Thread-16 ]: Finished running node test.dbsql_dbt_tpch.dateval_DimCompany_effectivedate.4df1621a42
[0m15:54:14.906948 [debug] [Thread-13 ]: Began running node model.dbsql_dbt_tpch.DimSecurity
[0m15:54:14.907311 [info ] [Thread-13 ]: 28 of 45 SKIP relation tpcdi.DimSecurity ....................................... [[33mSKIP[0m]
[0m15:54:14.907841 [debug] [Thread-13 ]: Finished running node model.dbsql_dbt_tpch.DimSecurity
[0m15:54:14.908174 [debug] [Thread-15 ]: Began running node model.dbsql_dbt_tpch.Financial
[0m15:54:14.908686 [debug] [Thread-18 ]: Began running node test.dbsql_dbt_tpch.not_null_DimSecurity_sk_companyid.e728df82fe
[0m15:54:14.909044 [info ] [Thread-15 ]: 29 of 45 SKIP relation tpcdi.Financial ......................................... [[33mSKIP[0m]
[0m15:54:14.909636 [info ] [Thread-18 ]: 30 of 45 SKIP test not_null_DimSecurity_sk_companyid ........................... [[33mSKIP[0m]
[0m15:54:14.910149 [debug] [Thread-15 ]: Finished running node model.dbsql_dbt_tpch.Financial
[0m15:54:14.910571 [debug] [Thread-18 ]: Finished running node test.dbsql_dbt_tpch.not_null_DimSecurity_sk_companyid.e728df82fe
[0m15:54:14.911107 [debug] [Thread-20 ]: Began running node test.dbsql_dbt_tpch.not_null_Financial_sk_companyid.529c7c2c12
[0m15:54:14.911567 [info ] [Thread-20 ]: 31 of 45 SKIP test not_null_Financial_sk_companyid ............................. [[33mSKIP[0m]
[0m15:54:14.912014 [debug] [Thread-23 ]: Began running node model.dbsql_dbt_tpch.DimTrade
[0m15:54:14.912372 [debug] [Thread-20 ]: Finished running node test.dbsql_dbt_tpch.not_null_Financial_sk_companyid.529c7c2c12
[0m15:54:14.912627 [debug] [Thread-21 ]: Began running node model.dbsql_dbt_tpch.FactWatches
[0m15:54:14.912908 [info ] [Thread-23 ]: 32 of 45 SKIP relation tpcdi.DimTrade .......................................... [[33mSKIP[0m]
[0m15:54:14.913454 [debug] [Thread-1  ]: Began running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
[0m15:54:14.913791 [info ] [Thread-21 ]: 33 of 45 SKIP relation tpcdi.FactWatches ....................................... [[33mSKIP[0m]
[0m15:54:14.914169 [debug] [Thread-23 ]: Finished running node model.dbsql_dbt_tpch.DimTrade
[0m15:54:14.914448 [info ] [Thread-1  ]: 34 of 45 SKIP relation tpcdi.tempSumpFiBasicEps ................................ [[33mSKIP[0m]
[0m15:54:14.914796 [debug] [Thread-21 ]: Finished running node model.dbsql_dbt_tpch.FactWatches
[0m15:54:14.915277 [debug] [Thread-22 ]: Began running node test.dbsql_dbt_tpch.not_null_DimTrade_sk_accountid.4db26dca2c
[0m15:54:14.915542 [debug] [Thread-1  ]: Finished running node model.dbsql_dbt_tpch.tempSumpFiBasicEps
[0m15:54:14.915790 [debug] [Thread-3  ]: Began running node test.dbsql_dbt_tpch.not_null_DimTrade_sk_securityid.13bd8f9da6
[0m15:54:14.916066 [debug] [Thread-10 ]: Began running node test.dbsql_dbt_tpch.tradecom_DimTrade_commission.ad38ac2d42
[0m15:54:14.916369 [debug] [Thread-14 ]: Began running node test.dbsql_dbt_tpch.tradefee_DimTrade_fee.5ee05431ee
[0m15:54:14.916832 [info ] [Thread-22 ]: 35 of 45 SKIP test not_null_DimTrade_sk_accountid .............................. [[33mSKIP[0m]
[0m15:54:14.917445 [debug] [Thread-2  ]: Began running node test.dbsql_dbt_tpch.not_null_FactWatches_sk_customerid.1c7356b8c9
[0m15:54:14.917827 [debug] [Thread-7  ]: Began running node test.dbsql_dbt_tpch.not_null_FactWatches_sk_securityid.8a6b5e97b3
[0m15:54:14.918160 [info ] [Thread-3  ]: 36 of 45 SKIP test not_null_DimTrade_sk_securityid ............................. [[33mSKIP[0m]
[0m15:54:14.918557 [debug] [Thread-4  ]: Began running node model.dbsql_dbt_tpch.FactMarketHistory
[0m15:54:14.918889 [info ] [Thread-10 ]: 37 of 45 SKIP test tradecom_DimTrade_commission ................................ [[33mSKIP[0m]
[0m15:54:14.919243 [info ] [Thread-14 ]: 38 of 45 SKIP test tradefee_DimTrade_fee ....................................... [[33mSKIP[0m]
[0m15:54:14.919591 [debug] [Thread-22 ]: Finished running node test.dbsql_dbt_tpch.not_null_DimTrade_sk_accountid.4db26dca2c
[0m15:54:14.919852 [info ] [Thread-2  ]: 39 of 45 SKIP test not_null_FactWatches_sk_customerid .......................... [[33mSKIP[0m]
[0m15:54:14.920186 [info ] [Thread-7  ]: 40 of 45 SKIP test not_null_FactWatches_sk_securityid .......................... [[33mSKIP[0m]
[0m15:54:14.920518 [debug] [Thread-3  ]: Finished running node test.dbsql_dbt_tpch.not_null_DimTrade_sk_securityid.13bd8f9da6
[0m15:54:14.920780 [info ] [Thread-4  ]: 41 of 45 SKIP relation tpcdi.FactMarketHistory ................................. [[33mSKIP[0m]
[0m15:54:14.921263 [debug] [Thread-10 ]: Finished running node test.dbsql_dbt_tpch.tradecom_DimTrade_commission.ad38ac2d42
[0m15:54:14.921670 [debug] [Thread-14 ]: Finished running node test.dbsql_dbt_tpch.tradefee_DimTrade_fee.5ee05431ee
[0m15:54:14.922092 [debug] [Thread-2  ]: Finished running node test.dbsql_dbt_tpch.not_null_FactWatches_sk_customerid.1c7356b8c9
[0m15:54:14.922380 [debug] [Thread-7  ]: Finished running node test.dbsql_dbt_tpch.not_null_FactWatches_sk_securityid.8a6b5e97b3
[0m15:54:14.922724 [debug] [Thread-4  ]: Finished running node model.dbsql_dbt_tpch.FactMarketHistory
[0m15:54:14.923208 [debug] [Thread-2  ]: Began running node model.dbsql_dbt_tpch.FactHoldings
[0m15:54:14.923730 [info ] [Thread-2  ]: 42 of 45 SKIP relation tpcdi.FactHoldings ...................................... [[33mSKIP[0m]
[0m15:54:14.924072 [debug] [Thread-6  ]: Began running node test.dbsql_dbt_tpch.not_null_FactMarketHistory_PERatio.83761a27d0
[0m15:54:14.924338 [debug] [Thread-11 ]: Began running node test.dbsql_dbt_tpch.not_null_FactMarketHistory_sk_securityid.16a0e9b8b2
[0m15:54:14.924615 [debug] [Thread-2  ]: Finished running node model.dbsql_dbt_tpch.FactHoldings
[0m15:54:14.924897 [info ] [Thread-6  ]: 43 of 45 SKIP test not_null_FactMarketHistory_PERatio .......................... [[33mSKIP[0m]
[0m15:54:14.925233 [info ] [Thread-11 ]: 44 of 45 SKIP test not_null_FactMarketHistory_sk_securityid .................... [[33mSKIP[0m]
[0m15:54:14.925638 [debug] [Thread-5  ]: Began running node test.dbsql_dbt_tpch.not_null_FactHoldings_currentprice.98a44eac00
[0m15:54:14.925886 [debug] [Thread-6  ]: Finished running node test.dbsql_dbt_tpch.not_null_FactMarketHistory_PERatio.83761a27d0
[0m15:54:14.926115 [debug] [Thread-11 ]: Finished running node test.dbsql_dbt_tpch.not_null_FactMarketHistory_sk_securityid.16a0e9b8b2
[0m15:54:14.926347 [info ] [Thread-5  ]: 45 of 45 SKIP test not_null_FactHoldings_currentprice .......................... [[33mSKIP[0m]
[0m15:54:14.926833 [debug] [Thread-5  ]: Finished running node test.dbsql_dbt_tpch.not_null_FactHoldings_currentprice.98a44eac00
[0m15:54:14.928529 [debug] [MainThread]: On master: ROLLBACK
[0m15:54:14.928780 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:54:15.112278 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:54:15.113139 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:54:15.113571 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:54:15.113974 [debug] [MainThread]: On master: ROLLBACK
[0m15:54:15.114352 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:54:15.114717 [debug] [MainThread]: On master: Close
[0m15:54:15.175598 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:54:15.177018 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.AccountIncremental' was properly closed.
[0m15:54:15.177771 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.BatchDate' was properly closed.
[0m15:54:15.178498 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.CashTransactionIncremental' was properly closed.
[0m15:54:15.179211 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.CustomerIncremental' was properly closed.
[0m15:54:15.179920 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.CustomerMgmtView' was properly closed.
[0m15:54:15.180794 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.DailyMarketHistorical' was properly closed.
[0m15:54:15.181626 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.DailyMarketIncremental' was properly closed.
[0m15:54:15.182386 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.DimBroker' was properly closed.
[0m15:54:15.183048 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.FinWire' was properly closed.
[0m15:54:15.183390 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.HoldingIncremental' was properly closed.
[0m15:54:15.183930 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.ProspectRaw' was properly closed.
[0m15:54:15.184270 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.TradeIncremental' was properly closed.
[0m15:54:15.184610 [debug] [MainThread]: Connection 'model.dbsql_dbt_tpch.WatchIncremental' was properly closed.
[0m15:54:15.190390 [info ] [MainThread]: 
[0m15:54:15.191027 [info ] [MainThread]: Finished running 12 view models, 15 table models, 18 tests in 0 hours 0 minutes and 2.97 seconds (2.97s).
[0m15:54:15.193708 [debug] [MainThread]: Command end result
[0m15:54:15.246347 [info ] [MainThread]: 
[0m15:54:15.246688 [info ] [MainThread]: [31mCompleted with 13 errors and 0 warnings:[0m
[0m15:54:15.246906 [info ] [MainThread]: 
[0m15:54:15.247111 [error] [MainThread]: [33mRuntime Error in model AccountIncremental (models/base/AccountIncremental.sql)[0m
[0m15:54:15.247318 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`AccountIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:54:15.247520 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:54:15.247711 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:54:15.247898 [info ] [MainThread]: 
[0m15:54:15.248088 [error] [MainThread]: [33mRuntime Error in model CashTransactionIncremental (models/base/CashTransactionIncremental.sql)[0m
[0m15:54:15.248282 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`CashTransactionIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:54:15.248470 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:54:15.248657 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:54:15.248848 [info ] [MainThread]: 
[0m15:54:15.249035 [error] [MainThread]: [33mRuntime Error in model BatchDate (models/base/BatchDate.sql)[0m
[0m15:54:15.249222 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`BatchDateuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:54:15.249414 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:54:15.249600 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:54:15.249785 [info ] [MainThread]: 
[0m15:54:15.249968 [error] [MainThread]: [33mRuntime Error in model DailyMarketIncremental (models/base/DailyMarketIncremental.sql)[0m
[0m15:54:15.250156 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:54:15.250343 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:54:15.250544 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:54:15.250729 [info ] [MainThread]: 
[0m15:54:15.250921 [error] [MainThread]: [33mRuntime Error in model HoldingIncremental (models/base/HoldingIncremental.sql)[0m
[0m15:54:15.251106 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HoldingIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:54:15.251315 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:54:15.251502 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:54:15.251691 [info ] [MainThread]: 
[0m15:54:15.251872 [error] [MainThread]: [33mRuntime Error in model CustomerIncremental (models/base/CustomerIncremental.sql)[0m
[0m15:54:15.252059 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`customerincrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:54:15.252247 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:54:15.252429 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:54:15.252629 [info ] [MainThread]: 
[0m15:54:15.252811 [error] [MainThread]: [33mRuntime Error in model DimBroker (models/silver/DimBroker.sql)[0m
[0m15:54:15.253005 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`HR` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:54:15.253200 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:54:15.253391 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 33 pos 6
[0m15:54:15.253601 [info ] [MainThread]: 
[0m15:54:15.253784 [error] [MainThread]: [33mRuntime Error in model DailyMarketHistorical (models/base/DailyMarketHistorical.sql)[0m
[0m15:54:15.253988 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`DailyMarketHistorical` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:54:15.254182 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:54:15.254371 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:54:15.254587 [info ] [MainThread]: 
[0m15:54:15.254804 [error] [MainThread]: [33mRuntime Error in model TradeIncremental (models/base/TradeIncremental.sql)[0m
[0m15:54:15.255009 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`TradeIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:54:15.255216 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:54:15.255417 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:54:15.255619 [info ] [MainThread]: 
[0m15:54:15.255823 [error] [MainThread]: [33mRuntime Error in model ProspectRaw (models/base/ProspectRaw.sql)[0m
[0m15:54:15.256022 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`ProspectRawuno` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:54:15.256215 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:54:15.256411 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 12 pos 4
[0m15:54:15.256607 [info ] [MainThread]: 
[0m15:54:15.256808 [error] [MainThread]: [33mRuntime Error in model WatchIncremental (models/base/WatchIncremental.sql)[0m
[0m15:54:15.257100 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `main`.`tpcdi`.`WatchIncrementaldos` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:54:15.257357 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:54:15.257578 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 11 pos 4
[0m15:54:15.257788 [info ] [MainThread]: 
[0m15:54:15.258002 [error] [MainThread]: [33mRuntime Error in model CustomerMgmtView (models/base/CustomerMgmtView.sql)[0m
[0m15:54:15.258204 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`roberto_salcido_tpcdi_stage`.`customermgmt10` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m15:54:15.258408 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m15:54:15.258610 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 10 pos 4
[0m15:54:15.258809 [info ] [MainThread]: 
[0m15:54:15.259008 [error] [MainThread]: [33mRuntime Error in model FinWire (models/base/FinWire.sql)[0m
[0m15:54:15.259202 [error] [MainThread]:   [UNSUPPORTED_DATASOURCE_FOR_DIRECT_QUERY] Unsupported data source type for direct query on files: text; line 9 pos 0
[0m15:54:15.259431 [info ] [MainThread]: 
[0m15:54:15.259649 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=13 SKIP=32 TOTAL=45
[0m15:54:15.260319 [debug] [MainThread]: Command `dbt build` failed at 15:54:15.260248 after 5.45 seconds
[0m15:54:15.260562 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1036a4b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15cf55a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15d0ef190>]}
[0m15:54:15.260800 [debug] [MainThread]: Flushing usage events
[0m15:56:53.414538 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102758220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1048cfe80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1048cfe20>]}


============================== 15:56:53.437500 | a0b84c66-ff65-403f-ab57-455bc14da53f ==============================
[0m15:56:53.437500 [info ] [MainThread]: Running with dbt=1.5.0
[0m15:56:53.437873 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:56:53.455672 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a0b84c66-ff65-403f-ab57-455bc14da53f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100694c10>]}
[0m15:56:53.457589 [debug] [MainThread]: Set downloads directory='/var/folders/94/_5czc1mn6cz2scfnd_7y6_6w0000gp/T/dbt-downloads-l3z4rv77'
[0m15:56:53.457904 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m15:56:53.690407 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m15:56:53.692571 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m15:56:53.852617 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m15:56:53.862614 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/spark_utils.json
[0m15:56:54.027649 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/spark_utils.json 200
[0m15:56:54.033465 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_external_tables.json
[0m15:56:54.224339 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_external_tables.json 200
[0m15:56:54.238724 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m15:56:54.711732 [info ] [MainThread]: Installed from version 0.8.0
[0m15:56:54.712111 [info ] [MainThread]: Updated version available: 1.1.1
[0m15:56:54.712392 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'a0b84c66-ff65-403f-ab57-455bc14da53f', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1049556a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104955040>]}
[0m15:56:54.712644 [info ] [MainThread]: Installing dbt-labs/spark_utils
[0m15:56:54.962608 [info ] [MainThread]: Installed from version 0.3.0
[0m15:56:54.962993 [info ] [MainThread]: Up to date!
[0m15:56:54.963278 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'a0b84c66-ff65-403f-ab57-455bc14da53f', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104955eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104955e20>]}
[0m15:56:54.963514 [info ] [MainThread]: Installing dbt-labs/dbt_external_tables
[0m15:56:55.399509 [info ] [MainThread]: Installed from version 0.8.2
[0m15:56:55.399893 [info ] [MainThread]: Updated version available: 0.8.5
[0m15:56:55.400385 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'a0b84c66-ff65-403f-ab57-455bc14da53f', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104955a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1049874c0>]}
[0m15:56:55.400711 [info ] [MainThread]: 
[0m15:56:55.400969 [info ] [MainThread]: Updates available for packages: ['dbt-labs/dbt_utils', 'dbt-labs/dbt_external_tables']                 
Update your versions in packages.yml, then run dbt deps
[0m15:56:55.403076 [debug] [MainThread]: Command `dbt deps` succeeded at 15:56:55.402959 after 2.01 seconds
[0m15:56:55.403413 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102758220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1005477c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100694c10>]}
[0m15:56:55.403679 [debug] [MainThread]: Flushing usage events
[0m15:57:09.624909 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107524220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110d4fdf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110d4fe80>]}


============================== 15:57:09.646895 | f4821275-8a80-47b1-a42a-70b6264bcc89 ==============================
[0m15:57:09.646895 [info ] [MainThread]: Running with dbt=1.5.0
[0m15:57:09.647269 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi', 'log_path': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:57:10.526390 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f4821275-8a80-47b1-a42a-70b6264bcc89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10520e550>]}
[0m15:57:10.536116 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f4821275-8a80-47b1-a42a-70b6264bcc89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x140246400>]}
[0m15:57:10.565989 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m15:57:10.644664 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:57:10.644975 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:57:10.652313 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f4821275-8a80-47b1-a42a-70b6264bcc89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x140753970>]}
[0m15:57:10.705713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f4821275-8a80-47b1-a42a-70b6264bcc89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1400b3fa0>]}
[0m15:57:10.706104 [info ] [MainThread]: Found 27 models, 18 tests, 0 snapshots, 0 analyses, 681 macros, 0 operations, 0 seed files, 33 sources, 0 exposures, 0 metrics, 0 groups
[0m15:57:10.706358 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f4821275-8a80-47b1-a42a-70b6264bcc89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1401ce580>]}
[0m15:57:10.706843 [debug] [MainThread]: Acquiring new databricks connection 'macro_stage_external_sources'
[0m15:57:10.707044 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:57:10.707203 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:57:10.719069 [info ] [MainThread]: 1 of 33 START external source tpcdi.StatusType
[0m15:57:10.723345 [debug] [MainThread]: On "macro_stage_external_sources": cache miss for schema ".tpcdi", this is inefficient
[0m15:57:10.733567 [debug] [MainThread]: Using databricks connection "macro_stage_external_sources"
[0m15:57:10.733949 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */
show tables in `tpcdi`
  
[0m15:57:10.734152 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:57:11.395433 [debug] [MainThread]: SQL status: OK in 0.6600000262260437 seconds
[0m15:57:11.406375 [debug] [MainThread]: While listing relations in database=, schema=tpcdi, found: 
[0m15:57:11.416890 [info ] [MainThread]: 1 of 33 (1) drop table if exists `main`.`tpcdi`.`StatusType`
[0m15:57:11.418709 [debug] [MainThread]: Using databricks connection "macro_stage_external_sources"
[0m15:57:11.418952 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
        drop table if exists `main`.`tpcdi`.`StatusType`
    

            
[0m15:57:11.537531 [debug] [MainThread]: SQL status: OK in 0.11999999731779099 seconds
[0m15:57:11.539819 [info ] [MainThread]: 1 of 33 (1) OK
[0m15:57:11.540443 [info ] [MainThread]: 1 of 33 (2) create table `main`.`tpcdi`.`StatusType` (                    st_id string,     ...  
[0m15:57:11.541618 [debug] [MainThread]: Using databricks connection "macro_stage_external_sources"
[0m15:57:11.542043 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table `main`.`tpcdi`.`StatusType` (
        
            st_id string,
            st_name string
    )  using csv
    options ('sep' = '|', 
'header' = 'false')
    
    
    
    location '/FileStore/tables/Batch1/StatusType.txt'
    

            
[0m15:57:11.682760 [debug] [MainThread]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table `main`.`tpcdi`.`StatusType` (
        
            st_id string,
            st_name string
    )  using csv
    options ('sep' = '|', 
'header' = 'false')
    
    
    
    location '/FileStore/tables/Batch1/StatusType.txt'
    

            
[0m15:57:11.684601 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [RequestId=673d59ab-d932-4021-8c24-7ad5b087e2cc ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme
[0m15:57:11.686196 [debug] [MainThread]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: com.databricks.sql.managedcatalog.UnityCatalogServiceException: [RequestId=673d59ab-d932-4021-8c24-7ad5b087e2cc ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.databricks.sql.managedcatalog.UnityCatalogServiceException: [RequestId=673d59ab-d932-4021-8c24-7ad5b087e2cc ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:33)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:23)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:105)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:3282)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.generateTemporaryPathCredentials(ManagedCatalogClientImpl.scala:3000)
	at com.databricks.sql.managedcatalog.ManagedCatalogCommon.generateTemporaryPathCredentials(ManagedCatalogCommon.scala:938)
	at com.databricks.unity.CredentialScopeSQLHelper$.checkPathOperations(CredentialScopeSQLHelper.scala:67)
	at com.databricks.unity.CredentialScopeSQLHelper$.register(CredentialScopeSQLHelper.scala:107)
	at com.databricks.unity.CredentialScopeSQLHelper$.registerCreateTableAccess(CredentialScopeSQLHelper.scala:277)
	at com.databricks.sql.managedcatalog.CredentialScopeTableCredentialHandler.injectCredential(ResolveWithCredential.scala:466)
	at com.databricks.sql.managedcatalog.ResolveWithCredential.com$databricks$sql$managedcatalog$ResolveWithCredential$$maybeDecorateCatalogTable(ResolveWithCredential.scala:75)
	at com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:148)
	at com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:146)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:209)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:208)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:32)
	at com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:146)
	at com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:61)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:229)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:229)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:226)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:218)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:296)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:296)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:296)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:383)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:376)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:283)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:376)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:304)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:379)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:355)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:156)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:348)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:380)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:817)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:380)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1040)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:377)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:150)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:140)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getRuntimeCategory$1(QueryRuntimePrediction.scala:300)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1040)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:299)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:349)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:345)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more

[0m15:57:11.687416 [debug] [MainThread]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe3\xdc\x95\x15\x8b\xb9\x88\xf1*\x97\xfa\xde\xa1'
[0m15:57:11.687859 [debug] [MainThread]: Databricks adapter: Error while running:
macro stage_external_sources
[0m15:57:11.688166 [debug] [MainThread]: Databricks adapter: Runtime Error
  [RequestId=673d59ab-d932-4021-8c24-7ad5b087e2cc ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme
[0m15:57:11.688596 [debug] [MainThread]: On macro_stage_external_sources: ROLLBACK
[0m15:57:11.688890 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:57:11.689154 [debug] [MainThread]: On macro_stage_external_sources: Close
[0m15:57:11.754427 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  Runtime Error
    [RequestId=673d59ab-d932-4021-8c24-7ad5b087e2cc ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme
[0m15:57:11.765350 [debug] [MainThread]: Traceback (most recent call last):
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 660, in exception_handler
    yield
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 705, in add_query
    cursor.execute(sql, bindings)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 477, in execute
    self._cursor.execute(sql, bindings)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 852, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 944, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 786, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 503, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: [RequestId=673d59ab-d932-4021-8c24-7ad5b087e2cc ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 660, in exception_handler
    yield
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 1043, in execute_macro
    result = macro_function(**kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 220, in macro
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 298, in call
    return __obj(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 52, in macro
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 298, in call
    return __obj(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/impl.py", line 131, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 289, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 728, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 722, in add_query
    cursor.close()
  File "/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 665, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  [RequestId=673d59ab-d932-4021-8c24-7ad5b087e2cc ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/task/run_operation.py", line 47, in run
    self._run_unsafe()
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/task/run_operation.py", line 37, in _run_unsafe
    res = adapter.execute_macro(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 1043, in execute_macro
    result = macro_function(**kwargs)
  File "/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 678, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  Runtime Error
    [RequestId=673d59ab-d932-4021-8c24-7ad5b087e2cc ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme

[0m15:57:11.767270 [debug] [MainThread]: Command `dbt run-operation` failed at 15:57:11.767124 after 2.17 seconds
[0m15:57:11.767662 [debug] [MainThread]: Connection 'macro_stage_external_sources' was properly closed.
[0m15:57:11.768008 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107524220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1407976d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1400b32e0>]}
[0m15:57:11.768393 [debug] [MainThread]: Flushing usage events
[0m16:03:12.440982 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106624700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110f4edc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110f4ed60>]}


============================== 16:03:12.462975 | a1bdaead-c1c3-4ff8-aecf-54d50883561f ==============================
[0m16:03:12.462975 [info ] [MainThread]: Running with dbt=1.5.0
[0m16:03:12.463401 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m16:03:13.356053 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a1bdaead-c1c3-4ff8-aecf-54d50883561f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1042e37c0>]}
[0m16:03:13.365695 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a1bdaead-c1c3-4ff8-aecf-54d50883561f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1330c6370>]}
[0m16:03:13.394919 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m16:03:13.467602 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:03:13.467905 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:03:13.475518 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a1bdaead-c1c3-4ff8-aecf-54d50883561f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1335d40d0>]}
[0m16:03:13.527513 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a1bdaead-c1c3-4ff8-aecf-54d50883561f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x132e348b0>]}
[0m16:03:13.527901 [info ] [MainThread]: Found 27 models, 18 tests, 0 snapshots, 0 analyses, 681 macros, 0 operations, 0 seed files, 33 sources, 0 exposures, 0 metrics, 0 groups
[0m16:03:13.528157 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a1bdaead-c1c3-4ff8-aecf-54d50883561f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106928b50>]}
[0m16:03:13.528643 [debug] [MainThread]: Acquiring new databricks connection 'macro_stage_external_sources'
[0m16:03:13.528844 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:03:13.528998 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:03:13.540978 [info ] [MainThread]: 1 of 33 START external source tpcdi.StatusType
[0m16:03:13.545360 [debug] [MainThread]: On "macro_stage_external_sources": cache miss for schema ".tpcdi", this is inefficient
[0m16:03:13.555591 [debug] [MainThread]: Using databricks connection "macro_stage_external_sources"
[0m16:03:13.555918 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */
show tables in `tpcdi`
  
[0m16:03:13.556101 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:03:14.100237 [debug] [MainThread]: SQL status: OK in 0.5400000214576721 seconds
[0m16:03:14.113115 [debug] [MainThread]: While listing relations in database=, schema=tpcdi, found: 
[0m16:03:14.127520 [info ] [MainThread]: 1 of 33 (1) drop table if exists `main`.`tpcdi`.`StatusType`
[0m16:03:14.129614 [debug] [MainThread]: Using databricks connection "macro_stage_external_sources"
[0m16:03:14.129905 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
        drop table if exists `main`.`tpcdi`.`StatusType`
    

            
[0m16:03:14.242932 [debug] [MainThread]: SQL status: OK in 0.10999999940395355 seconds
[0m16:03:14.244113 [info ] [MainThread]: 1 of 33 (1) OK
[0m16:03:14.244423 [info ] [MainThread]: 1 of 33 (2) create table `main`.`tpcdi`.`StatusType` (                    st_id string,     ...  
[0m16:03:14.245016 [debug] [MainThread]: Using databricks connection "macro_stage_external_sources"
[0m16:03:14.245234 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table `main`.`tpcdi`.`StatusType` (
        
            st_id string,
            st_name string
    )  using csv
    options ('sep' = '|', 
'header' = 'false')
    
    
    
    location '/FileStore/tables/Batch1/StatusType.txt'
    

            
[0m16:03:14.371425 [debug] [MainThread]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table `main`.`tpcdi`.`StatusType` (
        
            st_id string,
            st_name string
    )  using csv
    options ('sep' = '|', 
'header' = 'false')
    
    
    
    location '/FileStore/tables/Batch1/StatusType.txt'
    

            
[0m16:03:14.372237 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [RequestId=e051b279-75c3-9e49-a036-dd83aee440ed ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme
[0m16:03:14.373344 [debug] [MainThread]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: com.databricks.sql.managedcatalog.UnityCatalogServiceException: [RequestId=e051b279-75c3-9e49-a036-dd83aee440ed ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.databricks.sql.managedcatalog.UnityCatalogServiceException: [RequestId=e051b279-75c3-9e49-a036-dd83aee440ed ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:33)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:23)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:105)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:3282)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.generateTemporaryPathCredentials(ManagedCatalogClientImpl.scala:3000)
	at com.databricks.sql.managedcatalog.ManagedCatalogCommon.generateTemporaryPathCredentials(ManagedCatalogCommon.scala:938)
	at com.databricks.unity.CredentialScopeSQLHelper$.checkPathOperations(CredentialScopeSQLHelper.scala:67)
	at com.databricks.unity.CredentialScopeSQLHelper$.register(CredentialScopeSQLHelper.scala:107)
	at com.databricks.unity.CredentialScopeSQLHelper$.registerCreateTableAccess(CredentialScopeSQLHelper.scala:277)
	at com.databricks.sql.managedcatalog.CredentialScopeTableCredentialHandler.injectCredential(ResolveWithCredential.scala:466)
	at com.databricks.sql.managedcatalog.ResolveWithCredential.com$databricks$sql$managedcatalog$ResolveWithCredential$$maybeDecorateCatalogTable(ResolveWithCredential.scala:75)
	at com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:148)
	at com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:146)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:209)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:208)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:32)
	at com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:146)
	at com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:61)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:229)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:229)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:226)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:218)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:296)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:296)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:296)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:383)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:376)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:283)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:376)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:304)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:379)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:355)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:156)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:348)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:380)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:817)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:380)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1040)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:377)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:150)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:140)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getRuntimeCategory$1(QueryRuntimePrediction.scala:300)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1040)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:299)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:349)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:345)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more

[0m16:03:14.374542 [debug] [MainThread]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe4\xb4\xc6\x15\xad\xb6X<\xe7\x80\x855\xac'
[0m16:03:14.375074 [debug] [MainThread]: Databricks adapter: Error while running:
macro stage_external_sources
[0m16:03:14.375629 [debug] [MainThread]: Databricks adapter: Runtime Error
  [RequestId=e051b279-75c3-9e49-a036-dd83aee440ed ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme
[0m16:03:14.376412 [debug] [MainThread]: On macro_stage_external_sources: ROLLBACK
[0m16:03:14.376803 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:03:14.377137 [debug] [MainThread]: On macro_stage_external_sources: Close
[0m16:03:14.450586 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  Runtime Error
    [RequestId=e051b279-75c3-9e49-a036-dd83aee440ed ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme
[0m16:03:14.462012 [debug] [MainThread]: Traceback (most recent call last):
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 660, in exception_handler
    yield
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 705, in add_query
    cursor.execute(sql, bindings)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 477, in execute
    self._cursor.execute(sql, bindings)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 852, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 944, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 786, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 503, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: [RequestId=e051b279-75c3-9e49-a036-dd83aee440ed ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 660, in exception_handler
    yield
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 1043, in execute_macro
    result = macro_function(**kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 220, in macro
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 298, in call
    return __obj(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 52, in macro
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 298, in call
    return __obj(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/impl.py", line 131, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 289, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 728, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 722, in add_query
    cursor.close()
  File "/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 665, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  [RequestId=e051b279-75c3-9e49-a036-dd83aee440ed ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/task/run_operation.py", line 47, in run
    self._run_unsafe()
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/task/run_operation.py", line 37, in _run_unsafe
    res = adapter.execute_macro(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 1043, in execute_macro
    result = macro_function(**kwargs)
  File "/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 678, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  Runtime Error
    [RequestId=e051b279-75c3-9e49-a036-dd83aee440ed ErrorClass=INVALID_PARAMETER_VALUE] Missing cloud file system scheme

[0m16:03:14.464078 [debug] [MainThread]: Command `dbt run-operation` failed at 16:03:14.463889 after 2.05 seconds
[0m16:03:14.464557 [debug] [MainThread]: Connection 'macro_stage_external_sources' was properly closed.
[0m16:03:14.464953 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106624700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1330c6370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x132e348b0>]}
[0m16:03:14.465411 [debug] [MainThread]: Flushing usage events
[0m16:03:25.756253 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1078a4220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11130fe50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11130fdf0>]}


============================== 16:03:25.780169 | 79cfe5d3-4712-49a1-9891-5804437cc943 ==============================
[0m16:03:25.780169 [info ] [MainThread]: Running with dbt=1.5.0
[0m16:03:25.780533 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi/logs', 'profiles_dir': '/Users/franco.patano/dbt_projects/tpcdi_dbt/dbtpcdi', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m16:03:26.464985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '79cfe5d3-4712-49a1-9891-5804437cc943', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10446a550>]}
[0m16:03:26.474940 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '79cfe5d3-4712-49a1-9891-5804437cc943', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x137d06400>]}
[0m16:03:26.504693 [debug] [MainThread]: checksum: 4568eb639a77b8fcb3a1f4a07856f42b1ff63f1376652889143968e1dbdafbda, vars: {}, profile: , target: , version: 1.5.0
[0m16:03:26.618953 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:03:26.621682 [debug] [MainThread]: Partial parsing: updated file: dbsql_dbt_tpch://models/sources.yml
[0m16:03:26.638784 [debug] [MainThread]: 1699: static parser successfully parsed incremental/FactHoldings.sql
[0m16:03:26.648125 [debug] [MainThread]: 1699: static parser successfully parsed incremental/DimTrade.sql
[0m16:03:26.651141 [debug] [MainThread]: 1699: static parser successfully parsed incremental/FactCashBalances.sql
[0m16:03:26.653441 [debug] [MainThread]: 1699: static parser successfully parsed incremental/DimAccount.sql
[0m16:03:26.655571 [debug] [MainThread]: 1699: static parser successfully parsed incremental/FactMarketHistory.sql
[0m16:03:26.657989 [debug] [MainThread]: 1699: static parser successfully parsed incremental/FactWatches.sql
[0m16:03:26.660084 [debug] [MainThread]: 1699: static parser successfully parsed silver/DimSecurity.sql
[0m16:03:26.662181 [debug] [MainThread]: 1699: static parser successfully parsed incremental/tempSumpFiBasicEps.sql
[0m16:03:26.664411 [debug] [MainThread]: 1699: static parser successfully parsed silver/Financial.sql
[0m16:03:26.666498 [debug] [MainThread]: 1699: static parser successfully parsed silver/DimCompany.sql
[0m16:03:26.668657 [debug] [MainThread]: 1699: static parser successfully parsed incremental/DimCustomer.sql
[0m16:03:26.670922 [debug] [MainThread]: 1699: static parser successfully parsed incremental/Prospect.sql
[0m16:03:26.673063 [debug] [MainThread]: 1699: static parser successfully parsed incremental/DimCustomerStg.sql
[0m16:03:26.675079 [debug] [MainThread]: 1699: static parser successfully parsed base/AccountIncremental.sql
[0m16:03:26.677174 [debug] [MainThread]: 1699: static parser successfully parsed base/CashTransactionIncremental.sql
[0m16:03:26.679136 [debug] [MainThread]: 1699: static parser successfully parsed base/CustomerIncremental.sql
[0m16:03:26.681124 [debug] [MainThread]: 1699: static parser successfully parsed incremental/tempDailyMarketHistorical.sql
[0m16:03:26.683227 [debug] [MainThread]: 1699: static parser successfully parsed base/DailyMarketHistorical.sql
[0m16:03:26.685173 [debug] [MainThread]: 1699: static parser successfully parsed base/DailyMarketIncremental.sql
[0m16:03:26.687107 [debug] [MainThread]: 1699: static parser successfully parsed silver/DimBroker.sql
[0m16:03:26.689192 [debug] [MainThread]: 1699: static parser successfully parsed base/HoldingIncremental.sql
[0m16:03:26.691187 [debug] [MainThread]: 1699: static parser successfully parsed base/BatchDate.sql
[0m16:03:26.693177 [debug] [MainThread]: 1699: static parser successfully parsed base/TradeIncremental.sql
[0m16:03:26.695783 [debug] [MainThread]: 1699: static parser successfully parsed base/WatchIncremental.sql
[0m16:03:26.698053 [debug] [MainThread]: 1699: static parser successfully parsed base/ProspectRaw.sql
[0m16:03:26.924914 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '79cfe5d3-4712-49a1-9891-5804437cc943', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16851c0d0>]}
[0m16:03:26.939285 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '79cfe5d3-4712-49a1-9891-5804437cc943', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x137cc7460>]}
[0m16:03:26.939708 [info ] [MainThread]: Found 27 models, 18 tests, 0 snapshots, 0 analyses, 681 macros, 0 operations, 0 seed files, 33 sources, 0 exposures, 0 metrics, 0 groups
[0m16:03:26.939982 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '79cfe5d3-4712-49a1-9891-5804437cc943', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11138e670>]}
[0m16:03:26.940475 [debug] [MainThread]: Acquiring new databricks connection 'macro_stage_external_sources'
[0m16:03:26.940672 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:03:26.940828 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:03:26.952812 [info ] [MainThread]: 1 of 33 START external source tpcdi.StatusType
[0m16:03:26.957180 [debug] [MainThread]: On "macro_stage_external_sources": cache miss for schema ".tpcdi", this is inefficient
[0m16:03:26.967224 [debug] [MainThread]: Using databricks connection "macro_stage_external_sources"
[0m16:03:26.967553 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */
show tables in `tpcdi`
  
[0m16:03:26.967737 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:03:27.344419 [debug] [MainThread]: SQL status: OK in 0.3799999952316284 seconds
[0m16:03:27.351355 [debug] [MainThread]: While listing relations in database=, schema=tpcdi, found: 
[0m16:03:27.365739 [info ] [MainThread]: 1 of 33 (1) drop table if exists `main`.`tpcdi`.`StatusType`
[0m16:03:27.368011 [debug] [MainThread]: Using databricks connection "macro_stage_external_sources"
[0m16:03:27.368282 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
        drop table if exists `main`.`tpcdi`.`StatusType`
    

            
[0m16:03:27.475174 [debug] [MainThread]: SQL status: OK in 0.10999999940395355 seconds
[0m16:03:27.476315 [info ] [MainThread]: 1 of 33 (1) OK
[0m16:03:27.476624 [info ] [MainThread]: 1 of 33 (2) create table `main`.`tpcdi`.`StatusType` (                    st_id string,     ...  
[0m16:03:27.477202 [debug] [MainThread]: Using databricks connection "macro_stage_external_sources"
[0m16:03:27.477426 [debug] [MainThread]: On macro_stage_external_sources: /* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table `main`.`tpcdi`.`StatusType` (
        
            st_id string,
            st_name string
    )  using csv
    options ('sep' = '|', 
'header' = 'false')
    
    
    
    location 'dbfs:/FileStore/tables/Batch1/StatusType.txt'
    

            
[0m16:03:27.609535 [debug] [MainThread]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.0", "dbt_databricks_version": "1.5.4dev1", "databricks_sql_connector_version": "2.6.2", "profile_name": "sql_endpoints", "target_name": "dev", "connection_name": "macro_stage_external_sources"} */

                 
    
    create table `main`.`tpcdi`.`StatusType` (
        
            st_id string,
            st_name string
    )  using csv
    options ('sep' = '|', 
'header' = 'false')
    
    
    
    location 'dbfs:/FileStore/tables/Batch1/StatusType.txt'
    

            
[0m16:03:27.610014 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [RequestId=67dc734a-2fa7-4de9-a688-5466ed373cd7 ErrorClass=INVALID_PARAMETER_VALUE] Unsupported cloud file system scheme `dbfs`
[0m16:03:27.610881 [debug] [MainThread]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: com.databricks.sql.managedcatalog.UnityCatalogServiceException: [RequestId=67dc734a-2fa7-4de9-a688-5466ed373cd7 ErrorClass=INVALID_PARAMETER_VALUE] Unsupported cloud file system scheme `dbfs`
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:656)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:546)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:403)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:156)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:381)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:366)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:415)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.databricks.sql.managedcatalog.UnityCatalogServiceException: [RequestId=67dc734a-2fa7-4de9-a688-5466ed373cd7 ErrorClass=INVALID_PARAMETER_VALUE] Unsupported cloud file system scheme `dbfs`
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:33)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:23)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:105)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:3282)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.generateTemporaryPathCredentials(ManagedCatalogClientImpl.scala:3000)
	at com.databricks.sql.managedcatalog.ManagedCatalogCommon.generateTemporaryPathCredentials(ManagedCatalogCommon.scala:938)
	at com.databricks.unity.CredentialScopeSQLHelper$.checkPathOperations(CredentialScopeSQLHelper.scala:67)
	at com.databricks.unity.CredentialScopeSQLHelper$.register(CredentialScopeSQLHelper.scala:107)
	at com.databricks.unity.CredentialScopeSQLHelper$.registerCreateTableAccess(CredentialScopeSQLHelper.scala:277)
	at com.databricks.sql.managedcatalog.CredentialScopeTableCredentialHandler.injectCredential(ResolveWithCredential.scala:466)
	at com.databricks.sql.managedcatalog.ResolveWithCredential.com$databricks$sql$managedcatalog$ResolveWithCredential$$maybeDecorateCatalogTable(ResolveWithCredential.scala:75)
	at com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:148)
	at com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:146)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:372)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:209)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:208)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:32)
	at com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:146)
	at com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:61)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:229)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:229)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:226)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:218)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8(RuleExecutor.scala:296)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$8$adapted(RuleExecutor.scala:296)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:296)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:383)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:376)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:283)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:376)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:304)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:189)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:356)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:379)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:355)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:156)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:348)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:380)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:817)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:380)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1040)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:377)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:150)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:140)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getRuntimeCategory$1(QueryRuntimePrediction.scala:300)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1040)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:299)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:349)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:345)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:77)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:76)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:62)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:111)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)
	... 3 more

[0m16:03:27.611692 [debug] [MainThread]: Databricks adapter: operation-id: b'\x01\xee\x0e\xe4\xbc\xa8\x1b\x94\xa3\x9ab\xaf\xe1;\rn'
[0m16:03:27.611955 [debug] [MainThread]: Databricks adapter: Error while running:
macro stage_external_sources
[0m16:03:27.612152 [debug] [MainThread]: Databricks adapter: Runtime Error
  [RequestId=67dc734a-2fa7-4de9-a688-5466ed373cd7 ErrorClass=INVALID_PARAMETER_VALUE] Unsupported cloud file system scheme `dbfs`
[0m16:03:27.612409 [debug] [MainThread]: On macro_stage_external_sources: ROLLBACK
[0m16:03:27.612603 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:03:27.612779 [debug] [MainThread]: On macro_stage_external_sources: Close
[0m16:03:27.671341 [error] [MainThread]: Encountered an error while running operation: Runtime Error
  Runtime Error
    [RequestId=67dc734a-2fa7-4de9-a688-5466ed373cd7 ErrorClass=INVALID_PARAMETER_VALUE] Unsupported cloud file system scheme `dbfs`
[0m16:03:27.677934 [debug] [MainThread]: Traceback (most recent call last):
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 660, in exception_handler
    yield
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 705, in add_query
    cursor.execute(sql, bindings)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 477, in execute
    self._cursor.execute(sql, bindings)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/client.py", line 472, in execute
    execute_response = self.thrift_backend.execute_command(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 852, in execute_command
    return self._handle_execute_response(resp, cursor)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 944, in _handle_execute_response
    final_operation_state = self._wait_until_command_done(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 786, in _wait_until_command_done
    self._check_command_not_in_error_or_closed_state(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/databricks/sql/thrift_backend.py", line 503, in _check_command_not_in_error_or_closed_state
    raise ServerOperationError(
databricks.sql.exc.ServerOperationError: [RequestId=67dc734a-2fa7-4de9-a688-5466ed373cd7 ErrorClass=INVALID_PARAMETER_VALUE] Unsupported cloud file system scheme `dbfs`

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 660, in exception_handler
    yield
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 1043, in execute_macro
    result = macro_function(**kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 220, in macro
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 298, in call
    return __obj(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 52, in macro
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/jinja2/runtime.py", line 298, in call
    return __obj(*args, **kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/utils.py", line 72, in wrapper
    return func(*new_args, **new_kwargs)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/impl.py", line 131, in execute
    return super().execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 289, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 728, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 722, in add_query
    cursor.close()
  File "/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 665, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  [RequestId=67dc734a-2fa7-4de9-a688-5466ed373cd7 ErrorClass=INVALID_PARAMETER_VALUE] Unsupported cloud file system scheme `dbfs`

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/task/run_operation.py", line 47, in run
    self._run_unsafe()
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/task/run_operation.py", line 37, in _run_unsafe
    res = adapter.execute_macro(
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/base/impl.py", line 1043, in execute_macro
    result = macro_function(**kwargs)
  File "/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/Users/franco.patano/dbt_projects/tpcdi_dbt/dbt_venv/lib/python3.9/site-packages/dbt/adapters/databricks/connections.py", line 678, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc)) from exc
dbt.exceptions.DbtRuntimeError: Runtime Error
  Runtime Error
    [RequestId=67dc734a-2fa7-4de9-a688-5466ed373cd7 ErrorClass=INVALID_PARAMETER_VALUE] Unsupported cloud file system scheme `dbfs`

[0m16:03:27.678880 [debug] [MainThread]: Command `dbt run-operation` failed at 16:03:27.678789 after 1.94 seconds
[0m16:03:27.679129 [debug] [MainThread]: Connection 'macro_stage_external_sources' was properly closed.
[0m16:03:27.679339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1078a4220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1685fa6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107dc4250>]}
[0m16:03:27.679596 [debug] [MainThread]: Flushing usage events
